================================================================================
TITLE: Part III Tools to Validate Land Use Cover Maps: A Review
================================================================================

AUTHORS:
- David García-Álvarez
- Martin Paegelow
- Jean Mas
- María Teresa
- Camacho Olmedo
- Jean-François Mas
- Jean-François Mas
- Marta Gallardo
- Jean-François Mas
- Miguel Ángel
- Ramón Molinero-Parejo
- Sabina Nanu
- Francisco Escobar
- Miguel Castillo-Santiago
- Edith Mondragón-Vázquez
- Roberto Domínguez-Vera
- María Camacho
- Jean-François Mas
- Jean-François Mas
- Teresa María
- Jean-François Olmedo
- Martin Mas
- Paegelow
- Javier Hinojosa
- Jaime Quintero
- Francisco José
- Jurado Pérez
- Jean Mas
- Ramón Molinero
- Parejo Departamento De Geología
- Medio Ambiente
- Jean-François Mas
- Departamento De Geología
- M Camacho Olmedo
- J.-F Mas
- García -Álvarez
- Olmedo Camacho
- Paegelow Mt
- Jean-François Mas
- Medio Geografíay
- Ambiente
- J.-F Mas
- Toro Balbotín
- Jean-François Mas
- Á Molinero-Parejo
- J.-F Mas
- J.-F Mas
- J.-F Mas
- J Lara Hinojosa
- F Jurado Pérez
- Á Quintero

ABSTRACT:
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

================================================================================
PAPER CONTENT:
================================================================================

## Untitled ##
This chapter sets out the aims of this book and explains the methods and approaches applied in its production. It also aspires to be a guide, offering readers instructions as to how best to use the book. We therefore strongly encourage all readers to read this chapter carefully, so as to gain a clearer understanding of all the different aspects analysed in this book. This chapter also provides essential information for those wishing to do the practical exercises in this book.
We begin by presenting the aims of the book and we offer a few tips explaining how each group of users can make best use of this book according to their particular requirements. Then, we provide information about the software and the data required to carry out the practical exercises presented in Parts II and III (Sect. 5). In the last section, we offer a detailed explanation of the review of LUC datasets carried out in Chap. "Land Use Cover Datasets: A Review" and Part IV (Sect. 6).
The book is the fruit of two research projects which seek to provide a clearer understanding of the uncertainties associated with Land Use Cover maps and with the results of Land Use Cover Change modelling exercises (INCERTIMAPS Project: Suitability and uncertainty of land use and land cover maps for the analysis and modelling of territorial dynamics) and the promotion of Open Access software for teaching spatial science (PE117519: Herramientas para la Enseñanza de la Geomática con programas de Código Abierto). See complete information about these projects in the section Acknowledgements.
2 What is the Main Aim of This Book?
The aim of this book is to provide an up-to-date state of the art on Land Use Cover (LUC) datasets and validation tools. The book summarizes the available information and makes it accessible to any interested user, including some of the latest developments in the field.
The book was conceived as a practical tool to inform readers about currently available LUC datasets at global and supra-national scales and to help them understand more about the validation of LUC data and LUCC modelling exercises, so enabling them to validate their own data and models. To this end, the book combines brief theoretical explanations with practical information and exercises.
Part I of the book briefly covers the theoretical foundations of LUC mapping, LUCC modelling and the analysis and assessment of their associated uncertainties. Parts II and III were conceived as practical guides to enable any reader to use any of the tools and data. Part II covers the visualization of LUC data and the production of reference datasets to validate LUC maps. Part III describes the use of common validation tools and the interpretation of their results. All the practical exercises are accompanied by an explanation of the basic theory behind them, so as to enable users to understand the analyses and the principles on which the techniques are based. Finally, Part IV of the book characterizes the most relevant available LUC data. It also provides all the necessary information as to how to download and use the datasets.
As the book aims to reach the widest possible audience, the theory is briefly explained in simple, understandable terms. Practical exercises are implemented in QGIS, an open-source Geographical Information System, which can be downloaded for free.
3 Who is the Book Aimed At?
The book is aimed at anyone interested in Land Use Cover (LUC) mapping, Land Use Cover Change modelling and Land Use Cover Change analysis. Although to make full use of the book, some background in the field is recommended, it aims to be accessible and useful to all kinds of user, regardless of their level of expertise. Nonetheless, a basic knowledge of spatial analysis and GIS analysis is required to understand a lot of the information provided.
The book will be particularly useful for researchers working in the fields of LUC mapping and LUCC modelling and especially for those interested in validation methods and the available sources of LUC data. Those interested in the application of open-source software in LUC may also find this book very useful, as it is the only book working with open-source software that focuses on these topics from a holistic perspective. For the QGIS community, the book provides the relevant information and tools to enable users to take full advantage of the software and expand the fields in which it can be effectively applied.
4 How to Use This Book?
The book can be used in different ways, depending on the type of user and their particular background and interests. With this in mind, it has been conceived as a flexible tool that can be used for a wide variety of purposes.
Beginners in this field are referred to Chap. "Land Use Cover Mapping, Modelling and Validation. A Background", as are other users interested in gaining an overall picture of LUC mapping, LUCC modelling and the essential concepts required for uncertainty and validation analyses. This short, yet comprehensive chapter sets out the basic theoretical principles on which the rest of the book is based and is therefore recommended reading for all users.
For LUC data visualization and creation, readers are referred to Part II of this book. It provides an overview of the different options available for symbolizing LUC data and LUC change in GIS. It also addresses some of the problems associated with the spatial visualization of LUC information. This part of the book also includes a tutorial on the creation of a set of sample points for LUC data validation with QGIS.
Users interested in the validation of LUC datasets and Land Use Cover Change (LUCC) modelling exercises should refer to Chap. "Validation of Land Use Cover Maps: A Guideline". This provides guidelines for validating different LUC products: single LUC maps, LUC map series, and outputs from LUCC modelling exercises. The different tools and methods referred to in these guidelines are then described in detail and applied in practice in the example exercises in Part III of the book.
Users interested in doing the example QGIS exercises appearing in this book should refer to Sect. 5 of this chapter, which presents all the data and the cases studied in this book. It also offers essential information about the particular version of QGIS that we use and about how to integrate R software into QGIS, a necessary step when carrying out some of the exercises set out in the book.
Those interested in LUC data sources should refer to Chap. "Land Use Cover Datasets: A Review", which offers an introduction to LUC mapping at global and supra-national scales, including a review of the different datasets available. Part IV of the book offers in-depth descriptions of most of the datasets that are available for download, detailing their specific characteristics and how they can be accessed. The methodology followed in the review of the datasets is described in Sect. 6 of this chapter.

## QGIS Plugins ##
QGIS works with plugins written in the C++ and Python programming languages. These plugins are an easy way to expand the capabilities of the software, which is why many of the features of the software are currently implemented through these plugins.
There are two types of plugins: core and external plugins (QGIS Project 2020). The core plugins are maintained by the QGIS Development Team and automatically form part of the distributed software. The external plugins are developed by a community of users and are available at the QGIS Python Plugins Repository (https://plugins.qgis.org/plugins/).
The external plugins may be up-to-date or outdated and are usually available for specific QGIS versions. The official plugin repository includes information about all these questions. External plugins that are still in the early stages of development and have not been widely used are marked by QGIS as experimental plugins and are not directly available through the software.
Several QGIS plugins are used in the exercises presented in this book (Table 
The Semi-Automatic Classification Plugin is one of the most important QGIS plugins and is used in many of the exercises in this book. It was developed and updated by Luca 
LecoS (Landscape ecology Statistics) is a plugin developed by 
The R Processing Provider allows the R software capabilities to be integrated into QGIS. Full documentation on the plugin is available at the official website (see 
QuickMapServices is a very used QGIS plugin that allows to import to the QGIS interface many different web-map services of different kinds (XYZ tiles, TMS, WMS, WMTS, ESRI ArcGIS Services). More information on the plugin is available in the official website (see Table 
We also use MapAccurAssess, a plugin specifically developed for the exercises of this book by Domínguez Vera (2021). Although not available yet in the official QGIS plugin repository, it can be downloaded from the official repository of information accompanying this book (see Table 
To install any of these plugins in QGIS, access the "Manage and install plugins…" tool in the plugins menu to find the plugin you require. Once selected, click on the "Install Plugin" option (Fig. 

## Integrating R into QGIS ##
Some of the exercises presented in this book use R, a free, open-source statistical software. QGIS enables the R environment to be integrated into the software, making it easier for any QGIS user to take full advantage of the tools available through R.
QGIS does not have the required tools to compute all the validation tools and methods that have been reviewed in this book. We have therefore had to implement some of them in QGIS through the R processing environment. Users wishing to find out more about R and its integration into QGIS, with practical exercises about how to use both software packages in combination, should consult the manual by 
To integrate R into QGIS, users must begin by downloading the R software. R and any of its associated data can be downloaded from a comprehensive file network, from which users must select the mirror closest to their location at https://cran.r-project.org/mirrors.html.
Once downloaded and installed, users must also install a series of packages in R to execute the different tools and methods included in the book (Table 
Table 
After installing R and the required packages, we need to install the QGIS plugin that allows us to integrate the two software packages. This is the "Processing R provider" plugin. Instructions to this end can be found in Sect. 5.2 of this chapter. After installing the plugin, users must download the scripts we have developed to integrate the R tools and capabilities into QGIS. These scripts are listed in Table 

## Study Areas ##
The exercises provided in this book are applied to three specific study areas: the Ariège Valley (France), the Asturias Central Area (Spain) and the Marqués de Comillas municipality (Mexico). We now offer a brief introduction to these study areas, so as to give readers the contextual information they require for a clearer understanding of the results of the exercises.

## The Asturias Central Area (Spain) ##
The Asturias Central Area is a rural-industrial-urban area located in the heart of Asturias, in Northern Spain (Fig. 
The cities at the top of the urban hierarchy are Oviedo, Gijón and Avilés, which concentrate most of the urban LUC dynamics in recent decades (Gobierno del Principado de Asturias 2016). The area within the triangle formed by the three cities has also been the subject of important LUC dynamics, with the emergence of new industrial and residential developments, attracted by the accessibility that the area's extensive transport network provides 

## Ariège Valley (France) ##
The Ariège Valley area consists of the central part of the valley formed by the River Ariège, which is situated is in the department of the same name about 70 km south of Toulouse (Fig. 
In the past, the Ariège Valley was a centre for industrial and mining activities while today it is mainly rural. Tourism is increasingly common. The most notable LUC dynamics are reforestation and the increase in built-up areas, which are mainly concentrated along the river.

## Marqués de Comillas (Mexico) ##
Marques de Comillas is a physiographical region of the Lacandon rainforest in Chiapas, Mexico (Fig. 
A colonization programme by the Mexican Government in the 1970s encouraged the establishment of farming communities in forest-covered areas, promoting agriculture, agroforestry (cacao) and cattle ranching, which is currently the most important business activity. Over the last 40 years, Marqués de Comillas has suffered a dramatic loss in forest cover; in the mid-1980s, forests occupied 83% of the region, while today, this has fallen to just 29%, less than half of which are well-preserved forests. The landscapes are now made up above all of mosaics of agricultural lands, cattle pastures and human settlements. information about this dataset can be found in Chap. " General Land Use Cover datasets for Europe" of this book.
A simpler version of CLC is used in the Ariège Valley (Fig. 
Some of the maps in the Asturias Central Area case study were obtained after simplification of the SIOSE database. The maps were obtained after the classification of each SIOSE polygon into a single category and after the rasterization at 50 m of the original vector dataset (Fig. 
The Marques de Comillas LUC map (Fig. 
In the following tables, we list the files from the different datasets and LUC modelling exercises described above that have been used in different exercises in this book. More datasets are available online, including extra LUC maps and model drivers not considered in the exercises in this book.
The tables include information about the name of the file available for download and the descriptive name used to refer to these files in the book. For each dataset, we also provide the projection of the dataset and the file describing the legend of the maps. A document listing all these characteristics for the layers only available online is also provided when downloading the data.  Chapter "Land Use Cover Datasets: A Review" and Part IV of the book contain a review of the Land Use Cover datasets available at global and supra-national scales. Due to the limited extent and scope of this book, we did not review national and regional LUC datasets, which are far too numerous for our purposes.
The datasets we reviewed are classified into two groups, depending on the information they provide. The first group is made up of the datasets that provide information about the different land uses or covers without focusing on any one of them in particular, i.e. general LUC datasets. The second group consists of the LUC datasets that map a specific land use or cover in detail (e.g. vegetation, croplands, built-up areas…). These are referred to as thematic LUC datasets. Some datasets are difficult to assign to one of the two groups, as they map a wide range of LUC categories while also providing specific detail on just one of them. The authors decided which group to assign them to on a case-by-case basis.
The datasets were also classified according to their extent, differentiating between global and supra-national LUC datasets. The first group of datasets maps land uses or covers all over the Earth, while the second maps them for a specific area covering more than one country. The maps in the second group may cover a whole continent or focus on just a few countries.
When making the review, we consulted the most relevant web portals and repositories of LUC data (Table 
Very old or outdated maps, which were produced according to traditional cartographic methods, are not included in this review. Nor are other old maps that combine LUC information with other data about climate or biogeographic variables, such as the maps produced by 
There are plenty of other spatial datasets that provide important information for studying specific land covers. For vegetation covers, maps of live biomass are a good example 
Part IV of the book characterizes in detail all the reviewed LUC datasets that are currently available for download and may be relevant for a wide community of users. Datasets produced at very coarse scales or which are already very outdated are not described in Part IV, as they are of limited utility for most members of the LUC community. LUC datasets currently unavailable for download are not characterized in Part IV either. We tried to obtain, either online or by contacting the authors, all the global or supra-national datasets to which we found references. Some of them, however, are no longer available. These datasets have not been reviewed.
The LUC datasets described in Part IV were characterized according to the following elements: information about the project or context within which they were produced; information about their method of production; description of the data available for download; and practical information for using the dataset in an effective way. For each dataset we also provide all the technical references in which it is described as well as other references of interest in which it is used or analysed. A table summarizing the main characteristics of the dataset (extent, temporal availability, spatial resolution, updates, accuracy…) is also provided.

## Introduction ##
Land Use and Land Cover (LUC) data is an important source of information for a wide range of users from different backgrounds and scientific disciplines. It provides an overview of the different covers on the Earth's surface (e.g. vegetation, agricultural fields, rocks, water, artificial sur-faces…) and how they evolve over time. It also traces how these covers are used (land use) and how this use changes.
LUC data can be very useful in an array of different fields. It is especially valuable for understanding the impact that many natural and human-induced processes, such as climate change, deforestation and urbanization, can have on the Earth's surface. As a result, LUC research has been receiving increasing attention over recent decades, and the number of fields making use of this data is on the rise.
Researchers have been proposing new methods and techniques for producing LUC maps. This has increased the number of LUC datasets available at global, continental, regional and local scales. This has also led to an increase in the number of users who decide to make their own LUC maps. The validation of LUC data has also been the subject of specific research and new methods, strategies and techniques have been proposed for validating and analysing LUC maps.
Despite all these advances, many users are still unaware of the wide range of datasets available, while others lack a clear understanding of the methods or techniques that can be used to validate LUC data. Thus, in addition to producing more LUC datasets, more information is required. Users must be able to find out more about the most appropriate datasets for their field of study, and the general uncertainties and limitations of each one. They should also be informed about the methods that can be used to assess the specific utility and uncertainties of this data for their line of research.

## Land Use versus Land Cover ##
Although Land Use and Land Cover are often combined, for example, in references to LUC maps and information, they in fact have quite separate meanings. Many authors have proposed complementary definitions (Di Gregorio and Jansen 1998; 

## Directive INSPIRE (2007/2/EC) ##
Land Cover: Physical and biological cover of the earth's surface including artificial surfaces, agricultural areas, forests, (semi-)natural areas, wetlands, water bodies.
Land Use: Territory characterised according to its current and future planned functional dimension or socio-economic purpose 
Land cover refers to the Earth's biophysical covers. Areas without a specific cover, such as areas of bare rock or bare soil, are also regarded as land covers. By contrast, land use refers to the activities that humans carry out on the Earth's surface or on a specific land cover.
A land cover can have one or multiple uses, or even none. An artificial surface could be used to host people (e.g. residential area), production (e.g. industrial area) or leisure activities (e.g. sports facilities). In maps at coarser scales, this artificial surface can host all these uses together. For example, an urban area is an artificial cover which has multiple uses. Bare rock, on the other hand, often hosts no land use of any kind.
A specific land use can also be associated with multiple land covers at the same time. An airport is a land use that is usually associated with several artificial covers, such as buildings, roads and runways, and also with vegetation covers, like grassland.
Whereas land covers are usually visible in aerial or satellite images, land uses are more difficult to distinguish. For instance, a building could have multiple uses: apartments, offices, industrial plants, sports facilities, etc. Sometimes the land use can be deduced from contextual information in the image, but, in most cases, additional information is required. This makes map production more difficult and expensive. As a result, most maps only provide information about land covers. In other cases, they focus on the land use of certain specific covers, such as artificial or agricultural areas, so providing both Land Use and Land Cover (LUC) data. This is why in LUC science, we generally talk about Land Use and Land Cover information, as the two aspects tend to be combined within the same datasets.
3 Land Use and Land Cover Mapping:
A History Some information on Land Use and Land Cover was available prior to the advent of remote sensing instruments 
Before the emergence of aeroplanes and satellites, the main method for map production was ground survey 
With the advent of aerial imagery and, later, satellite imagery, mappers obtained a view of the Earth's surface from the top of the atmosphere or from space. Mapping became easier and cheaper 
Information collected in the field was still required to validate what was photointerpreted and to include some extra information that was not discernible in the image 
Aerial images became increasingly common from the beginning of the twentieth century, with the development of the aeroplane industry within the context of the two World Wars 
The launch of the first satellite into space in 1957 proved a turning point in the history of LUC mapping 
Satellites record the reflectance of the Earth's surface in different regions of the electromagnetic spectrum. The reflectance curve for each land cover can be independently characterized and defined (Chuvieco 2016; 
Despite these limitations, the availability of satellite imagery and the ease with which land cover information could be obtained from them boosted the production of land cover maps, which until then had been relatively rare 
Manual photointerpretation was still common in the early years of satellite remote sensing 
As LUC mapping became easier, cheaper and quicker, many institutions, scientists and other users began producing LUC datasets at all the different scales 
The AVHRR sensor on board the NOAA weather satellites launched in 1978 
Since then, LUC mapping practice has been developed in parallel with the launch of new satellites and the increasing improvement in their spatial and spectral resolutions 
The key role played by the USGS is undeniable. It authored the first research laying down the foundations of modern LUC mapping 
Users now have more information available than ever 

## Uses of LUC Data ##
The importance and utility of Land Use and Land Cover information is beyond doubt. LUC data is a valuable source of information for scientists 
Policymakers also need LUC data for proper resource management and to help them deal with many of the challenges facing society today 
Local administrations need land use information for spatial planning. Regional and national governments may require LUC information for water management, flood control or in the design and assessment of environmental policies. At the international level, LUC data provides important evidence on which to base decisions regarding many of the global challenges facing society today.
Most of the current global agendas refer to policy objectives involving Land Use and Land Cover. They play a direct role in 7 out of 17 UN Sustainable Development Goals (SDGs), and in the UN Framework Convention on Climate Change (UNFCCC), the Convention on Biological Diversity, the UN Convention to Combat Desertification (UNCCD) and the Ramsar Convention on Wetlands 
The Group on Earth Observations (GEO) has defined eight Social Benefit Areas (SBAs) in which Earth observations, including LUC data, provide useful evidence in support of policymaking. 
Among scientists, LUC maps are frequently used as a basis for modelling exercises 
LUC information is also used for many other research activities, most of them related to the different policy fields mentioned above. In recent years, it has been applied, for example, in studies analysing habitat distribution and ecosystem services 

## Land Change Science ##
Although LUC information is employed for manifold purposes, the field taking most advantage of this data is Land Use and Land Cover Change (LUCC) analysis 
LUC change analyses are widely used in climate change studies 
The importance of LUCC studies has led to the emergence of a specialist field called Land Change Science 
Land Change scientists are responsible for monitoring LUC change, understanding it and modelling for the future, so obtaining knowledge and evidence that may be useful for policymaking 
Many international programmes and organizations have stressed the importance of LUCC and Land Change Science 

## Land Use and Land Cover Change Modelling ##
As previously noted, Land Change Science is not only a question of analysing and understanding LUC changes, but it also seeks to model them in the near future 
Models allow us to play around with the system we are studying so as to predict how different policies affect LUC and the changes they may cause 
LUC maps are the main input for LUCC models 
Many types of LUCC models are available today 
Process-based models simulate the processes taking place, rather than the pattern (O'Sullivan and Perry 2013). There are different kinds of process-based models, with agent-based LUCC models gaining increasing popularity. These models simulate the behaviour of the agents or actors that take part in the system being modelled and their interactions (Crooks and Heppenstall 2012). These agents cause the processes taking place on the ground and the changes in the landscape pattern. Although important, LUC maps do not play the same key role in these models as they do in pattern-based models, as most of the parameters used in process-based models are inferred from other sources 
LUCC models can also be classified according to the scale of analysis, their stochastic or deterministic nature, the type of scenarios they can produce and the techniques and methods they apply (García-Álvarez 2018a). For example, some models include Markov chains to estimate the quantity of simulated change in the future 
Modelling exercises normally consist of four main phases: calibration, simulation, validation and the proposal of scenarios 
Calibration refers to the setting-up and parametrization of the model 
The methods and techniques used for calibration are similar to, if not the same as, those used in the validation phase 

## Uncertainty and Validation ##
The increased availability of satellite and aerial imagery and the development of new methods and techniques for image processing and classification has enabled the production of an increasing number of LUC maps and time series of LUC maps at all scales 
With the increasing production and use of LUC maps and LUCC models, more attention has been paid to the uncertainty and limitations of these data and analyses 
It is important to realize that all spatial data and analyses contain some degree of uncertainty 
In the case of LUC maps, the complexity and variety of real landscapes is normally translated into a given set of categories (Di 
Mapping the full complexity of the Earth remains beyond human capacity, and even beyond existing computer capabilities 
To understand the uncertainty and limitations of our data and analyses, we usually carry out uncertainty assessments 
Although validation is already a common practice and there are many methods, strategies and reference data available for validating LUC maps and LUCC models, there is still a lot of room for improvement. In the case of LUCC maps, when 
In LUCCM, several authors emphasized the importance of analysing the uncertainty of the results, even when general validation exercises are carried out 
The uncertainty of most of the available LUC datasets has been assessed in a large range of research studies 
Many users develop their own maps, given the increasing availability of free imagery and tools with which to process and classify the images easily 

## Conclusions ##
Many frequent users of LUC data and LUCC models are unaware of the latest developments in validation and uncertainty analysis of LUC data. It is also possible that they have limited knowledge of many of the datasets currently available for carrying out LUC exercises.
Many of the recent advances in this field remain within closed scientific communities and are not disseminated among the wider LUC community outside the research arena. This book seeks to respond to their needs. It provides an overview of the state of the art on LUC datasets, including time series of LUC maps, and the tools and methods available for LUC map validation. It also presents and explains frequently used tools and guidelines for validating the results produced by LUCC models. As many of the tools and techniques reviewed here are used in both LUC mapping and LUCC modelling validation exercises, in this book we address these two analyses together.
A full validation exercise, characterizing all the uncertainties of a given dataset or model, is a complex task that requires a high level of expertise and a wide range of tools and strategies, each one addressing different sources of uncertainty. This is beyond the scope of this book. Here we focus on the quantitative validation of LUC maps and LUCC model results. For detailed information about qualitative analyses of uncertainty, we refer readers to more specialized bibliography, depending on the specific objectives of their research. Readers wishing to find out more about other important aspects of uncertainty and validation practice, such as uncertainty communication, are also referred to specific literature on this topic.

## Further Reading ##
Giri C (ed) (2012) Remote sensing of land use and land cover. Principles and applications. CRC Press. This is one of the main reference books on Land Use Cover mapping, focusing specifically on LUC mapping and analysis. It offers an overview of the main concepts associated with LUC mapping and remote sensing and provides an introduction to this field, tracing its history. It also addresses the main methodological issues in relation to LUC mapping using remote sensing techniques, such as validation practices, land cover change detection and image classification methods. In the third part, the book includes examples of regional LUC mapping and LUCC monitoring for different parts of the world. Focused on Europe, this book is part of the reference bibliography for LUC mapping and LUCC monitoring. It provides a state of the art of LUC mapping globally, for Europe and at a national level for some of the European countries. Several chapters focus on remote sensing practices and methods for LUC mapping and LUCC detection. The book also has several introductory chapters on the role of remote sensing in the production of LUC information. Other chapters focus on the LUCC monitoring of processes relevant for policymaking.
Camacho Olmedo MT, Paegelow M, Mas J-F, Escobar F (2018) Geomatic Approaches for Modeling Land Change Scenarios. 

## García ##
Land Use Cover Mapping, Modelling and Validation. A Background

## Introduction ##
Validation is a required step prior to the effective use of any Land Use Cover (LUC) dataset or of the results of a Land Use Cover Change Modelling (LUCCM) exercise. We need to understand to what extent these datasets and results are uncertain in order to be able to assess the limits that these uncertainties may impose on the conclusions of our analyses and studies.
There are many methods, tools and strategies currently available for validating LUC data and LUCCM exercises. However, comprehensive guidelines providing users with clear instructions and recommendations about how to carry out this validation are scarce. 
In this chapter, we aim to provide readers with a general overview of the available tools and strategies for validating LUC data-specifically LUC maps-and LUCCM exercises. We give readers different guidelines according to the type of maps they want to validate: single LUC maps (Sect. 3), time series of LUC maps (Sect. 4) and results of LUCCM exercises (Sect. 5). Although some of the available methods and tools can be applied to all these maps, each type of validation exercise has its own specific aspects that users must bear in mind. For example, the results of LUCCM exercises include soft and hard LUC maps. The hard outputs of a model-hard maps-are very similar to input LUC maps, while the soft outputs-soft maps-are continuous and ranked. We therefore also present some validation methods that focus specifically on soft maps.
Before presenting these validation methods and functions, it is important to make clear that visual inspection is an essential part of any validation exercise. It can provide a great deal of information about the uncertainties of the data being evaluated, which are not detected by the quantitative methods reviewed in this book. Visual inspection should be conducted during all validation exercises, at the beginning, at the end and throughout the entire process. The exercises presented in Part III have been applied using the Quantum GIS (QGIS) software and R scripts. To homogenize the exercises across the different chapters, they have the same standard objectives: to validate a map (t 1 ) against reference data/map (t 1 ) (single LUC map); to validate a series of maps with two or more time points (t 0 , t 1, t 2 …) (LUC maps series/ LUC changes); and, for results from LUCCM exercise, to validate soft maps produced by the model against a reference map of changes (t 0t 1 ) (soft LUC maps), to validate a simulation (T 1 ) against a reference map (t 1 ) (single LUC map -hard LUC maps) and to validate simulated changes (t 0 -T 1 ) against a reference map of changes (t 0t 1 ) (LUC maps series / LUC changeshard LUC maps). However, in certain specific cases, additions have been made to these standard titles. In addition to the applications of each method/function implemented in the practical exercises in this book, the cells shaded in grey in Fig. 

## Validation of Single Land Use Cover Maps ##
The validation of single LUC maps is the most widespread practice of all those addressed in this book. 
Users have been validating their maps since the advent of digital remote sensing and the first classifications of digital imagery, as a means of assessing to what extent the classified images resemble the real LUC on the ground. Now, several decades later, the validation of single LUC maps is a very common practice, and although new methods and tools have been developed over the years, the original ones remain popular. These are based above all on the comparison of the assessed LUC map with reference datasets through cross-tabulation 
The reference datasets for validating single LUC maps may be obtained from different sources of LUC data. These can be classified into two main groups: ground samples and reference LUC maps. However, in the validation exercises, other reference spatial data can also be used, such as the raw imagery used in the classification process or the soft maps obtained as a result.
The ground samples collected through field surveys provide highly accurate, detailed data. However, this information is very expensive to obtain and fieldwork is not an option when working with large study areas. This is why most reference LUC samples are obtained by photointerpretation or classification of satellite imagery. The data obtained via photointerpretation must be of higher quality that the data being validated. This usually involves careful interpretation of a set of samples using imagery with a higher spatial resolution than the images used to create the map. Another option is photointerpretation of the same imagery used to obtain the dataset, applying a different workflow and methods or techniques that guarantee better quality.
Those using these methods to obtain LUC samples for validation purposes should provide information about their accuracy or uncertainty. When obtaining reference data by field surveys or photointerpretation, users must take particular care when selecting the sampling strategy they will apply during the collection of this information, as it can have an important impact on the results of the validation exercise and on their validity (see chapter "Visualization and Communication of LUC Data").
LUC maps can also be validated against other LUC maps. In these cases, the reference LUC map must have a higher spatial resolution and greater detail that the map being assessed. They must also be of proven quality, i.e. maps or datasets with verified accuracy and uncertainty. Although less precise, validation exercises carried out by comparing the evaluated map with other LUC maps are quick and very cheap, hence their popularity. This also allows a wider set of methods and techniques to be used compared to the possibilities offered by reference datasets other than maps.
Users can also validate their LUC maps against additional sources of information other than reference datasets, in order to characterize the maps in more detail and gain a clearer picture of their uncertainty. Such sources include raw imagery, which is often used in the classification process, or the soft maps obtained from it, which are used to assess the characteristics of the pixels that make up each class. Raw imagery can be used to evaluate the reflectance value for all the pixels belonging to a particular class and how close it is to the reference reflectance value used in the classification process. When available, users can also compare each category pixel with soft maps showing the percentage of each pixel belonging to each of the LUC categories under consideration. Similar insights into the accuracy of LUC maps can be obtained by comparing them with continuous LUC If we focus on validation tools (Fig. 
In some cases, the level of agreement may vary at different levels of spatial detail. For example, when spatially aggregated and simplified, the LUC map being evaluated may show more agreement with the reference dataset. The choice of spatial resolution is therefore a source of uncertainty. To account for this uncertainty, we can cross-tabulate the assessed and reference datasets at multiple spatial resolutions (see Sect. 2 in chapter "Basic and Multiple-Resolution Cross Tabulation to Validate Land Use Cover Maps"), i.e. the original resolution and other coarser ones.
Different metrics are calculated from the confusion matrix (see chapters "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps" and "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"). These metrics summarize the agreement between reference and validated datasets in a single value and are therefore very easy to interpret. As a result, they have been widely used in LUC validation.
The most common metrics are the accuracy assessment statistics (see Sect. 5 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") and the Kappa Indices (see Sect. 3 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"). The accuracy assessment statistics are standard metrics that provide information about the similarity between two georeferenced data. They are obtained from the cross-tabulation matrix and enable the extraction of specific information contained in the matrix. They include, among others, the overall, producer's and user's accuracy metrics. They are usually supplied with the cross-tabulation matrix, providing extra information in addition to that provided by the matrix itself (e.g. category area adjusted by the error level, confidence intervals…).
Of all these metrics, the most commonly used in validation exercises is probably Overall accuracy. There has been great debate in the literature about the threshold above which the Overall accuracy of a map can be considered acceptable. The 85% threshold proposed by 
The overall accuracy metric does not provide information about the accuracy at which each category on the LUC map is mapped. Important differences are often identified in terms of the relative accuracy of the different categories. Mixed LUC categories do not usually show the same accuracy as spectrally pure categories. At high levels of thematic detail, very similar LUC categories can be easily confused and will, therefore, have lower levels of accuracy. Users must take these differences at the category level into account and report the accuracy values for each category. The general approach for agreement between maps at global and stratum level may be useful to this end (see Sect. 4 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"). Some authors talk specifically about Overall and Individual Spatial Agreement, proposing different metrics for these purposes 
It is also important to remember that the accuracy of a LUC map is not usually the same across the entire mapped area and considerable spatial variations are possible. The bigger the area being mapped, the more likely it is for there to be spatial differences in accuracy levels across the mapped area. The cross-tabulation matrix does not provide information about these spatial differences. When mapping large study areas made up of different, clearly distinguishable regions, each region can be validated independently, producing a specific cross-tabulation matrix in each case. The global analysis would cover the entire map, while specific areas of the map (e.g. a region, a municipality…) could also be analysed at the stratum level.
Overall Accuracy is highly correlated with the Kappa Index 
The methods mentioned above do not employ fuzzy logic and, instead, apply a binary logic when calculating agreement, i.e. the two elements agree or don't agree. Partial agreements are not considered. However, there are some tools for calculating map agreement that incorporate fuzzy logic, such as the Fuzzy Kappa or the Fuzzy Kappa Simulation 
Other metrics, similar to Kappa, have also been proposed. Usually they aim to outperform Kappa and correct some of its associated problems. These include, among others, the F-Score (Pérez-Hoyos et al. 2020), Scott's pi statistic 
Extensive research by 
Users can also specifically assess the pattern of the map they want to validate to find out how much its pattern coincides with that of the reference map. Pattern agreement can be assessed using Spatial metrics (see Sect. 1 in chapter "Spatial Metrics to Validate Land Use Cover Maps") and the Map Curves method (see Sect. 1 in chapter "Advanced Pattern Analysis to Validate Land Use Cover Maps"). Spatial metrics allow us to characterize different aspects of the map's pattern in detail, such as its fragmentation, the proportion allocated to each category, the complexity of the patches… 
Geographic weighting methods (GWR) (see chapter "Geographically Weighted Methods to Validate Land Use Cover Maps") can also be used to study the spatial distribution of LUC accuracy measures. The overall, user's and producer's accuracy metrics mentioned above are derived from the cross-tabulation matrix and are therefore not spatial metrics, i.e. they provide overall information for the entire area, without assessing the spatial distribution of error and accuracy. The application of Overall, user's and producer's accuracy metrics through GWR (see Sect. 1 in chapter "Geographically Weighted Methods to Validate Land Use Cover Maps") can help the user to assess the suitability of the LUC data and to observe local variations in accuracy and error on the map 

## Validation of Land Use Cover Maps Series/Land Use Cover Changes ##
There is no common practice or set of methods for validating or evaluating the uncertainty of a LUC map series with two or more time points (t 0 , t 1, t 2 …). Most of the exercises for the validation of LUC data only refer to single LUC maps, without focusing specifically on the LUC change studied through a series of LUC maps. One of the facets that users most demand from LUC data is the ability to study and display LUC changes over time. We therefore need methods and tools to assess the uncertainty of the changes that are measured from LUC maps. It is worth noting that the individual accuracy of two LUC maps involved in a post-classification comparison offers few clues as to the accuracy of change, because the relation between the errors in the two maps is unknown. As pointed out by 
One of the main limitations when it comes to validating LUC changes and LUC map series is the lack of reference data. We could obtain reference datasets via photointerpretation or field surveys. However, it is difficult to guess where the LUC changes will take place, as they may happen at different places and with different intensities and patterns over space and time. In addition, there is a clear lack of LUC map series showing accurate, validated LUC change that could be used as reference data. Another option would be to validate the LUC changes against other types of reference data. This could be done for example by comparing the LUC changes measured over a time series of LUC maps against the difference in reflectance between two satellite images for the same time period. This is because when LUC change takes place, there is a significant change in the reflectance value registered by the satellite capturing the images.
Nevertheless, as commented earlier, the most common situation is that there are no reference datasets available. In these cases, the uncertainty of the LUC map series must be assessed by evaluating the consistency and the logic of the measured LUC change. The tools and techniques recommended here provide a great deal of information to the user. However, the final interpretation of the measured LUC change will be subjective, based on the user's expertise and understanding of the study area. In this situation, visual inspection can be very useful for quickly understanding many of the uncertainties in the time series of LUC maps that cannot be measured using quantitative metrics. This is why we recommend visual inspection as a first essential step prior to the validation of any LUC map or LUC modelling exercise.
Users must be aware that LUC change usually represents a very small portion of the mapped area. For a specific, not very large landscape, we would only expect a few features to change over a short period of time. In addition, the same area would not normally be expected to be affected by various successive changes. On the contrary, when an area changes, the new land use or cover tends to remain unchanged over time. In addition, there are some LUC transitions that make less sense than others. For example, one would not expect an artificial area to change to vegetation or agricultural land. These general assumptions may be adapted in line with the particular characteristics of the study area and also within the context of each element being analysed.
The same validation techniques reviewed above for single LUC maps (Sect. 3) can also be applied when comparing measured and reference changes or just for evaluating the consistency and logic of measured LUC change. However, some tools are specific to time series (Fig. 
The cross-tabulation matrix (see Sect. 1 in chapter "Basic and Multiple-Resolution Cross Tabulation to Validate Land Use Cover Maps") is the tool that provides most information about the change happening between two LUC maps. For a time series, we can compare each pair of LUC maps to find out the changes that take place at each date and the area they cover, for the map as a whole and at category level. We can summarize the main processes of change in our study area, such as, for example, the artificialization or deforestation rates for each time period. This gives us an overview of the change that has taken place over our map series and makes it easier to interpret some of the inconsistencies in measured change. Some authors also propose making a summary of all the transitions taking place, associating some of them with a default degree of uncertainty 
Multi-resolution cross-tabulation (see Sect. 2 in chapter "Basic and Multiple-Resolution Cross Tabulation to Validate Land Use Cover Maps") offers a means of checking whether some of the errors, inconsistencies or uncertainties we detect at the original resolution are not detected at coarser resolutions. When this happens, the errors and inconsistencies probably arise due to the level of detail at which the dataset was created.
The cross-tabulation matrix is an excellent source of information, which we can easily summarize using other tools and metrics. As commented in Sect. 3, Areal and spatial agreement metrics (see Sect. 2 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") and Kappa Indices (see Sect. 3 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") are used to assess the agreement between two maps. Despite their limitations, these metrics can be used to chart, in a generic way, the persistence or changes between two dates. If two maps in a series undergo the normal rate of change that we associate with any landscape, the differences between them should be slight, which means that the Kappa and agreement metrics should reflect high levels of coincidence between the maps being compared.
The Agreement between maps at global and stratum level (see Sect. 4 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") analysis could provide additional specific information about the agreement in a time series of LUC maps at whole map level, or for a given stratum, i.e. a smaller area or a specific LUC category. Accuracy assessment statistics can also be calculated for a LUC map series, either globally (see Sect. 5 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") or locally (Sect. 1 in chapter "Geographically Weighted Methods to Validate Land Use Cover Maps"). For example, when the LUC map series is obtained using a base map that is progressively updated, the first stage is to validate the base map of the series using the same procedure described earlier for validating single LUC maps. Once this has been done, we can validate the changes against a reference dataset of changes through cross-tabulation, obtaining from the resulting table the overall, producer's and user's accuracy metrics. 
Change statistics (see Sect. 1 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") 
Robert Gilmore Pontius Jr. has made major contributions to the family of validation techniques based on the cross-tabulation matrix (chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"). The LUCC budget (see Sect. 2 in chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") 
Quantity and allocation disagreement (see Sect. 3 in chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") show, at overall and category level, differences between pairs of maps in terms of category proportions due to the different allocation of the categories. Few changes are expected in a time series of maps. This means that quantity and allocation disagreement should be low and should centre on the most dynamic categories.
The number of incidents and states (see Sect. 5 in chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") 
Spatial metrics (see Sect. 1 in chapter "") and Map curves (see Sect. 1 in chapter "Advanced Pattern Analysis to Validate Land Use Cover Maps") enable us to characterize the pattern of each LUC map in the series. We do not expect the pattern of the map to vary significantly over the time period being analysed. This means that only smooth changes should be observed when comparing the spatial metrics for each of the periods analysed.
Spatial metrics that specifically measure the areas that change between pairs of maps may also be useful. In the case of a pair of maps or a time series, the detection of change on pattern borders (see Sect. 2 in chapter "Advanced Pattern Analysis to Validate Land Use Cover Maps") 

## Validation of Land Use Cover Change Modelling Exercises ##
Validating a LUCC modelling exercise is a complex task. In this case, we are not validating a single LUC map or a series of LUC maps, but a model application made up of multiple inputs, which interact to deliver new results. When validating LUCC modelling exercises, users tend to focus exclusively on the validation of the model's hard maps, i.e. maps with a categorical legend similar to the input LUC maps 
Given the nature of this book, we will be dealing exclusively with the validation of LUC maps associated with LUCC modelling exercises: input LUC maps, output soft LUC maps and output hard LUC maps. Users must bear in mind that other sources of data can be used in LUCC modelling exercises and can be validated via complementary methods.
Modellers can begin a modelling exercise by evaluating the uncertainty of the input LUC maps used in the model and their changes according to the guidelines set out in Sects. 3 and 4 above. This is because the quality of the input LUC maps can have a significant effect on the performance of the model. When setting up LUCC models, it is essential to understand the changes that take place in the set of input and reference maps. An assessment of the uncertainty of these LUC changes is therefore vital for determining and characterizing the uncertainty of the LUCC modelling exercise.
In the following subsections, we present the validation tools for output LUC maps, i.e. the products obtained by the model, differentiating between soft and hard LUC maps.

## Soft LUC Maps ##
Soft LUC maps, also referred to as suitability, change potential or change probability maps, are produced by the model to express the propensity to change over space, that is, the potential of each pixel to become a specific category in the future 
Soft LUC maps are usually validated against a reference map of changes (t 0t 1 ), and there are various methods for carrying out this analysis (see chapter "Validation of Soft Maps Produced by a Land Use Cover Change Model"). The Pearson and Spearman correlation (see Sect. 1 in chapter "Validation of Soft Maps Produced by a Land Use Cover Change Model") is appropriate for a quick assessment of the soft map, by computing it against the map of observed change 
In short, the previous three methods evaluate the relationship between the observed changed area and the soft LUC map, assuming that a good model output allocates the highest change probability values to the areas that did actually change, and the lowest change probability values to the areas that did not change. Unlike the previous methods, the total uncertainty, quantity uncertainty and allocation uncertainty indices (see Sect. 4 in chapter "Validation of Soft Maps Produced by a Land Use Cover Change Model") 
In addition to these specific indices for soft LUC maps, validation can also be conducted after reclassifying the original soft maps, so transforming continuous, ranked maps (soft) into categorical maps (hard) (see Sects. 1 and 2 in chapter "Basic and Multiple-Resolution Cross Tabulation to Validate Land Use Cover Maps"). This preliminary step enables most of the validation tools presented in this chapter to be applied for this purpose.

## Hard LUC Maps ##
The second output obtained by the model is the hard LUC map. Also known as prospective LUC maps, these are simulated LUC maps with an identical categorical legend to the input LUC maps 

## Single LUC Maps ##
The simulation (T 1 ) can only be validated against a single LUC map (t 1 ) if both maps correspond to the same year. This will also enable users to apply the panoply of tools presented in Sect. 3. The Accuracy assessment statistics, computed either globally (see Sect. 5 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") or locally (see Sect. 1 in chapter "Geographically Weighted Methods to Validate Land Use Cover Maps") could also be applied to validate the simulation against other LUC data such as ground points.
In addition to this generic list of tools, some metrics are specifically used for validating the hard LUC maps obtained from LUCCM exercises. Allocation distance error (see Sect. 3 in chapter "Advanced Pattern Analysis to Validate Land Use Cover Maps") 

## LUC Maps Series/LUC Changes ##
The most appropriate, most complete validation procedure for hard maps must include three different maps: the simulation (T 1 ), a reference LUC map for the same year (t 1 ) and the base map over which the simulation is executed (t 0 ). In other words, if our modelling exercise starts in the year 2010, we will need a base map for 2010 to establish the initial landscape on which the simulation will be calculated. Then, if we run a simulation for the year 2020, we will also need a reference map for 2020 in order to be able to compare how well our model simulates change. By comparing the simulation and the reference map we can understand to what extent the simulation matches the reference data. The changes that take place on the reference map and the simulation can be extracted by comparing them with the base map. The changes extracted from the two maps can then be compared so as to find out how well the simulated changes agree with the changes that took place on the reference maps.
There are many tools for validating and understanding the errors and uncertainties of simulated changes. In fact, all the methods and strategies explained in Sect. 4 can be applied in LUCC modelling. In this case, however, the main purpose is to achieve the best possible fit between the results of the model and the reference data.
The majority of metrics are obtained from the cross-tabulation matrix (see Sect. 1 in chapter "Basic and Multiple-Resolution Cross Tabulation to Validate Land Use Cover Maps"). The cross-tabulation matrix offers a detailed picture of the changes that were simulated (by cross-tabulating the simulation with the base map), the changes we used as a reference (by cross-tabulating the reference map with the base map) and the agreement and disagreement between the simulation and the reference map (by cross-tabulating the simulation with the reference map). The cross-tabulation matrix can also be used to summarize simulated and reference change in a series covering the main processes of change 
Cross tabulation can be carried out at multiple resolutions (see Sect. 2 in chapter "Basic and Multiple-Resolution Cross Tabulation to Validate Land Use Cover Maps") (the original and coarser ones), to find out at which resolution there is the greatest agreement. Sometimes, the simulation and the reference landscape do not agree on the details but show high consistency at coarser scales. This implies that the model is unable to simulate the precise location of the changes, but it does simulate the main patterns of change correctly.
Different metrics have been proposed for summarizing the agreement between the simulation and the reference maps that the cross-tabulation matrix shows in raw (see chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"). The Areal and spatial agreement metrics (see Sect. 2 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") could be applied to summarize the agreement between two maps of changes, the simulated and the reference change maps, overall or per category. Kappa (see Sect. 3 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") also summarizes the overall agreement between two maps. However, it has been widely criticized because it assesses the similarity between the simulation and the reference map, but does not distinguish between the areas that change between the two dates and those that do not. Therefore, in maps that simulate permanence correctly, the Kappa metric will be high. Accordingly, we only recommend Kappa for assessing how well permanence is simulated, and it should not be used for a detailed assessment of the accuracy of simulated changes. The Kappa Simulation proposed by 
The Agreement between maps at global and stratum level (see Sect. 4 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") analysis can assess for a specific LUC transition, for example, whether the agreement between an observed (reference map) and a simulated transition varies or not for several distance classes resulting from a driver (e.g. distance to roads). Other metrics, such as change statistics (see Sect. 1 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"), are widely used for characterizing the simulated changes, providing extra information that may be helpful for their validation.
Pontius proposes several metrics for validating simulated change (see chapter "Pontius Jr. Methods Based on a Cross Tabulation Matrix to Validate Land Use Cover Maps"). Some of them can also be used to validate time series of LUC maps and were therefore described in Sect. 2. The LUCC budget (see Sect. 2 in chapter "Pontius Jr. Methods Based on a Cross Tabulation Matrix to Validate Land Use Cover Maps") technique helps users to understand the changes that take place between the simulation and the base map and between the reference and the base maps. This tool calculates the gross and net changes, overall and per category, as well as the category swaps, in both the simulated and the reference landscapes. This enables us to assess in detail whether the changes we simulated are similar to the changes that take place on the reference maps and follow the same trends.
Quantity & allocation disagreement (see Sect. 3 in chapter "Pontius Jr. Methods Based on a Cross Tabulation Matrix to Validate Land Use Cover Maps") differentiates, at an overall level and per category, between the (dis)agreement between two maps in terms of the proportion of the map occupied by each category (quantities) and the (dis) agreement due to the allocation of the categories in the same/different places on the map (allocation). It is therefore useful for assessing how much of the disagreement is due to the way the model simulates quantities and how much is due to its incorrect allocation of categories. By making the analysis at the category level, it also allows us to assess where (i.e. in which categories) the errors and uncertainties arise.
If a chronological series of simulations (more than two-time points) is available, Incidents and States (see Sect. 5 in chapter "Pontius Jr. Methods Based on a Cross Tabulation Matrix to Validate Land Use Cover Maps" may also be employed. This metric helps identify pixels that follow illogical transition patterns, with changes at successive time intervals between the same pair of categories (e.g. from agricultural to urban fabric and then back to agricultural).
Intensity analysis (see Sect. 6 in chapter "Pontius Jr. Methods Based on a Cross Tabulation Matrix to Validate Land Use Cover Maps") compares the different intensities of change per category in simulations and reference maps over at least three points in time. In this way we can assess whether our model correctly simulated the change trend displayed by the reference data. The flow matrix (see Sect. 7 in chapter "Pontius Jr. Methods Based on a Cross Tabulation Matrix to Validate Land Use Cover Maps") could also be applied to validate simulated changes in a generic way, assessing the stability and instability of the real and simulated changes over time.
The Null model 
The Merit can be used to discover whether the model estimates more or less change than the reference map. It is also highly recommended for evaluating the congruence of model outputs and model robustness. This is a form of validation that evaluates the agreement between simulations obtained using different models or using the same model parametrized in different ways 
None of the above tools assesses the accuracy of the pattern of LUC change in the simulation. This aspect is important because even if the quantities simulated are wrong and the categories are not allocated in the same positions as in the reference maps, the pattern of LUC change may have been simulated correctly. Pattern can be validated using Spatial metrics (see Sect. 1 in chapter "Spatial Metrics to Validate Land Use Cover Maps") and the Map Curves (see Sect. 1 in chapter "Advanced Pattern Analysis to Validate Land Use Cover Maps") method, which compare the pattern of the simulation with the pattern of the reference landscape.
Spatial metrics characterize many different elements of the landscape: fragmentation, shape complexity, category proportions, diversity…. They can be calculated specifically for the simulated and reference changes, so allowing users to identify the specific pattern characteristics of the features that changed during the simulation period. In this way we can understand the size and shape of the simulated changes, inferring from this information how logical or uncertain they may be.
The MapCurves method gives a summary figure for the pattern agreement between two maps, and is therefore much easier to interpret. However, it does not provide all the complex detail that can be revealed by applying the different spatial metrics.
We can also analyse the changes that take place on the borders of existing patches and the changes that result in the appearance of new patches. This distinction may be useful for identifying errors or inconsistencies. The detection of change on pattern borders (see Sect. 2 in chapter "Advanced Pattern Analysis to Validate Land Use Cover Maps") enables us to evaluate and identify errors in the simulations, which may be due to different parameters being applied in the model allocation procedure, such as, for example, the use of a contiguity filter. The Allocation distance error (see Sect. 3 in chapter "Advanced Pattern Analysis to Validate Land Use Cover Maps") calculates the distance between wrongly simulated patches and reference patches, so as to gain a better picture of how well the patches are simulated. In this sense, a model that wrongly allocates change close to areas that actually change on the ground would be considered to have performed better than a model that allocates them further away. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## Introduction ##
Nowadays, there are many sources of Land Use Cover (LUC) data. The availability of LUC data has been increasing since the end of the last century, in line with the development of remote sensing techniques and easier access to aerial and satellite imagery. LUC data is available at all spatial scales, from local to global. Access to spatial information, including LUC datasets, has also improved in the last decade with the development of the open access culture.
Most of the LUC data being produced today refers to LUC maps, which are either single, one-off maps or form part of a time series. These maps provide layers of spatial data with LUC information for each part of the area being mapped at one (single maps) or several points in time (series of maps). Other spatial sources of LUC information include reference datasets used to validate LUC maps or train remote sensing classifiers. Although datasets of this kind have been produced since the beginning of the satellite remote sensing era, they have only recently become widely available for general purposes.
In this chapter, we review the main producers of LUC maps and the most relevant LUC datasets currently available -both LUC maps and data packages with reference data. Although this aspires to be a comprehensive review, some LUC products may be missing. We focus on the datasets that are available for download and can be used in practice. When relevant, we also mention others that are currently unavailable for download.
Many older LUC maps are not included, because they were drawn at very coarse resolution using old-fashioned production methods and therefore cannot meet the demands of modern users. Because of the scope and extent of the book, we focus exclusively on datasets at global and supra-national levels. A detailed description of the approach followed when carrying out this review appears in chapter "About This Book" of this book.
The most important datasets reviewed in this chapter are described in detail in Part IV of this book (chapters "Global General Land Use Cover Datasets with a Single Date "-"Supra-national Thematic Land Use Cover Datasets"), where users can find a detailed description of each dataset, including classification schemes, production methods and download options.

## The Producers of LUC Data ##
We have classified LUC data producers into four main groups (Fig. 
At local and detailed scales, many organizations and users create their own LUC datasets. The fact that they have easy access to aerial/satellite imagery and to software for processing, photointerpreting and classifying these images has facilitated this process. This allows users to obtain very specific datasets that match their particular requirements. The datasets created for small projects and for specific purposes are not usually disseminated and remain the property of the communities or users that produce them. When these datasets are made available, they are often provided without the necessary technical information and general metadata.
At regional, national, supra-national and global scales, an increasing number of LUC databases are being produced for a broad range of users. Often these databases are specially designed for specific communities, such as the climate change research community. In other cases, they provide more general LUC information for a wide range of research fields and as support for policy decisions.
There are two main producers of LUC datasets. Firstly, nationally or internationally funded research projects, which Fig. 
Depending on the specific objectives of the projects and the institutions involved, these datasets may or may not be available for download. The quality of metadata and auxiliary information can also vary a lot from one project to the next. In some cases, a lot of technical and auxiliary information is provided, while in others users can only access the dataset itself and the research paper in which it is presented.
Governmental and other organizations are the other big producers of LUC data. In these cases, the objective is to provide information about the areas for which the organization is responsible or the areas affected by its policies and/or decisions. This data is a useful source of information for the policymaking process and is usually part of wider cartographic efforts by national and regional governments, and sometimes by international organizations, to provide geographic information of reference.
As these projects are part of official mapping work conducted by nations, regions and other large organizations, they are usually backed by significant long-term funding. These databases are therefore more likely to be updated or improved in the future. Another advantage is that they usually provide highly detailed, accurate information. They are also quite flexible. As a result, these databases are widely used by the whole scientific community, public and private sector professionals and many other users.
In recent years, there has been an increase in the data produced by members of the public through crowdsourcing or similar practices. This kind of information is known as Volunteered Geographic Information (VGI) and is part of a movement called 'citizen science', in which private citizens participate in scientific research, either by gathering or validating data or by assisting in any of the other phases of the scientific process.
Approaches of this kind allow local knowledge and expertise to be incorporated into data production. Highly detailed, up-to-date datasets can be produced easily and cheaply. Nevertheless, important issues can arise in terms of data quality and uncertainty, due to possible inconsistencies in the methods and procedures followed by the contributors, their different levels of expertise, etc.

## Land Use Cover Maps ##
Reviewing all the LUC maps currently available is a daunting task, which perhaps explains why it has rarely been attempted. To our knowledge, the only researchers to carry out an extensive review of LUC maps at global and regional scales were 
The dividing line between general and thematic LUC products is not always clear. Some LUC maps, for example, provide general information on several different land covers (e.g. artificial, vegetation, water) while providing a detailed study of just one of them, thereby adopting a thematic approach. Although, in our review, we classify LUC maps as either general or thematic, readers should be aware of these possible inconsistencies.
Both types of LUC maps, general and thematic, can also be classified according to the extent they cover, differentiating between global, supranational, national, regional and local LUC maps. However, a comprehensive review of national, regional and local maps would be a huge task that is beyond the scope of this book. We will therefore be focusing exclusively on global and supranational LUC maps.
LUC maps for national and, especially, for regional and local areas, are usually only available for developed countries, or even highly developed countries, which can afford to invest in the production of spatial information and in research programmes. The most developed nations of the European Union, Australia and the United States usually have detailed LUC datasets, not only at a national level but also for specific regions. In China, the government has invested heavily in research, so enabling the production of national and regional LUC products. China is, together with the USA, the country producing most research on LUC mapping today 

## Platforms and Repositories ##
A few online platforms and repositories provide an overview of the LUC datasets available. The Geo-Wiki platform (www.geo-wiki.org) is one of the most recent. It was initially developed to collect reference LUC information through crowdsourcing and to create a hybrid LUC map. It now hosts both general and thematic LUC maps. The Google Earth Engine Platform, which was also recently launched, includes a repository of spatial datasets, with a specific section devoted to Land Cover data (https://developers.google.com/ earth-engine/datasets/tags/landcover).
The FAO Geonetwork repository (www.fao.org/ geonetwork/) makes a great deal of spatial datasets available to users. The repository includes a specific section on LUC data. It hosts LUC maps at all scales and is a valuable source of LUC information for developing countries. The Land Processes Distributed Active Archive Center (LP DAAC) (https:// lpdaac.usgs.gov/) holds most of the LUC datasets produced by NASA and the United States Geological Survey (USGS), in addition to other important global datasets.
The Copernicus Land Monitoring System website (https://land.copernicus.eu/) is the main source of LUC products created through the Copernicus programme, and is of particular interest for those working with European LUC information. All Copernicus layers are also available through the WEkEO Copernicus DIAS service (https://wekeo.eu/), a cloud-based platform that provides access to Copernicus datasets and to various tools for processing them, including all the land monitoring data.

## General Land Use Cover Maps ##


## Global LUC Maps ##
The production of global LUC datasets started at the end of the twentieth century. By then, coarse-resolution satellite imagery was available for producing consistent global LUC datasets at a low cost. A previous attempt had been made to create a global LUC map through photointerpretation of aerial imagery 
The first global general LUC map of which we have record dates from 1994 (Table 
The next global LUC maps were also produced by the team from Maryland. These were an improvement on their original map. Two maps were produced at spatial resolutions of 8 km and 1 km, respectively 
A lot of new maps have been produced since these first global general LUC maps appeared, especially since 2010. Tables 
As in the case of the pioneering maps from the University of Maryland, all the datasets reviewed here have been Most of these datasets are intended for use in climate change modelling, for which coherent global LUC maps at coarse resolutions are required. However, these databases are becoming increasingly popular and are used for many other purposes, a lot of them related with land change. This has been one of the drivers promoting the creation of new maps, with better quality and higher detail.
Below, we characterize the global LUC datasets produced in the last decades according to their method of production, level of accuracy and spatial, temporal and thematic resolutions. Over this period, map production methods have becoming increasingly complex in order to create more accurate maps that provide better spatial, temporal and thematic information.

## The Production Methods ##
Nowadays, global LUC maps are created using improved and innovative production methods, involving advanced classifiers, such as those based on machine learning, as well as a lot of auxiliary data. In many cases, specific LUC categories are mapped through several specific procedures due to their particular patterns, reflectance behaviour, etc. Additional post-classification treatments have also become common in a bid to avoid some of the uncertainties and errors associated with the production of these maps.
In recent years, due to the increasing availability of LUC datasets, more and more global LUC maps are being produced by data fusion, in which new maps are created by combining existing datasets using a range of different algorithms and approaches. The aim of these projects is to create datasets with higher levels of accuracy and, therefore, less uncertainty. To this end, they usually combine the most accurate or highest quality LUC information from each dataset.
FAO-GLCShare is perhaps the best-known example of an attempt to build a new global LUC map from data fusion. It was created in 2014 by merging high-quality detailed national and regional LUC databases 
LUC maps obtained from data fusion do not have a single specific date of reference for the mapped area. When first produced, they are considered as up-to-date LUC databases. However, if they are not updated frequently, they eventually become obsolete and can no longer be regarded as useful sources for LUC change analysis.
The maps obtained through crowdsourcing, i.e. by aggregating a large number of individual inputs supplied by a community of people, could undergo the same problems. Although still relatively rare, they could play an important role in the future. OSM-LULC, released in 2017 
The recent advent of the Google Earth Engine (GEE) platform has encouraged the production of new global LUC maps, some general and others thematic. GEE provides a powerful cloud computing service, giving users the chance to process and classify tons of satellite imagery. This is particularly important when users do not have the necessary computer power to do this themselves. The availability of cloud-computing services will lead to an increase, in the near future, in the number of highly detailed LUC products being created using complex computer production methods. Many of these will be produced at global scales.

## Accuracy ##
The development and application of new methods and techniques to produce LUC maps has not improved the accuracy of these datasets. Although some global LUC maps are more accurate than others, there is no correlation between time, the introduction of new methods and techniques and the achievement of higher levels of accuracy 
Global LUC datasets usually have accuracy levels of over 60%. In the best cases, they are around 80%. They are therefore still subject to high degrees of uncertainty. This is to be expected given the high level of abstraction they require. The entire surface of the Earth is being mapped according to the same method and must fit into the same legend. This means there is little room for local or regional specificities, which inevitably introduces a degree of uncertainty.

## Spatial Resolution ##
LUC mapping has evolved over time, with the result that global LUC maps are produced at an increasing number of spatial resolutions. Initially, the AVHRR and VEGETA-TION sensors, with a spatial resolution of 1 km, were the main source of imagery for global LUC mapping. Later, imagery from MODIS (500 m) and MERIS (300 m) became the standard source of information. In recent years, it has become increasingly common to use the huge stock of Landsat imagery to produce global LUC maps at 30 m. Some projects have gone even further, producing global LUC maps at even finer resolutions. One example is the 2017 edition of FROM-GLC (10 m) 
Sentinel satellites will be providing free, long-term, high-quality imagery over the coming years. This may boost the production of global LUC maps at increasingly high levels of detail.

## Temporal Resolution ##
The temporal resolution of LUC maps has also increased over time, especially in recent years. Historical time series of LUC maps are becoming more common (Table 
However, in most of these series, LUC change cannot be reliably detected by cross-tabulating the different maps that make up the dataset. Different methods of production for each year, changes in the source of imagery, differences in the reflectance of the images, etc., introduce a lot of noise in the comparison. This makes it impossible to obtain meaningful results from LUC change analyses.
The latest version of the MODIS Land Cover (Collection 6) incorporated important changes in the product algorithm and workflow to account for these sources of uncertainty 
New time series of LUC maps have been produced recently with the specific purpose of enabling change detection. These include the LC-CCI (ESA 2017) and GLASS-GLC maps 

## Classification Schemes ##
Unlike the spatial and temporal resolutions, there are no important variations over time in the thematic resolution of most global LUC products. In fact, standard LUC classification systems are now widely used so as to ensure that the different databases are comparable. One of the most common is the International Geosphere-Biosphere Programme  

## Supra-national LUC Maps ##
A lot of international institutions and organizations need comprehensive and coherent worldwide data to support their activities. Global datasets are also required by research communities that study the whole Earth as a system. For their part, national governments and organizations require large amounts of data to support policymaking at a national level. Many other institutions, associations, professionals and researchers need very detailed data that is only available at regional and local scales.
Within this context, supra-national datasets do not provide much detail and work at a different scale to that at which most institutions and organizations implement their policies. They therefore do not meet the requirements of the research and policy-making communities working at global scales. This means that there is less interest and consequently less funding for datasets at these scales, hence the relative lack of supra-national LUC maps. Supra-national LUC maps have been developed by the European institutions to assist policymaking and environmental monitoring in Europe. In other continents, supra-national LUC maps are usually developed within the context of different projects funded by international institutions, such as the FAO and various different US and European institutions. The latter include the European Space Agency (ESA) and the Joint Research Centre (JRC) of the European Commission, which have been actively involved in the production of supra-national LUC maps for many developing areas with important biodiversity values.

## Europe ##
Europe is the continent with the widest range of supra-national LUC maps. The European Union (EU) has certain powers over the European environment and is therefore interested in monitoring any changes in land use. To this end, the EU has invested in the production of EU-wide reference data as a reliable source of information on which to base their policy decisions. As a consequence, plenty of detailed, high-quality datasets are now available providing LUC information for the European continent (Table 
Of all the European LUC datasets, CORINE Land Cover (CLC) is by far the best known. It is one of the oldest and most successful programmes on land monitoring, offering very high levels of accuracy and detail. All these qualities have made CLC a reference in LUC mapping worldwide. It is the only cross-country initiative working at similar scales that provides detailed, temporally rich LUC data, which can be used effectively for change detection. CLC is one of the best examples of decentralized, coordinated LUC mapping. CLC is produced at a national level, which allows European countries to develop their own national datasets while taking advantage of the work and the resources invested to create CLC.
A few non-European countries have mapped the land uses and covers in their entire nations or in certain specific areas following the CLC model. Some of them have done so with the help of the European institutions and other European research groups. These include Palestine, Morocco, Tunisia, San Salvador, Guatemala, Honduras, Haiti, Dominican Republic, Colombia, Burkina Faso and Gabon 
Through the Copernicus programme, the EU has also developed coherent and consistent LUC mapping products aimed at monitoring the LUC dynamics of specific areas (e.g. coastal and metropolitan areas, riparian zones, Natura 2000 network…). These are very detailed products in both spatial and thematic terms, which have been designed to meet the needs of their potential community of users or to provide information in support of a range of different policies. Their production is centralized, so avoiding the inconsistencies that might result from a coordinated, decentralized production method. Although they were only recently launched, the EU has assured their long-term continuity, so providing consistent time series of data.
Two other series of LUC maps, which are complementary to CLC, are also available for Europe. Annual Land Cover is a recently launched product that provides annual LUC maps, so overcoming the temporal resolution limitations of CLC, which is only updated once every 6 years. Annual Land Cover is produced as part of a project funded by the European Commission, which aims to create harmonized spatial datasets for Europe. However, it is not recommended for change detection, as there is a lot of inter-annual variability between LUC covers.
HILDA is another LUC dataset providing a long time series of LUC maps for Europe. Although it has a coarser resolution, it provides the longest time series of maps reviewed here: 1900-2010. It was produced by a research project team, who combined various different datasets and applied complex modelling techniques 

## Africa ##
A large number of supra-national LUC maps have also been found for Africa (Table 
Only a few projects tried to offer an overview of the LUC covers for the entire African continent. The FAO mapped the covers for many African countries as part of the AFRI-COVER project, but did not encompass the whole continent. The first comprehensive, Africa-specific, general LUC dataset only appeared quite recently. It was produced by EU research and earth-observation organizations. No similar initiatives have been found for America, Asia and Oceania. They are also quite rare for Europe as a whole, where continental LUC data usually covers the EU and associated countries.
There are three datasets providing a time series of LUC maps for different African countries. However, only one of these (West Africa Land Use Land Cover) was obtained by applying a common mapping approach which provides LUC information for all mapped areas at the same dates. In the other two, the time series is made up of national or regional LUC maps produced for different years of reference, so hampering cross-country LUC change analyses.

## The Americas ##
In the Americas, there is a clear distinction between the datasets covering North America and those covering South America and the Caribbean (Table 
South America 30 m, developed by 

## Asia and Antarctica ##
We only found one supra-national dataset for Asia, which covered the LUC of the Himalayan region (Table 
No supra-national maps are available for Oceania, due to its particular characteristics in which continental areas and islands are usually separate individual nations. These countries have no shared continental or inland regions for which a supra-national LUC dataset might be useful. As a result, no datasets of this kind have been produced.
Finally, a specific LUC map for Antarctica was produced recently by Chinese researchers 

## Thematic Land Use Cover Datasets ##
Thematic Land Use Cover (LUC) datasets map parts of the Earth's surface as a specific land cover, considering not just its extent but also its intensity of distribution. They normally focus on land covers and provide very little information about land use. Thematic LUC maps are usually produced using automatic remote sensing techniques that find accurate land use characterization difficult. Thematic LUC maps usually represent land covers in greater detail than general LUC maps. Some provide information about the proportion of the study area occupied by a particular land cover on the ground. In other cases, they delineate the extent of a specific cover with great detail and accuracy. Other thematic LUC maps share certain features with general LUC maps, in that they map the Earth according to a set of predefined categories, which are usually subclasses of a specific type of cover (e.g. vegetation). Many maps charting vegetation in its various different forms can therefore be regarded as thematic sources of LUC information in that they characterize a specific cover.
Some maps may provide thematic information about specific land covers together with other relevant data. This was especially true in the twentieth century, when many different maps combining biogeographic and climate information were produced for the climate and other research communities. These maps were usually produced by merging different techniques and datasets. Examples include the maps produced by 
Prior to the advent of satellite remote sensing, there were also a large number of traditional maps obtained through photointerpretation of aerial imagery and field surveys that provided information on certain specific land covers. These maps charted vegetation above all and, to a lesser extent, agricultural areas. These can be useful sources of information for historical LUC change analysis. However, as they are usually only available for national or more detailed areas and in many cases have not been digitalized, they are not reviewed here either.
There are also plenty of other spatial datasets that provide useful information for studying specific land covers. One example for vegetation covers are maps of live biomass 
The progress made in recent decades in the production of general LUC maps has also been achieved in thematic LUC mapping, with increasing levels of detail and more innovative, more complex methods. Some of the newest products have been produced using the cloud-computing capabilities of Google Earth Engine, which seems likely to play a key role in thematic LUC mapping in the future, and will allow more thematic datasets to be produced. Until now, the Landsat archive has been the most detailed source of imagery for LUC thematic mapping, although the imagery provided by the Sentinel constellation of satellites will soon enable users to expand the catalogue of thematic LUC datasets at highly detailed spatial resolutions of less than 30 m.

## Global Thematic LUC Maps Focusing ##
on Vegetation Covers One of the most common features mapped by thematic LUC products is natural vegetation and tree and forest covers in particular. In fact, forest monitoring is one of the main applications of Landsat data, as reviewed by 
LUC maps focusing on vegetation covers usually offer coherent time series of LUC data that support change detection (Table 
VCF datasets provide information about the vegetation cover fraction for each pixel in the analysed area. FCover is the only dataset that provides information on the percentage of vegetation cover, whereas all the others focus on tree or forest covers. Whereas FCover considers all kinds of natural vegetation, MEaSUREs VCF (VCF5KYR), MODIS VCF (MOD44B), Landsat VCF (GFCC) and the Hansen Forest Map focus exclusively on tree covers. In addition, GFCC and Hansen Forest Map include specific layers of forest change. Forests are mapped as such when a minimum fraction of their area is covered by trees. Therefore, changes in tree cover changes do not necessarily mean forest changes.
Two recent projects have explored the potential of radar data for mapping forest extent 

## Global Thematic LUC Maps Focusing ##
on Agricultural Covers Agricultural areas are also widely mapped with specific LUC products (Table 
Unlike other LUC thematic products, those mapping agricultural areas do not usually offer a time series, which means they cannot be used for land change analysis. Mapping agricultural areas is quite complex and this has hindered the production of coherent time series of agricultural LUC maps. One exception to this general trend was the dataset by 

## Global Thematic LUC Maps Focusing ##
on Artificial Covers Built-up areas are becoming a common subject for thematic LUC products. As with the datasets focusing on vegetation covers, they provide time series of data which support change detection (Table 

## Supra-national Thematic LUC Maps ##
We have only reviewed a few experiences of supra-national thematic LUC mapping (Table 
The European Commission, through the Copernicus programme, is behind some of the few supra-national thematic LUC datasets that focus on other covers such as artificial surfaces or agricultural areas.

## Reference Land Use Cover Data ##
Reference data is required to train supervised remote sensing classifiers and to validate LUC maps. Reference LUC datasets consist of a series of geographically distributed sample points with LUC information. Each point contains information about the specific land use or cover in the pixel or polygon of the Earth's surface represented by the point.
The reference datasets are subject to the same spatial abstraction required in LUC maps. Reference points are associated with a specific pixel or polygon. The level of abstraction required varies depending on the size of these points. The uncertainty of the reference information will also vary accordingly. The fact that a single land use or cover is assigned to a whole pixel or polygon, even though they may contain other land uses or covers, can also produce uncertainty. In addition, there is always a degree of subjectivity in the decision to assign a pixel or polygon to a particular category, especially in borderline cases that are not clear-cut. This can create an additional source of uncertainty.
Relatively few general LUC reference datasets are currently available. This is because many reference datasets were created ad hoc every time a new LUC map was validated or reference data was required to train a remote sensing classifier, and it was therefore unnecessary to have a ready supply of general LUC reference datasets. These datasets are also affected by some degree of thematic generalization, as is any LUC map. LUC information must conform to a specific classification system or legend. Given the ad hoc nature of many reference datasets, the classification or legend used to classify the land uses and covers was normally also case-specific. However, the recent emergence of standard LUC reference datasets aimed at a wide range of users and research fields has extended the use of standard legends and classification systems, such as the FAO LCCS, when drawing up these datasets. One of the most renowned LUC reference datasets is the Land Use Cover Area frame Sample (LUCAS), produced by EUROSTAT every 3 years since 2006. It is made up of more than 330,000 survey points across the EU. 
In recent years, various reference datasets used to validate and train classifiers of global LUC maps have been made available online, so enabling them to be used for other purposes rather than just in the production of one specific map. The work done by the team from the GOFC-GOLD Land Cover Office is of special note. They collected and improved the reference datasets from six different LUC products (GLC2000, GlobCover 2005, STEP, VIIRS, GLCNMO and the urban dataset from the University of Tokyo). Samples of these datasets (with up to 70% of all the available reference points) are freely available for download on the project website. 
The most famous of these initiatives is Geo-Wiki, which is frequently used to collect LUC information for calibration and validation practices. Geo-Wiki provides a user-friendly online tool that makes it very ease to visualize LUC maps and to collect the reference LUC data required to validate them. Many international research projects working on LUC mapping and citizen science have based their research on Geo-Wiki. One of the most important is the H2020 Land-Sense Citizen Observatory.
Many other tools and platforms have been developed in recent years with similar purposes: Collect Earth, GLFC LT, VIEW-IT… 
Although they cannot be considered LUC data as such, volunteered geo-referenced photographs may be useful for obtaining reference LUC datasets. They provide a fixed picture of a landscape at a given point in time. By analysing the picture, users can identify the dominant land cover or land use, so obtaining LUC reference data.
Several initiatives for collecting volunteered photographs of specific geographic locations are already ongoing. Flickr is one of the most famous, although its purposes and objectives have little to do with science or scientific methods. The Degree Confluent Project (DCF)

## Further Reading ##
Fonte CC, Bastin L, 
See L, 
Tsendbazar NE, de Bruin S, Herold M (2015) Assessing global land cover reference datasets for different user communities. ISPRS J Photogramm Remote Sens 103:93-114. https://doi.org/10.1016/j.isprsjprs.2014.02.008
The paper compares and analyses 12 LUC reference datasets in detail. These datasets are used in the production and validation of global LUC maps. This is one of the most comprehensive reviews of the LUC reference datasets currently available. It also assesses the potential reuse of these datasets, focusing on the data requirements imposed by different user communities. The authors try to identify the particular features that LUC reference datasets must have to enable them to be used by a wide range of users. A long but detailed reflection on the progress that has been made and the changes in Land Cover mapping since the appearance of remote sensing.

## D. García-Álvarez and Sabina Florina Nanu ##
Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http:// creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## Part II Data Access and Visualization ##


## Visualization and Communication of LUC Data ##


## Francisco Escobar ##


## Abstract ##
The increasing number of disciplines and public and private sectors interested in land use/land cover (LUC) information has boosted the demand for and the production of related cartographic products. However, the communicating power of the final maps may be impaired, if any of the cartographic transformations performed during the mapping process does not adapt well to the particular subject or area being mapped. This chapter takes the reader on a guided tour through the map production process, offering an overview of the cartographic language, the rules and practices that contribute to the success of the map as a communication tool and the most common forms in which LUC maps appear. Recent developments in geovisualization tools applied to LUC are also discussed.

## Introduction ##
The main purpose of cartography is to communicate geospatial information. The map serves as a channel through which a message is transmitted from the sender-the mapmaker-to the receiver-the map user 
Like any other communication tool, cartography possesses its own language. The term "language" has been used by a number of authors in this field and can be defined as a system of signs enabling communication 
Subsequently, various studies explored this concept in greater depth, culminating in 1967 with the seminal piece by Jacques Bertin "Semiology of Graphics", a genuine world reference on this subject. This was followed in 1978 by Ratajski, who outlined that, in modern thematic cartography, the ultimate goal of semiotics is to build an accurate, unambiguous cartographic language.
In cartography, semiotics unfolds as two different categories of signs; on the one hand it refers to geometric signs, the spatial dimensions (zero, one, two or three) and the geometric nature of map features (points, lines, polygons and volumes), and on the other, to visual variables, defined as the possible elementary variations in perceptible marks 
In this chapter we will be focusing on both kinds of signs and their role in the cartographic representation of land use/land cover (LUC).
Recent technological advances in the GIS industry have popularized cartography, giving rise to what some people refer to as a "geospatial society" in which maps are increasingly ubiquitous and used in all kinds of applications. This has brought new opportunities for cartography as a science but it also poses new challenges, one of which is that many new mapmakers lack the necessary cartographic skills to produce effective maps. Unfortunately, there are numerous examples in the literature that illustrate the fact that GIS has made it easy to produce large numbers of wrong or F. Escobar (&) Departamento de Geología, Geografía y Medio Ambiente, Universidad de Alcalá, Alcalá de Henares, Spain e-mail: francisco.escobar@uah.es confusing maps more quickly than ever before. In the case of LUC mapping, no matter how sophisticated and expensive the technology for the collection and processing of the information may be, inexpert mapmakers often fail to communicate the relevant information correctly.
In order to help overcome these issues, this chapter aims to provide the basic ground rules for the correct representation and interpretation of LUC maps.

## Geometric Signs ##
The geographic entities we find in the landscape are portrayed on maps as cartographic objects of varying geometric nature. Different land use areas are no exception and are usually depicted as polygons. The process for representing this information on a 2-dimensional piece of paper or on a screen is anything but simple as it involves, at least, the following transformations; (1) projecting the irregular and curved surface of the Earth on a plane, ( 
(3) aggregating the information at the right administrative level when analysing LUC distribution over statistical spatial units. These three transformations have important implications for LUC mapping, which we will now go on to explain.

## Cartographic Projection and LUC Mapping ##
The representation of our curved planet on a 2-dimensional map requires the application of mathematical models, known as cartographic or mapping projections, to project the Earth's surface on a plane 
• Conformal projections are used in navigation charts, as their main characteristic is the preservation of angles. Parallels and meridians intersect in a perpendicular manner, so forming four 90º angles at each intersection and an orthogonal network as a whole. However, these maps show important distortions in terms of the proportionality of areas and distances. • Equidistant projections preserve the distances between specific pairs of points and distort areas and angles. These kinds of projections are mainly used in engineering and construction works. • Equivalent or equal area projections preserve the proportionality of areas and by doing so distort the shapes and distances.
The bigger the area represented, the greater the impact of our choice of projection. This is noticeable in world maps where familiarity with the shapes of countries and continents make it easy for the reader to understand the deformations in each case. However, in smaller areas whose shape is not usually familiar to the general population, the map reader will find it difficult to notice the deformations. Of course, given the limited portion of the Earth's surface portrayed, the effects of the deformations are not as obvious as in world maps, but they do exist and can have an impact on LUC mapping. Since the choice of the projection results in significantly different maps, as Fig. 

## The Minimum Mapping Unit in LUC Maps ##
The minimum mapping unit, or MMU, defines the size of the smallest cartographic object that will appear on the map 
Today, the predominance of digital maps over paper-based maps and their capacity to zoom in and out mean that the MMU is not as obvious as in the past. However, all maps are affected by the mapmaker's choices regarding their final scale, and the MMU has to be set in such a way as to facilitate the useability and readability of the map. In digital maps, the zoom feature may incorporate 'intelligent' functions, which allow it to display certain map elements, features and labels, solely at the appropriate level of zoom. The result is that when the user zooms out, the smaller features are hidden and when they zoom in again, more and more small features become visible. For the intelligent zoom to work properly, the mapmaker must establish a different MMU at each zoom level, in this way deciding which elements will be visible at each different scale, an important decision in the mapmaking process.
CORINE Land Cover is a well-known European project, which established an MMU of 25 hectares for areal entities and a minimum width of 100 m for linear features (European Environment Agency 2017). This means that in a printed map at the recommended working scale of 1:100,000 the MMU will occupy 0.5 cm 2 or 25 mm 2 .
The MMU also plays an important role in the data collection phase. Regardless of whether data is collected by field work or by interpretation of aerial or satellite imagery, the features that are smaller than the MMU will not appear on the map.
Some authors work almost exclusively with raster structures for which the pixel is the basic unit. As a result, they tend to conceive the MMU in terms of pixel size. From this perspective, it is generally accepted that the smallest observable feature in the final map, i.e. the MMU, should comprise at least four contiguous pixels 
When it comes to determining the MMU of LUC maps, it is important to differentiate between databases and maps. Patches that might be a suitable size for data analysis could be completely inappropriate for map publishing. Single pixels or small groups of pixels forming small areas below the MMU threshold might be considered in data analysis, but would not appear on the map.
Three intrinsic characteristics of LUC mapping must be taken into consideration when deciding the most appropriate MMU: (i) Confusion between use and coverage, (ii) Definition of land use categories and associated land size, and (iii) High sensitivity of LUC maps to the interrelations between MMU and scale. The scale at which LUC information is expressed also has an enormous impact on the communication capacity of the resulting map 
In what is a common confusion between land use and land cover, different MMUs can result in maps showing different categories. For instance, at a relatively coarse resolution, a MMU of 1 km 2 would lead to an airport being depicted as such in both a land use map and a land cover Conic (equidistant). For demonstration purposes only, the differences between (d) and (e) have been accentuated by applying a World and a European projection system respectively map. However, if we increase the resolution by reducing the MMU to 50 m, the land use map would still depict it as an airport, but the land cover map would classify the areas covered by runways, buildings, or green areas into different categories.
The second characteristic of LUC information that affects the MMU is directly related to the first. The increasing availability of Earth observation products with greater spatial resolution could lead to the false idea that the higher the resolution of the images, the better the quality of the data obtained from them. However, land use, i.e. the "arrangements, activities and inputs people undertake in a certain land cover type to produce, change or maintain it" (Di Gregorio and Jansen 2000) cannot be observed in areas smaller than that required to carry out said activities and arrangements. For instance, the MMU for a LUC map category representing low-density residential development must be at least as small as the basic unit (house with garden) for this kind of land use.
The third intrinsic characteristic of LUC information that impacts on the MMU is its nature as a covering phenomenon. Mapping LUC information involves the delimitation of areas showing homogeneous coverage. This poses a problem in the data collection phase of small-scale LUC maps, in which the MMU covers a significantly large area that probably includes several LUC categories. In these cases, the identification of homogenous areas becomes a much more complex task. In order to assign a single value to the area in question, the cartographer must apply one of the available criteria. The most frequently used criteria include allocating the area: (i) to the LUC category covering the largest proportion of the area or (ii) to the predominant LUC category in the surrounding area. Related issues arise when attempting to downscale or upscale previously existing geospatial information. This increases the uncertainty of the map 

## The Modifiable Areal Unit Problem (MAUP) and the Category Aggregation Problem (CAP) ##
LUC can be mapped and conceptualized in different ways; from the most typical LUC maps in which the areas are classified into homogeneous categories, to choropleth maps which summarize, at selected administrative levels, different statistical values for the LUC they contain. In all cases, LUC information is expressed via polygon-based geometry but the MAUP is most noticeable in choropleth maps.
The MAUP was analysed in depth by Openshaw and Taylor (1979) and its effects have been tested in a number of research studies 
LUC mapmakers and users need to be aware of the impact of the MAUP in order to facilitate both successful communication and well-informed decision-making.
Another issue in relation to the downscaling of information is the Category Aggregation Problem (CAP), which was formulated more recently 
In LUC these constraints are key aspects in the correct production and analysis of related maps. Figure 

## Visual Variables ##
The expression 'visual variable' was used by J. 

## Shape ##
Shape is the first variation distinguishable on any map. It helps identify the different types of objects appearing on a map, which are described by different contours. These contours may be regular and abstract (geometric signs) or figurative (pictograms). Shape corresponds to a nominal level of measurement and only allows us to convey either associations between objects with the same shape or differences between elements represented by different shapes. Shape is neither ordinal nor quantitative and cannot therefore be used for thematic phenomena with ordinal or quantitative levels of measurement 
In LUC mapping as in any other kind of polygon-based mapping, shape can only affect filling patterns, not the shape of the polygons themselves. The only exception to this rule are cartograms, in which both the size and the shape of polygon objects vary in line with quantitative thematic values. In maps showing point and line features, shape is frequently used to highlight different associations between categorical objects.

## Orientation ##
The orientation of a sign refers to its position relative to a reference framework and it is expressed in degrees (between 0 and 360). As with shape, orientation can only represent the attributes on a nominal level of measurement and can only affect point-based elements 

## Colour Hue ##
Colour hue (often referred to simply as colour) is the most complex visual variable and its use in maps has been extensively analysed by cartographers 
As a visual variable on a map, unlike shape and orientation, colour can be used not only in points, but also in lines and polygons. As regards its properties in relation to thematic information, colour is selective, separative and associative. Colour hues are neither ordered nor quantitative, which means they cannot be used to represent attributes measured at quantitative scales, and are therefore only suitable for representing phenomena measured at nominal scales. However, under certain conditions and when arranged in the appropriate order, colours can also be used to express order and opposition. For instance, yellow, orange and red can represent low, medium, and high data values, respectively (White 2017). In addition to Bertin's pioneer work and the revisions to his visual variables made by subsequent authors, a milestone in the application of colour hue schemes in digital mapping is the ColorBrewer Tool developed by Cynthia Brewer at Penn State University (Brewer 2021). The ColorBrewer tool offers an extensive collection of colour ramps, which are well-suited for any measure of scale and for colour-blind map users. In terms of LUC mapping, an interesting proposal for colouring LUC maps with coarse pixel data can be found in 
The use of colour in mapping is also affected by its cultural connotations. As pointed out by 
In addition to these cultural constraints, for map communication to be successful, the use of colour in mapping must honour some generally accepted conventionalisms. In LUC mapping, for instance, water bodies are always represented in light blue, while residential areas are normally depicted in red.
A very useful, well-known colour scheme for LUC mapping was established by the European Environmental Agency in the Corine Land Cover project (EEA 2017). Its 44 categories are represented by colours whose different hues are assigned to different groups of categories. In this way, artificial areas are represented in reds and purples, agricultural uses in yellow, forests in green, open spaces in grey and green, and wetlands and water bodies in blue.

## Colour Value ##
White (2017) defined colour value as the lightness or darkness of a colour from pure black to pure white. Its variation constitutes "a continuous progression which the eye perceives in the grayscale stretching from black to white" (Bertin 1967) in a given area. 
As this is an excellent way of expressing order, it highlights the differences in a hierarchical system. Even though it is frequently used to represent quantities, the human capacity to associate different colour values with different quantities is very limited. Today, however, digital mapping allows black to be allocated in amounts that vary in proportion with the thematic value, so making it possible to use value ramps that overcome this limitation.
Like colour hue, colour value can be used in all geometric forms, although the best results are obtained on an area or volume, as the map user requires a certain minimum amount of surface area to perceive the variations of grey.
Since colour value is not suitable for representing nominal data, in LUC maps it is only used to summarize quantitative variables related to land use within administrative areas.

## Texture ##
Texture or pattern is a complex visual variable that comprises a varying number of components depending on the author you consult. According to White (2017), textures combine size, value, hue, shape and orientation. Other authors reduce these components to shape, arrangement, grain and spacing. Shape is the basic graphic unit making up texture. Arrangement refers to the layout of the basic graphic elements, either regular or irregular. Grain refers to the size of these elements and spacing to the distance between them. The use of textures for data measured at different levels is also controversial. While White recommends that textures only be used for nominal and ordered attributes of areas and lines, other authors 
Nowadays, textures are not used as often in mapping as they once were. In the past, when colour printing was significantly more expensive, textures were frequently used to fill out areas containing nominal, ordinal or quantitative information. Today textures have largely been replaced by colour. However, they are sometimes used in combination with other covering visual variables such as colour hue or colour value, so as to increase the amount of information provided by the map.
Textures can be useful in LUC mapping when the basic LUC information is combined with other relevant information. In the case shown in Fig. 

## Size ##
Size is, together with colour value, the most frequently used visual variable for representing quantitative data. Size can be defined as the variation in the area or the volume of a sign. It is rarely used in LUC mapping as these maps are normally based on categorical data. Although in theory, size expresses quantity, order and selection (Bertin 1967), its use in representing qualitative information can lead to confusion. Thus, size is only recommended for representing ordinal or quantitative data.
As regards the geometries of the map, size can only be fully applied to points. In the case of lines, since the distance between two points is fixed, size can only be applied as a variation in line width. As for polygons, any variation in their size based on a quantitative attribute other than surface area would result in the loss of their cartographic projection properties. Given this constraint, when the nature of the attribute is such that its representation with size is recommended, polygons may be represented by a point, usually at their centroid, which varies in size according to the value attached to the polygon attribute.
LUC map products using this visual variable are therefore limited to those summarising quantities such as the proportion of land occupied by each land use category, the proportion of land undergoing a land use change between two dates, or other related quantitative variables. In all cases, these quantities are summarized on a superimposed spatial structure, usually administrative units.

## Visual Variables and Geometric Dimension ##
In the previous paragraphs, we have seen how some visual variables adapt better than others to the varying geometric forms in which geographical information is presented.
Figure 

## Visual Variables and Measurement Level ##
In the above descriptions of the visual variables, we also outlined the meanings with which they are associated, and consequently the most suitable level of measurement for them. In general terms, the visual variables that can be ordered (colour value, size, texture and colour hue if properly ordered) are best suited for attributes measured at ordinal level. Visual variables indicating quantity (size and to some extent colour value and texture) can be applied to represent attributes at interval or ratio measurement levels. For their part, the visual variables with selective and associative properties, such as colour hue, shape and orientation, are used to represent attributes measured at nominal level.
Orientation is a special case. It usually has the same meaning as shape, but under certain conditions it can also be used to represent ordered attribute series. For instance, an arrow symbol pointed at any angle in the 360°of a circle could be associated with an ordered attribute depicting every point in a hierarchy based on the angle of the arrow.
As regards textures, their complex nature makes them suitable for any measurement level. Changes in the shape, orientation and colour hue of the pattern of elements that make up the texture would apply to attributes at nominal scale while size and colour value variations would be used to represent attributes at quantitative and ordinal measurement scales. Figure 

## Representing Nominal LUC Data ##
Most common LUC maps depict an area or region, highlighting with different colours the homogeneous patches of the different LUC categories it contains. As described above, for these maps to serve as successful communication tools, they must comply with a series of cartographic rules.
In terms of cartographic projection, the proportionality of areas must be preserved. If not, it would be impossible to compare the respective size of the different categories on the map. Equivalent projection must therefore be used.
The final size of the map will determine the scale and therefore the size of the Minimum Mapping Unit. In the case of digital maps, we recommend that an intelligent zoom be used so that the map only displays features equal to or greater than the minimum size. As a fixed image, the final LUC map must also strike a balance between the MMU and the number of LUC categories.
The visual variable best suited for categorical data is colour hue. Its use in LUC mapping must adhere to generally accepted conventions such as the use of blue colours to represent water bodies, reds and purples for built-up areas and so on.
In line with these recommendations, Fig. 

## Representing LUC Quantitative Data ##
As pointed out above, the cartographic representation of LUC quantitative data requires additional layers, such as administrative units, for the computation of these quantities at a meaningful spatial level. Some sort of selection must be undertaken in order for the resulting maps to be readable. Figure 
As with any map representing quantitative attributes, special attention must be paid to the number of intervals and their limits. An excessive number of intervals would make it difficult to differentiate between the associated symbols, regardless of whether they are based on size or colour value. By contrast, if too few intervals are used, this will reduce the level of detail of the information provided by the map. 

## Representing LUC Changes ##
One of the key areas in LUC studies is the analysis of the cover changes that have taken place in the past or are predicted to occur in the future, according to different scenarios 
The cartographic representation of the LUC change that has taken place between these two dates is often expressed in terms of the amount of land gained or lost by each land use category. This is a quantitative attribute and is therefore subject to the constraints summarized in Sect. 5.
As regards the representation of categories as nominal data, an excessively large number of land use categories in the input maps and their associated, theoretically possible transitions would in turn result in an excessively large number of new categories. This means that some kind of selection process must be performed. The options include: (i) reducing the map to the binary categories of "stable" and "changed"; (ii) selecting just one land use category to represent the areas gained or lost by it; and (iii) selecting the areas gained or lost by one specific land use category, in order to represent the land use categories from which or to which these areas have changed.
In order to make the comparison, the two input maps must be overlaid. During this process, it is highly likely that new areas of varying size will appear on the output map. The issues relating to the MMU discussed in Sect. 2.2. apply to the representation or possible generalization of these new polygons. Figure 

## New Forms of Visualizing and Communicating LUC Data ##
Throughout the examples presented so far, we have made clear that LUC representation is a far from simple task and that LUC maps convey even the most relevant aspects of LUC information with difficulty. These limitations can have serious consequences when it comes to taking policy and land planning decisions. The abstract representation, normally by means of colour hues, of land use categories or the transitions between them does not necessarily make it easier for users to understand the real landscape changes they represent. Policy makers may not be expert map users, and will therefore require more intuitive information in order to fully comprehend the impacts of predicted land use changes on landscapes, economy, society and the environment. 
In an attempt to alleviate these issues, various interesting case studies have integrated new approaches to cartographic visualization 
In addition to these realistic 3D examples, technological developments in the mapping industry have enabled the production of new cartographic tools that have yet to be explored in the communication of LUC information. Three areas are in need of further research and implementation. First, the current predominance of digital maps that are viewed through a computer device equipped with speakers, contrasts with the almost complete absence of research into sound mapping applied to LUC analysis. Second, the limited interactive capacity of LUC digital maps makes it difficult to compare them. And third, the possibilities offered by the computerised environment for visualizing animations, perhaps the most efficient tool for communicating changes over time, have yet to be applied in LUC change studies.

## Conclusions ##
In this chapter we have reviewed the main cartographic methods for representing and communicating LUC and LUCC information. The maps produced must comply with basic cartographic rules and must therefore have an appropriate cartographic projection, a balanced level of generalization, MMU and attribute details, a suitable set of visual variables and, in the case of quantitative data, a proper method for the classification of the thematic variable.
Even the maps that comply with these rules are often not fully comprehensible for their final users. This may be because the scale used in the final printed maps, the format in which most decision-makers receive the information, is too small or simply because not all the actors involved "speak" the cartographic language.
In order to overcome these issues, new cartographic methods including geovisualization techniques like realistic 3D mapping, are being explored. Other technological advances like sound mapping, fully interactive mapping or animated mapping are still underused in LUC studies. The integration of realistic 3D models with animation and sound will enable the inclusion of moving living creatures (like animals or people), human-made moving objects (like vehicles or windmills), vegetation, topography, buildings, and variations in the atmosphere or the light. Progress of this kind in LUC representation will make LUC maps more realistic and will enhance their communication capabilities, which in turn will help ensure better-informed decision-making processes. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## Sample Data for Thematic Accuracy Assessment in QGIS ##
Miguel Ángel Castillo-Santiago, Edith Mondragón-Vázquez, and Roberto Domínguez-Vera

## Abstract ##
We present an approach that is widely used in the field of remote sensing for the validation of single LUC maps. Unlike other chapters in this book, where maps are validated by comparison with other maps with better resolution and/or quality, this approach requires a ground sample dataset, i.e. a set of sites where LUC can be observed in the field or interpreted from high-resolution imagery. Map error is assessed using techniques based on statistical sampling. In general terms, in this approach, the accuracy of single LUC maps is assessed by comparing the thematic map against the reference data and measuring the agreement between the two. When assessing thematic accuracy, three stages can be identified: the design of the sample, the design of the response, and the estimation and analysis protocols. Sample design refers to the protocols used to define the characteristics of the sampling sites, including sample size and distribution, which can be random or systematic. Response design involves establishing the characteristics of the reference data, such as the size of the spatial assessment units, the sources from which the reference data will be obtained, and the criteria for assigning labels to spatial units. Finally, the estimation and analysis protocols include the procedures applied to the reference data to calculate accuracy indices, such as user's and producer's accuracy, the estimated areas covered by each category and their respective confidence intervals. This chapter has two sections in which we present a couple of exercises relating to sampling and response design; the sample size will be calculated, the distribution of sampling sites will be obtained using a stratified random scheme, and finally, a set of reference data will be obtained by photointerpretation at the sampling sites (spatial units). The accuracy statistics will be calculated later in Sect. 5 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps" as part of the cross-tabulation exercises. The exercises in this chapter use fine-scale LUC maps obtained for the municipality of Marqués de Comillas in Chiapas, Mexico. When conducting error assessment, it is important to strike a balance between the theoretical requirements and the practical reality of implementation 
There is no single right way to calculate the ideal sample size; in general, this task could be regarded as a process of successive approximations, in which criteria such as the availability of resources, levels of sampling error, or the desired degree of accuracy all play an important role. The expertise of the user and his/her interest in certain thematic classes are also important factors in the success of the estimation process.
An initial estimation of the most appropriate sample size can be made with the formulae used in statistical sampling. Equations for the validation of thematic maps have often been taken from the original work by 
where O = accuracy expressed as a proportion (in the case of simple random sampling O is the anticipated overall accuracy, whilst in stratified sampling it is the anticipated user's accuracy); n = number of sampling sites; z = percentile from the standard normal distribution (z = 1.96 for a 95% confidence interval); and d = desired half-width of the confidence interval of O. It can also be expressed as
In the case of stratified random sampling, 
where S(Ô) = standard error of estimated overall accuracy; Wi = mapped proportion of the area of class I
; and Ui = User's accuracy for class i.
Note that in both cases, it is necessary for the user to define certain parameters in advance, such as the permissible level of error (S(Ô)) or the user's accuracy values. These data should be obtained from prior or approximate knowledge regarding the quality of the map or from previous experience in producing maps with similar characteristics.
Sometimes it may be difficult to estimate user's accuracy, so practical recommendations for sample size calculation may be useful. 
Sampling design is another important factor to consider. Frequently used types include systematic, simple random and stratified random sampling. Traditionally, the cost or ease of fieldwork was a criterion for preferring some designs over others. With the increased availability of high-resolution imagery, in many cases, it is no longer essential to obtain data directly in the field. Reference data can now be interpreted from the imagery, so reducing costs dramatically.
The systematic sampling is easier to implement in the field, but has the disadvantage that it cannot be used to construct an unbiased variance estimator 
Regardless of the design chosen, a problem that sometimes arises is the under-representation of small or rare thematic classes in the sample. In other words, once the sample has been calculated and distributed, it may leave some classes with too few sites (<50). Some authors 
Once all the different stages of the accuracy assessment process have been performed, the precision values obtained should be reviewed, e.g. the magnitude of the overall accuracy standard error or the width of the user's accuracy confidence intervals. Even if there are some variations from the expected values, if the values obtained meet the analyst's targets as regards accuracy, then there would be no need to repeat the analysis 
QGIS Exercise: To calculate sample size and to distribute sample sites using a random stratified approach Next, we present a practical way to carry out the sampling design for obtaining reference data. In this exercise, we will estimate the sample size for a stratified random design, for which we will have to specify the expected standard error of the overall accuracy and to provide an a priori estimate of user's accuracy values. Sometimes, these figures may be difficult to provide in which case we can use the default values provided by the tool we will be using.
Available tools
There are several useful tools in QGIS for statistical sampling design. All of them are external plugins such as Semi-Automatic Classification Plugin (SCP), AcATaMa and MapAccurAssess. The MapAccurAssess plugin is a trial version specifically developed in the context of this book, which is not yet available in the official QGIS repositories.
SCP, which was developed by 
AcATaMa was developed by the Group from the Forest and Carbon Monitoring System for the validation of single LUC maps (Llano 2019). It consists of a set of tools that guide the user through a series of steps: (a) sampling design (stratified or simple); (b) sample classification; and (c) calculation of the confusion matrix and accuracy statistics. In the sample classification step, the spatial unit is a pixel (or points in the GeoPackage or shapefile format), which is not very convenient for those who prefer to use a different spatial unit, such as group of pixels or polygons. At that stage (classification), a set of tools is enabled to zoom in on each of the samples, and four windows are created to display images of interest. An editable attribute table is also created to classify the samples.
In this exercise, we will be using MapAccurAssess, a plugin developed by the authors of this chapter, which includes several of the suggestions proposed by 
This plugin provides several functions for calculating sample size in a stratified random design, using Neyman's optimal allocation to calculate the number of sampling sites in each thematic class. If, after that, any class has less than 50 sampling sites, it must be assigned between 50 and 100 sites depending on the complexity of the map. The result is a layer of points (shapefile) that are distributed over all the thematic categories of the map according to the stratified random design criteria. The points can be further modified to represent a polygon using QGIS functions.

## Materials ##


## Marqués de Comillas Land Use Cover Map 2019 ##


## Requisites ##
To calculate the area of each thematic class the LUC map must be projected in any cartographic projection (not geographic coordinates). The plugin has been tested on map projections with distance units in metres (rather than feet for example).

## Execution ##
Step 1
Install the MapAccurAssess plugin. All the relevant information regarding the installation of the plugin can be found in Chapter "About This Book" and the plugin's manual, which is included in the plugin's download.
Step 2
Go to the Plugins Menu, select the Accuracy Assessment and Random point options. Alternatively, you can click on the Random Point icon (Fig. 

## MapAccurAssess plugin icon ##
Step 3
In the dialogue box in Fig. 
The Ui values (User's Accuracy for class i) refer to an a priori estimation of accuracy for the thematic class, which could be based on expert judgement or on previous assessments. If there are any doubts about these values, the default values can be retained. Whilst Ui can vary between 0 and 1, a value of 0.5 was allocated to a large number of sites. Values of over 0.5 will generate a smaller sample size. The last stage is to select the folder where the results will be saved.

## Results and Comments ##
The results are displayed in the Record tab, and two types of output are generated and saved in the selected folder: (i) a. csv file with statistics about the thematic classes and (ii) a point shapefile where the points represent the centres of the sampling sites.
The .csv file contains a row for each thematic class and four columns showing id, area, the number of sampling sites estimated using Neyman's optimal criteria and the suggested number of sampling sites, adjusted to ensure that none of the classes have less than 50 sites (Table 
The vector point layer contains the spatial location of the centres of randomly distributed sampling sites (Fig. 

## Collection of Reference Data for Assessing the Accuracy of a Thematic Map ##
One of the major challenges of map evaluation is to obtain a reliable reference dataset with minimal positional errors and with the same date as the LUC maps. The aim is to obtain a data subset that faithfully represents the population from which it was extracted, so as to obtain confident accuracy estimates 

## MapAccurAssess plugin ##
The collection of reference data requires the prior definition of several aspects relating to the size of the sampling area and the characteristics of the information we want to obtain 
The reference source can be either observed field data or data interpreted from satellite imagery and aerial photographs. Although data collected in the field is always preferable, this method is much more expensive, and the interpretation of aerial photographs and satellite images is often regarded as an acceptable alternative. In this case, it is important to ensure that the reference data has a higher quality and resolution than the images used in the initial mapping process. The labelling protocol should be the same as that used in the mapping, i.e. the land-cover classes or types of change, and the photointerpretation criteria for labelling the sampling sites should be the same as those used when drawing the map being assessed.
When the reference data are obtained from satellite imagery, there is a degree of uncertainty associated with the level of expertise of the photointerpreter. This uncertainty can be reduced if classification criteria are established before obtaining the reference data. To minimize interpreter bias, we suggest that at least two specialists perform the class assignment independently. When different labels are assigned to the same sampling unit, a third interpreter must decide which class it should be assigned to.
It is also necessary to establish the criteria for dealing with non-ideal situations. When the spatial reference unit, defined as a set of pixels, contains several different land-cover classes we suggest, when possible, assigning the reference unit to the majority category, representing more than 50% of the area. Another complex situation could be when the reference unit contains a linear feature or corridor which is assigned to several different land-cover classes. In this case, we suggest moving the sampling site to another place in which there is less uncertainty regarding class allocation. The producer and the person(s) assessing the map should always reach agreement on such decisions and document them, so as to avoid biases in the accuracy assessment.

## QGIS Exercise: To collect reference data ##
This exercise is a guide to collecting reference data. Instead of fieldwork, high-resolution satellite imagery, available on Google Earth, is used together with various QGIS tools. The result of this exercise is a set of comparisons of land-cover observation taken from the high-resolution image (reference data) and the land-covers extracted from the LUC map under evaluation. The output data are formatted to compute the error matrix and accuracy statistics. These calculations are explained in Part III of this book (Sect. 5 in chapter "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps").
Available tools
The Semi-Automatic Classification Plugin (SCP) and the AcATaMa plugin have a module for the collection of reference data. AcATaMa provides a multi-view interface that allows spatial units to be added and revised in an orderly manner. However, spatial units can only be one pixel in size. For its part, the process for collecting the reference data using SCP is very similar to the process that would be followed if just QGIS tools were used. Notwithstanding, as SCP uses a unique data format (.scp), it is quite complicated to add other types of data or to use information from other platforms.
Both plugins have valuable tools that assist in the capture of reference data. However, as we intend to use larger spatial units than one pixel and wish to keep the installation of new interfaces and formats to a minimum, we will only use the basic QGIS (Buffer) tools, and other data services such as the Google Earth Engine Data catalog and QuickMapServices.

## Materials ##
Centroids of sample sites-Marqués de Comillas (the point vector layer RandomSample.shp created in the previous exercise that contain the centres of the sample sites)

## Execution ##
Step 1
Before data collection can begin, the size and shape of the spatial unit must be established, i.e. the area over which we will be making the comparison between the thematic map values and the reference values. The minimum mappable area of the thematic map to be used in this exercise is 1 ha, and it is generally recommended that the spatial unit should be of a similar size. Accordingly, in this exercise, we will be using square polygons of 1 ha as the spatial unit.
The point layer containing the centroids (Centroids of sample sites-Marqués de Comillas) will be used to create the spatial assessment units. To form square polygons centred on each of the points in the point layer, use first the Buffer tool in the Geoprocessing Tools menu. The input will be the point layer, and the distance value depends on the desired size of the square. In this case, 50 m. Change the End cap style to "Square" and leave the rest of the parameters unchanged (Fig. 
The newly created layer will have two attributes: the id and the value of the thematic class (inherited from the previous exercise). To avoid bias in the photointerpretation decision-making process, we advise hiding the class column (the value of the thematic class taken from the LUC map). To hide a column in the attribute table without deleting it definitively, right-click on the area of the attribute table headers, select the option Organize Columns, and then select the columns to hide (Fig. 
Step 2
Ideally, to identify the land-cover type of each sampling unit, it would be necessary to overlay them on high spatial resolution images with the same (or similar) date as the images used in the mapping. If such data are available, photointerpretation of the spatial units can proceed directly. However, acquiring high-resolution images to verify extensive areas could be expensive. In this regard, sometimes the resources are limited, which restricts the use of this source of imagery.
In the following steps, we propose a partial solution to this problem based on the combined use of image servers (Google Earth, Bing, ESRI) with high spatial and temporal resolution. However, in these servers it is impossible to identify and select scenes according to their acquisition dates. One way to estimate the dates of these data is to compare them with images with higher temporal resolution, which have a known acquisition time, and for which a longer Sample Data for Thematic Accuracy Assessment in QGIS historical record is available, such as Sentinel or Landsat images. For this purpose, we will install two plugins with which we can access the high spatial resolution image servers of Google, Bing and ESRI (QuickMapServices) and Sentinel, Landsat, Aster and other images (Google Earth Engine Data Catalog). To see how to install these plugins in QGIS, see Chapter "About This Book".
Step 3
Once QuickMapServices is installed, open the plugin and select Setting. Then select the More Services tab and click on the Get contributed pack. To add images with high spatial resolution to the QGIS Project, in the Web option in the main menu select QuickMapService, then Google and finally Google Satellite. After selecting these options, the Google Satellite images become available in the Layers menu.
Step 4
Add Sentinel-2 images from the Google Earth Engine Data Catalog plugin. The plugin requires to define the product type, date and cloud cover percentage (Fig. 
Step 5
To facilitate the collection of reference data, we suggest creating multiple windows to display images with different dates or resolutions, a good way to work when assessing land-cover change maps.
In the Layers panel, select the Image and vector layer (Centroids of sample sites-Marqués de Comillas) that will be added to the second window, click on "Manage Maps Themes" button (represented as an eye) and select "Add Theme". Name the theme "Image 1". Then go to "View" in the main menu, and select "New Map View". This will create a new display window. Enter the new window and do the following: (a) Select the layers set to display (Image 1) by clicking on the on "Manage Maps Themes"; (b) Synchronise the windows by selecting the "view settings" tool, and then click on the "Synchronise scale" option. The example in Fig. 
Step 6
To capture the reference data, the "Centroids of sample sites -Marqués de Comillas" vector layer must be edited and a new field (integer type) must be added. We suggest naming it "Refer_data". Step 7
To make reference data collection easier, we recommend displaying the Attribute Table as a form and anchoring it to the main window, displaying only the selected data. To do this, open the Attribute Table, select the "Dock Attribute Table " icon, select the "switch to form view" button and then "show Selected Feature" (Fig. 
Step 8
If you have completed Steps 1-8 successfully, you can now start photointerpreting high-resolution satellite images. The exercise involves identifying the predominant land-cover type in each sampling unit and recording the corresponding code in the "Refer_data" column of the attribute table (Fig. 
Photointerpreting all the spatial units of the sample can be a lengthy process, so we suggest that you try to photointerpret at least 10 to 20 spatial units and then compare your results with the "Photo-interpreted reference dataset-Marqués de Comillas 2019", a reference dataset was prepared by the authors. It is available, together with all book's data, at https://doi.org/10.5281/zenodo.5418318. For more information, see Chapter "About This Book".

## Results and Comments ##
The result of this exercise should be a shapefile with an attribute table in which the columns class (map class code) and refer-data (photointerpreted class code) are filled in, as shown in Fig. 
It is worth remembering that in the absence of high spatial resolution imagery, medium resolution imagery, such as Landsat or Sentinel, can provide sufficient information to validate maps, especially small-scale maps.
Although the spatial assessment unit used in this exercise is widely used and recommended, it may contain several land-cover types. This means that clear rules should be established when deciding the category to which the unit should be allocated in these circumstances.

## References ##
Cochran 
In this chapter, we describe the fundamental principles and the normal procedure followed when cross-tabulating two datasets. Cross-Tabulation analysis (Sect. 1) is usually the first step in the validation of Land Use Cover (LUC) data. It compares two datasets to observe their spatial relationship, i.e. their degree of spatial (dis) agreement. Results are usually displayed in the form of maps, tables and other statistical measures. Multiple-Resolution Cross-Tabulation (Sect. 2) compares two raster datasets at multiple spatial resolutions. Basic Cross-Tabulation can compare raster and vector data, while Multiple-Resolution Cross-Tabulation only works with raster data, which is what we use in the exercises provided as examples. In the exercises, raster data were obtained from vector data previously rasterized at different spatial resolutions. As a reference we use LUC maps, although ground points could also be used as reference data for these analyses. Examples of Cross-Tabulation analyses at one and multiple resolutions are presented for four different cases: to validate single LUC maps, to validate the soft maps produced by a model, to validate a simulation exercise and to validate and study land change in a series of LUC maps. In the example exercises, we used CORINE and SIOSE maps from the Asturias Central Area database, as well as maps from the modelling exercises carried out with this database. In the Chapters "Metrics Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps" and "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps", we focus on specific analyses that can be carried out on the basis of Cross-Tabulation analyses, such as Land Use Cover Changes (LUCC) Budget or Quantity and Allocation disagreement. These help unleash the full potential of Cross-Tabulation analysis.

## Keywords ##
Cross-Tabulation Á Multiple-Resolution Á Land Use Cover data Á Validation 1 Basic Cross-Tabulation Description Cross-Tabulation is a primary analysis that crosses two datasets, either raster or vector, to analyse their spatial relation. This analysis combines the datasets in spatial terms. It produces a map or table that shows how the values of one dataset spatially relate with the values in the other, thereby informing us as to whether the two datasets share the same values at a given location and, if not, with which other values they have established a relation.

## Utility ##
Exercises 1. To validate a map against reference data/map 2. To validate soft maps produced by the model against a reference map 3. To validate a simulation against a reference map 4. To validate a series of maps with two or more time points
Starting with a map and some reference data, we can use Cross-Tabulation to determine to what extent the map we want to validate agrees with the reference data. In this way we can compare the success of a LUC classification exercise or a LUCC modelling exercise against reference data. We can also assess how uncertain a map is with regard to the data used as a reference. Cross-Tabulation can also be used to study the LUC changes between pairs of maps at two or more different points in time, or to validate a chronological series of maps, as it can detect unusual or abnormal changes, which could be due to technical errors.
The Cross-Tabulation matrix provides users with a lot of information from the maps in one single analysis. However, in order to take advantage of the full potential of this analysis, it is important for them to understand what all this information means. This is what we will be explaining in this chapter.
The results of Cross-Tabulation can then be used to make further analyses and to extract other metrics that allow us to take full advantage of this basic analysis. These methods (e.g. LUCC budget, Quantity & Allocation disagreement, the Figure of Merit, Intensity Analysis) (see Sects. 2, 3, 4 and 6 in Chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") make it easier for users to interpret the results. However, they also require many further analyses and are therefore more time-consuming. We will now provide an overview of some relevant examples: The maps to be compared or assessed may be in either raster or vector format. For those in raster, we can use both hard and soft maps, such as suitability, transition potential and probabilities maps.

## QGIS Exercises ##
The methods and techniques presented in Chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps" (e.g. LUCC Budget, Intensity Analysis, Quantity and Allocation disagreement…) are based on this basic Cross-Tabulation analysis. In this chapter, we will therefore only be describing the fundamental principles and the normal procedure followed when performing a Cross-Tabulation between two datasets. QGIS includes many tools for cross-tabulating spatial data through the associated GRASS and SAGA models. The "Semi-Automatic Classification Plugin" also includes tools to cross-tabulate datasets for different purposes.
Table 
The associated R software can also be used to cross-tabulate pairs of maps. This is done using the crosstab function, which is part of the "raster" package. 
Of all the tools available in QGIS, the one we will be recommending and using in this book is the "Semi-Automatic 

## Requisites ##
The two maps must have the same extent, spatial resolution, projection and classification legend. If the maps have different classification legends, the user must reclassify the maps in such a way as to unify the two legends.

## Execution ##
Step 1
Open the "Semi-Automatic Classification Plugin" and select the "Postprocessing" tab from the sidebar. Then click on Accuracy and select the required parameters: raster to assess (CORINE map) and reference raster (SIOSE map) (Fig. 

## Results and Comments ##
Once the function has been executed, QGIS creates an output raster that gives each pixel a code. This code identifies every single possible combination of values between the two input rasters. The meaning of each code is presented in a table in CSV format, which is stored in the same folder as the raster. This information is also displayed in the "output" window of the "Semi-Automatic Classification Plugin" (Fig. 
If we analyse the first matrix shown in the "output" window (ErrMatrixCode/Reference/Classified/PixelSum), it will help us understand the meaning of the codes in the raster. The "ErrMatrixCode" is the number that identifies each pixel in the new raster. "Reference" is the code for the category on the reference map (i.e. SIOSE Land Use Map). "Classified" is the code for the category on the compared map (i.e. CORINE Land Use Map), and "PixelSum" refers to the number of pixels for each combination in the new raster.
The ErrMatrixCode 1 identifies 234,164 pixels (Pix-elSum) in category 0 in SIOSE (Reference) and 0 in COR-INE (Classified). The codes for combinations in which the reference and the classified categories are the same (e.g. 0, 0) mean agreement, while those in which the reference and the classified categories are different (e.g. 0, 1) mean disagreement. Code 2 is therefore a disagreement area because the pixel is classified as 0 in SIOSE and as 1 in CORINE.
If we symbolize the obtained raster in such a way that all the codes that refer to combinations of the same classes 
With the obtained raster, we can for example represent where the urban fabric of CORINE ( 
To do so, we must first identify the codes (ErrMa-trixCode) for the combinations we are looking for, i.e. pixels which are urban fabric in CORINE (Classified is 2) and which belong to any other category in SIOSE (Reference is not 2). These are codes 
In addition to the map, the accuracy analysis of the "Semi-Automatic Classification Plugin" also generates two error/Cross-Tabulation matrixes, one in cells and the other in square meters (area proportions). The matrix in cells (Fig. 
An analysis of the two tables (area-based error matrix and error matrix pixel count) offers us a detailed picture of how the categories on one map relate with the categories on the other. This highlights the degree of agreement between the reference map and the one we are trying to validate. In both tables, the combinations in which there is agreement can be seen on a diagonal line running across the table. All combinations outside this diagonal mean disagreement (Table 
If we look at urban fabric, of a total of 28,110 pixels labelled as urban fabric in CORINE (Total column on the right), 19,455 are also labelled as urban fabric in SIOSE. That is, almost 70% of the pixels identified as urban fabric by CORINE are also considered urban fabric in SIOSE. In the other 30%, CORINE mostly confuses urban fabric with industrial and commercial areas (category 3, 2244 confused pixels), artificial green urban areas (category 9, 1643 confused pixels) and road and rail networks (category 6, 1216 confused pixels).
These results are due to the greater degree of generalization when mapping CORINE, as explained above. On the basis of these results and taking SIOSE as a reference, we can conclude that CORINE maps urban fabric correctly and can be considered a valid map for our exercises.
Users can also carry out more complex analyses with these matrixes using the CSV file generated by the tool. In this way   the matrixes can be imported in spreadsheet format with software such as Excel or OpenOffice Calc. We can then calculate the agreement and disagreement percentages for the whole raster or for each of the categories under consideration, as we did manually for the urban fabric above. The error matrixes also provide useful statistical measures (Fig. 
Exercise 2. To validate soft maps produced by the model against a reference map Aim To find out whether the urban fabric soft map produced by our model agrees with the urban fabric areas of the reference map for the year of the simulation.

## Materials ##


## CORINE Land Use Map Asturias Central Area 2011 Urban fabric suitability map -CORINE model ##


## Requisites ##
The two maps must have the same extent, spatial resolution and projection. The soft map must be categorical. The Land Use map must only contain information about the category being assessed. For a proper validation, the reference map must refer to the same date on which the landscape was simulated.

## Execution ##
Step 1
Only discrete or categorical maps can be cross-tabulated. As the soft map we want to validate is continuous (continuous values from 0.1 to 1), the first step must be to convert it into a categorical map, using the Reclassify by table function (Processing toolbox > Raster analysis > Reclassify by table).
After opening this tool, we select the map we want to reclassify (Urban fabric suitability map) and fill in the "Reclassification table" with the new values that will be replacing the old ones in the raster (Fig. 
Step 2
Given that our objective is to compare the suitability values for urban fabric in the model with the areas classified as urban fabric on the 2011 map, we must ignore all the other categories on the Land Use Cover map. We must therefore obtain a binary map from the initial CORINE map. In this binary map, 1 will mean the category being evaluated (urban fabric) and 0 all the others.
To obtain this binary map, we repeat the same process as in Step 1. In this case, we reclassify the CORINE map, assigning a value of 1 to urban fabric (code 2 in the original map) and a value of 0 to the other categories (codes 0, 
Step 3
Once we have obtained the two maps, we then carry out Cross-Tabulation using the "Semi-Automatic Classification Plugin". We click on the "Postprocessing" tab and select the Cross classification option.
We then select the required parameters. In "Select the classification" we choose the reference Land Use Cover map obtained after reclassification (Step 2). In "Select the reference vector or raster" we choose the soft map obtained after reclassification (Step 1) (Fig. 

## Results and Comments ##
Once the function has been executed, QGIS creates a raster and a CSV file with all the results of the Cross-Tabulation. These are also displayed in the "Output" window (Fig. 
The first table provides information about the meaning of each code in the new raster. Pixels with value 2 refer to areas that are urban fabric (Classification is 1) and have a suitability of less than 0.2 (Reference category is 1). This combination occurs in just 2 pixels (PixelSum), which represent an area of 5000 m 2 on the map (Area [metre 2 ]).
The second table gives an overview of the possible combinations on the two maps and the area, in square meters, covered by these combinations. This shows that the areas that are not urban fabric (Classification is 0.0) and have a suitability of below 0.25 (Reference 1) occupy 2,312,499 m 2 .
From all the possible combinations, we can see that most of the pixels that are urban fabric on the reference map fit with the areas with the highest suitability to become urban fabric (26,137 pixels, 65,342,474 m 2 ). There are relatively few urban fabric pixels with a suitability of between 0.5 and 0.75 (1971 pixels, 4,927,498 m 2 ) and an insignificant number with a suitability of less than 0.5.
These results indicate that our suitability map has been validated. In other words, the high suitability values on the soft map correspond with urban fabric areas on the reference map. For their part, the low suitability values correspond to areas where there is no urban fabric on the map. This means that when we use this map in our simulation, it will help us to correctly identify those areas that can become urban in the future and those that cannot.
Other more sophisticated tools, such as the ROC curve and the Difference in Potential (see Sects. 2 and 3 in Chapter "Validation of Soft Maps Produced by a Land Use Cover Change Model"), can be used to complement this analysis and offer the user a full overview of the validity of their potential maps.  

## Requisites ##
The two maps must have the same extent, spatial resolution, projection and legend. For a proper validation, the reference date must refer to the date on which the landscape was simulated. 

## Execution ##
Step 1
Open the "Semi-Automatic Classification Plugin", click on the "Postprocessing" tab and select the section Accuracy. Then, select the required parameters: raster to assess (Simulation) and reference raster (CORINE reference map) (Fig. 

## Results and Comments ##
When we execute the function, QGIS creates an output raster showing the combination of classes between the two input maps. The function generates three tables in the "output window", which are also stored in CSV format in the same folder as the raster. They specify the meaning of each code in the new raster. They also include a couple of error/Cross-Tabulation matrixes, in cells and in square meters (proportional quantities) (Fig. 
If we symbolize the raster and focus on the information in the Cross-Tabulation matrix of most interest for assessing our simulation, we can understand the errors we made in our modelling exercise in greater detail.
In our exercise we only actively modelled two categories: urban fabric and industrial areas. In the raster we can identify the simulated areas that show agreement (or disagreement) with the reference map for each of these two categories. To do this, the first step is to identify the code for the combinations involving the two categories being considered: urban fabric ( 
The combination codes for urban fabric are 3, 16, 29, 42, 81, and 120, while code 29 represents the areas that were simulated as urban fabric (Classified is 2) and also appear as urban fabric on the reference map (Reference is 2). The combination codes for industrial and commercial areas are 4, 17, 30, 43, and 82, while code 43 represents the pixels that are industrial and commercial areas in both the simulation and the reference map.
If we symbolize the raster obtained using these codes in terms of agreement (codes 29 and 43) and disagreement (all the other codes mentioned above), we can visualize the pattern of error in our simulations compared to the map we use as a reference (Fig. 
Most of the simulated areas agree with the reference map. Disagreement can only be observed in a few cases. However, this conclusion may be misleading. Most of the agreement refers to areas that were already urban fabric or industrial and commercial areas, i.e. areas that were correctly simulated as permanence.
Simulating permanence for artificial surfaces is very easy. A high rate of success is expected in all cases. If we focus on the areas that actually changed during the simulation period in relation to the reference map and those that were simulated as change, we can detect a higher proportion of errors. However, this cannot be detected on our map. In order to focus on these errors, we should only cross-tabulate the changes in the simulation with respect to the initial map 
Once again, whereas most of the agreement refers to areas that were already urban fabric in the past and were correctly simulated as persistence, confusion seems to refer above all to areas that were not correctly simulated. That is, agricultural and vegetation areas where new urban fabric was simulated but which, according to the reference map, did not actually undergo any change. We therefore need to repeat the analysis, focusing only on the areas that actually change so as to assess the success of our simulation more effectively.
Other tools, such as the Figure of Merit (see Sect. 4 in Chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"), can also be useful to help validate the simulation and overcome some of the limitations we have encountered. 

## Requisites ##
The two maps must be raster and must have the same extent, spatial resolution, projection and classification legend. If the maps have different classification legends, the user must reclassify the maps in such a way as to unify the two legends. The maps must refer to two different points in time.

## Execution ##
Step 1
The first step is to obtain a raster for the whole study area, showing the areas that changed during the study period and those that remained the same.
To get this map, open the "Semi-Automatic Classification Plugin", click on the "Postprocessing" tab and then select Land cover change. Then, complete the required parameters, selecting the older map as the reference classification 
Step 2
To obtain a map that only shows the areas that changed during the study period, we must repeat the same operation, this time leaving the "Report unchanged pixels" option unmarked (Fig. 

## Results and Comments ##
After executing Steps 1 and 2, QGIS creates two output rasters, one showing changes and permanent areas (Fig. 
The function also generates a table for each map in the "output" window and stored in CSV format. This table shows each possible combination and the code with which it is represented in the output rasters (Fig. 
Both the rasters and the table can be used to understand the changes in our study area. The table shows those that took place during the study period (Table 
Of the various different transitions of agricultural areas, the one to urban fabric (from category 0 in 2005 to category 2 in 2011) is the most important with a total of 751 pixels. As regards the transitions in vegetation areas (category 1), the most common was the change from vegetation areas to agricultural areas (from category 1 in 2005 to category 0 in 2011), with a total of 588 pixels. This change in pixels (Table 
Most of the change in our area was between agricultural and vegetation areas and vice versa and from agricultural and vegetation areas to artificial surfaces. However, there were also various other interesting transitions, such as the conversion of water bodies into port areas (from category 11 to category 7), which affected a total of 657 pixels. This was due to the construction of a dock in Gijón in the north of our study area.
By symbolizing the raster of changes (Fig. 
In the composition of the map in Fig. 
The map shows the changes for the example area of Gijón. In the north, we can observe the new dock built in the port area. Apart from the port, most of the growth in industrial land took place in the south of the city. The same is true for urban fabric, with the construction of a new residential development in Roces. As can be seen on the map, this new residential area is cut off from the existing urban fabric of the city. There is a highway running between the two.
The results of this analysis can also be useful to validate a chronological series of maps. When interpreting the changes, it can help detect unrealistic changes that may be due to errors in the input data. We can also detect changes in the boundaries of the study area which cannot be fully represented on the maps because the study area has been clipped.
Other tools and techniques, such as LUCC budget or Quantity and Allocation disagreement, can also help characterize real changes in the study area and detect areas where no changes have taken place, despite being marked as change areas on the maps. In this way, these techniques can provide useful, complementary information on this question. However, users can compare the dataset at as many different resolutions as they deem fit. These must always be coarser than the original spatial resolution.
The concept of spatial resolution refers to the level of spatial detail available in the spatial data. It applies to data in raster format, where the spatial resolution is defined by the pixel size. This means that, unlike basic Cross-Tabulation, this analysis can only be performed with raster data.

## Utility ##
Exercises 1. To validate a map against reference data/map 2. To validate soft maps produced by the model against a reference map 3. To validate a simulation against a reference map This technique aims to control the multiscale uncertainty of a validation exercise, which is not considered in basic Cross-Tabulation analyses. It can also be used to evaluate the uncertainty of a LUC classification exercise, a LUC map or a LUCC modelling exercise against reference data.
Maps that show a lot of disagreement at detailed scales can refer to the same information at coarser scales. This technique can therefore be used to discover at which spatial resolution a map is considered least uncertain according to the information provided by a reference map.
This analysis can be used as a complement to fuzzy logic tools 
Multiple-Resolution Cross-Tabulation can only be carried out with raster data. However, we can make the comparison with either hard-or soft-classified raster maps, such as suitability, transition potential or probabilities maps. In the last case, we must always reclassify the soft-classified raster maps in a set of categories. It is not possible to cross-tabulate rasters with a continuous range of values.
As in the case of basic Cross-Tabulation, if we want to explore the full potential of the results of these analyses, we can use other complementary metrics such as Land Use Cover Change budget (LUCC budget, see Sect. In addition to the basic Multiple-Resolution Cross-Tabulation presented in this section, some more sophisticated variants have been proposed by other authors. These include:
• Costanza (1989), who proposed a method to determine the goodness of fit between model output and spatial and/or time series data based on the idea that the measurements at one resolution are not sufficient to describe more complex patterns. In his method, an expanding window is used to gradually degrade the resolution of the data, establishing, among the lack of fit, situations of "registration", "resolution" and residual components. 

## QGIS Exercises ##
Available tools QGIS does not include a tool to cross-tabulate maps at multiple resolutions. To carry out this analysis, it is therefore necessary to combine raster resampling tools with the basic Cross-Tabulation tools. For detailed information of the tools available in QGIS for performing Cross-Tabulation, please refer to Sect. 1.
Various different tools can be used to resample raster maps in QGIS. The GRASS module provides a tool (r.resample) for resampling the raster according to the Nearest Neighbour method. The GDAL module provides a tool to reproject rasters (Warp (reproject)) that also enables resampling through different methods, including the Nearest Neighbour. For its part, the SAGA toolbox provides a tool for resampling rasters with similar options. In addition, the QGIS interface allows the user to resample maps by making a copy of a displayed map via the option "Save raster layer as…" (Layer > Save as).
For categorical maps such as Land Use Cover maps, two resampling strategies are usually applied: Nearest Neighbour and Majority Rule. We decided to apply Nearest Neighbour because this is the method that best preserves the landscape composition and configuration or in other words, the proportions of the different categories and their patterns.
The four resampling tools available in QGIS are all equally valid. In this case we decided to use the tool that becomes available when making a copy of an existing raster (Save as…) because of its simplicity and efficiency. Nevertheless, users must be aware that the resampled rasters will vary slightly depending on the method chosen, and are therefore not fully comparable. Once a method or tool has been selected, all the resampling procedures must be performed using this same method or tool. 

## Requisites ##
The two maps must have the same extent, projection and classification legend. If the maps have different classification legends, the user must reclassify the maps in such a way as to unify the two legends.

## Execution ##
Step 1
Given that to carry out Cross-Tabulation at multiple resolutions we need to have maps in raster format, the first thing we have to do is rasterize our vector maps. If you would like to perform this analysis by resampling original raster maps, please refer to Exercise 2 Step 1.
We are going to convert our original vector file to raster at four different spatial resolutions: 25, 50, 75 and 100 m. Our analysis will be based on the same four spatial resolutions.
To rasterize vector data, we use the Rasterize (Vector to raster) tool. Once inside this tool, we begin by indicating the vector layer we want to rasterize (SIOSE 2011 map). Then, we go to "Field to use for burn-in value [optional]" where we indicate the field in the attribute table of the vector layer that will give the raster the pixel values (Metro) (Fig. 
We must also set the spatial resolution for the raster we want to create. To do this, we must first define the units for the spatial resolution in the "Output raster size unit" option (Georeferenced Units). Then, we choose the spatial resolution or pixel size through the "Width/Horizontal resolution" (25) and "Height/Vertical resolution" options (25). We must also specify the extent of the raster that will be created in the option "Output extent (xmin, xmax, ymin, ymax)". We are going to use the extent of the layer we are rasterizing (SIOSE 2011) through the submenu on the right (Use layer extent…).
The final stage is to assign a value to the background, i.e. the pixels that are not covered by any polygon in the vector file. Given that the vector already has values from 0 to 11, we will define the background with code 12. We do this via the option "Pre-initiate the output image with value [optional]", available under the "Advanced parameters" options (Fig. 
Our background value (12) will also be the nodata value of our raster. We can assign a nodata value for the raster we are going to create using the option "Assign a specified nodata value to output bands [optional]" (Fig. 
Once we have finished the first rasterization, we must repeat the same procedure for the other three spatial resolutions that we need for the SIOSE dataset. Then, we must repeat the whole workflow for the CORINE map. Once all these tasks have been completed, we will have 8 different maps (4 SIOSE and 4 CORINE) at 4 different spatial resolutions (25, 50, 75 and 100 m).
Step 3 Once all the maps have been created, we can start the Cross-Tabulation. To do this, open the "Semi-Automatic Classification Plugin", click on the "Postprocessing" tab and select Cross Classification. Then, select the required parameters: raster to assess (CORINE map 25 m) and reference raster (SIOSE map 25 m) (Fig. 
Step 4
After the first execution, repeat this process with the other pair of maps (one for CORINE and one for SIOSE) at different spatial resolutions.

## Results and Comments ##
Once we have executed the function four times, QGIS will create an output map for each execution with the combined classes and an error/Cross-Tabulation matrix. These will be stored in the folder we selected earlier when executing the tool. Matrixes are also displayed in the "output" window. For a detailed description of each of these results, please refer to the Sect. 1.
If we compare the results of each of the error matrixes, we can see that there are few differences between them. Error matrixes show the area in square meters covered by each possible combination between classes. The combination that covers most area is always the agreement between agricultural areas: pixels that are 0 (agricultural areas) in both the validated (CORINE) and the reference (SIOSE) maps. At a spatial resolution of 25 m, these areas occupy 585,267,500 m 2 ; at 50 m, 585,225,000 m 2 ; at 75 m, 585,815,625 m 2 ; and at 100 m, 584,660,000 m 2 . The differences are therefore very small.
A similar pattern can be observed if we look at the rest of the combinations. This means that at all the spatial resolutions there are very similar levels of agreement and disagreement between the classes on the two maps (CORINE and SIOSE). We can therefore conclude that the spatial resolution selected to make the analysis has no substantial effect on the results.
That means that the areas classified differently on the two maps are not due to small details drawn on one map that do not appear on the other. Disagreement is not the result of isolated pixels on one map that are not classified in the same category on the other. If this were true, the agreement between the two maps should be higher at coarser resolutions because they are more generalized, so ruling out minor details.
In conclusion, it would seem that the differences between the two maps are structural. In other words, they are not caused by the spatial resolution or level of detail of the maps, and instead result from the fact that each map represents a different reality on the ground. If we generalize both maps and rule out all small details, both maps show a similar level of agreement. Notwithstanding this, we must always remember that most of the areas in both maps agree, as confirmed in the Sect. 1.
When compared with SIOSE, CORINE can be considered a valid map because the agreement between the two is very high. The differences between them are the same regardless of the spatial resolution employed to make the analysis, at least within the resolution range we used (from 25 to 100 m). Thus, although the differences between SIOSE and CORINE are the result of their different scale and Minimum Mapping Unit, they cannot be eliminated simply by generalizing the maps using coarser spatial resolutions. In fact, their agreements and disagreements remain the same, which suggests that the different scale of production introduces important structural differences in the way the two maps draw the ground land uses and land covers.

## Exercise 2. To validate soft maps produced by the model against a reference map ##


## Aim ##
To evaluate to what extent the urban fabric suitability map of our model agrees with the urban fabric areas of the reference map for the year of the simulation at multiple spatial resolutions, determining the resolution at which there is most agreement.

## Materials ##


## CORINE Land Use Map Asturias Central Area 2011 Urban fabric suitability map-CORINE model ##


## Requisites ##
The two maps must have the same extent, spatial resolution and projection. The soft map must be a categorical map. The Land Use map must only contain information about the category being assessed. For a proper validation, the reference map must refer to the same date as the simulation. 

## Execution ##
Step 1
We begin by converting our soft map into a categorical one to comply with the requirements of the Cross-Tabulation tool. This is done using the Reclassify by table function (Processing toolbox > Raster analysis > Reclassify by table).
There are no standard criteria for the reclassification of soft maps and users can apply whatever thresholds they think best. In this case, we will use the same thresholds we used in Exercise 2 of the Sect. 1. We will therefore reclassify the map into four new categories: 1 (suitability 0-0.25), 2 (0.25-0.50), 3 (0.50-0.75) and 4 (0.75-1).
Step 2
As stated in the requisites, we will cross-tabulate the reclassified soft map with a map that only shows the Land Use Cover category of interest, i.e. urban fabric. To this end, we must extract the urban fabric areas from the LUC map (CORINE) using the same function as in Step 1 (Reclassify by table). In the reclassification, we will assign a value of 1 to urban fabric (code 2 in the original map) and a value of 0 to the other categories (codes 0, 
Step 3
Once we have the two maps, we can then resample them at different spatial resolutions to carry out the Multiple-ResolutionCross-Tabulation. In our case, as the original pixel size is 50 m, we will resample our maps at 75, 100, 125 and 150 m using the Save As…tool. In this tool, we need to indicate the name of the map we are going to resample (the reclassified suitability map of urban fabric) and the spatial resolution at which we will resample the maps (Fig. 
Once we have obtained all the maps we need, we can then carry out the Cross-Tabulation exercise using the Cross classification tool from the "Semi-Automatic Classification Plugin". Once inside the tool, we must indicate the two rasters that we want to cross-tabulate: the soft map (Select the classification) and the land use map for the category of interest (Select the reference vector or raster) (Fig. 
Step 6
After we do this for the maps at the original resolution (50 m), we repeat the process at the other 4 spatial resolutions (75, 100, 125 and 150 m).

## Results and Comments ##
After executing the function for each pair of maps at each spatial resolution, the tool produces (for each spatial resolution) an output map with the combination and two matrixes detailing how the values of both maps cross-tabulate. These are stored in the folder we selected and are also displayed on the screen (Output tab). For a detailed description of each of these results, please refer to the Sect. 1.
"The "Cross Matrix" is the most interesting of all these results in that it provides us with all the information we need for our analysis. It details how much of the area for each category in the reclassified suitability map falls inside areas that are urban fabric in our reference maps (Tables 
For the analysis at a spatial resolution of 50 m, there are 4999 m 2 of low suitability (suitability below 0.25) that cross-tabulate with areas that are urban fabric in the  reference LUC map. If we consider that each pixel represents an area of 2500 m 2 (50 m Â 50 m), this means that only 2 pixels of urban fabric cross-tabulate with areas of low suitability on the suitability map. 1971 pixels with medium to high suitability (0.5-0.75) cross-tabulate with areas that are urban fabric. Finally, most of the urban fabric pixels cross-tabulate with areas with the highest suitability (0.75-1): this combination is represented by 26,137 pixels. These data show that there is a positive correlation between suitability and the presence of urban fabric. We can therefore conclude that suitability is a good driver for our model.
Varying the spatial resolution of the analysis did not lead to any major differences in the correlation between the suitability map and the urban fabric areas in the reference maps. At the five spatial resolutions assessed, most of the pixels fell within the highest suitability category 
The dissimilarities between the analyses at different resolutions were very small. At 75 m, just two pixels fell within the areas of lowest suitability (11,245 m 2 ). At 100 m, there were a lot more: 74 pixels (738,436 m 2 ). At 125 m there was just 1 pixel (15,651 m 2 ), and at 150 m, no pixels at all (0 m 2 ). Similar behaviour can be observed for the other two categories of suitability at all five resolutions. This indicates that the suitability map for urban fabric in our modelling exercise is correct. It positively correlates with those areas that are urban fabric in our reference map, so helping us to identify the areas in which new urban fabric is most likely to appear. However, no conclusions can be drawn regarding the best spatial resolution at which to carry out the modelling exercise. As the explanatory power of the suitability maps is very similar at all the spatial resolutions assessed, the decision as to which spatial resolution would be best for our modelling exercise should be based on other factors, such as how realistic the pattern looks or what the minimum level of detail might be for the model to be useful for stakeholders and users.
This analysis could be complemented with more sophisticated tools like the ROC curve and the Difference in Potential (see Sects. 2 and 3 in Chapter "Validation of Soft Maps Produced by a Land Use Cover Change Model"). These tools also provide information about how well a model soft map simulates a category of interest, such as urban fabric.
Exercise 3. To validate a simulation against a reference map

## Aim ##
To validate a simulation for the year 2011 against a reference map for the same year at multiple spatial resolutions, determining the resolution at which both maps show the best agreement.

## Materials ##
Simulation CORINE Asturias Central Area 2011 CORINE Land Use Map Asturias Central Area 2011

## Requisites ##
The two maps must have the same extent, spatial resolution, projection and legend. For proper validation, the reference date must refer to the date on which the landscape was simulated.

## Execution ##
Step 1
For Multiple-Resolution Cross-Tabulation, we need first to resample the original rasters (50 m) at other spatial resolutions. In this case, we will resample our simulation at 100, Step 2). Once inside the tool, we fill in the required parameters: name of the raster to be sampled (Simulation CORINE) and spatial resolution (100 m).
Step 2
Once we have resampled the first map, we then repeat the procedure for the other spatial resolutions (150 and 200 m) and for the reference map. By the end, we should have 8 maps (4 simulations and 4 reference maps) at 4 spatial resolutions (50, 100, 150 and 200 m).
Step 3
With all these resampled maps, we can then carry out the Cross-Tabulation exercise at multiple resolutions. To do this, open the "Semi-Automatic Classification Plugin", click on the "Postprocessing" tab and select Accuracy. Fill in the required parameters: raster to assess (Simulation CORINE 11 map at 50 m) and reference raster (CORINE 11 map at 50 m) (Fig. 
Step 4
Repeat the same procedure for the other pairs of maps at 100, 150 and 200 m.

## Results and Comments ##
After this function has been executed for each spatial resolution, QGIS will create an output map, a couple of matrixes and some statistical measures. All the tables and statistics can be consulted in the "output window" and all the results will be saved in the folder we selected earlier. For a detailed description of each of these results, please refer to the Sect. 1. The analysis of the matrixes at the different spatial resolutions shows no important differences between resolutions, and very similar results in all cases. In general, there is a high level of agreement between the simulation and the reference map, as studied above in the Sect. 1 when conducting the analysis at the original resolution of the modelling exercise.
If we take Overall Accuracy as a summary metric describing the similarity between the two maps, we can see that similarity is very high in all cases (Table 
We must also bear in mind the limitations for this exercise mentioned in the Sect. 1. Validating a simulation by cross-tabulating the simulated exercise with a reference map may be misleading. Most of the areas in both maps agree The best way to validate the changes modelled in our exercise is to focus exclusively on the simulated changes and on a map of reference showing the changes on the ground. In this case, the Multiple-Resolution exercise could provide very interesting insights, as agreement between simulated and reference changes may be higher at coarser spatial resolutions.
Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http:// creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. The overlaying of two map layers is a standard GIS procedure. As we saw in the previous chapter, it enables us to compute the intersection between two feature classes and cross-tabulate either the area or the pixel count of the intersecting features depending on whether raster or vector data are being used. Cross-tabulation can be used to evaluate different topics depending on the nature of the input data. In this chapter, cross-tabulation is used to assess land cover changes, the spatial agreement between maps and map accuracy. In Sect. 1, Land use/cover changes (LUCC) are quantified by comparing two LUC maps, computing different indices of change and creating a change matrix. In Sect. 2, we used various metrics to evaluate the spatial agreement between two maps. This procedure was applied to compare a LUC map with a reference map, a simulated LUC map with a reference map and a simulated LUCC map with a reference map of changes. Section 3 introduces the Kappa indices, which allow us to assess the agreement between two datasets, given the agreement expected by random coincidence. We used the indices to compare observed or simulated maps with a reference map. In Sect. 4 we evaluate the agreement between maps at a global level (the entire map) by focusing on a specific feature such as a smaller area or a particular category (stratum level). Finally, in Sect. where A 1 and A 2 are the category areas in question at dates 1 and 2, respectively. AC can be divided by the number of years between the two dates to obtain the average annual change area over the study period.
Relative change (RC) is obtained by normalizing the absolute change value by the category area at date 1.
This formula expresses the proportion of the category area that changed over the study period.
Other indices of LUCC include rates of change. The most popular rate of change is the annual rate of deforestation proposed by the 
An alternative equation, also based on the compound interest law, was proposed by 
Both formulae give similar results except when LUCC is very high, in which case r is significantly higher than t 
All the change indices presented above indicate net change, which results from the balance after gross losses have been subtracted from gross gains. For instance, a given forest category could show an absolute change of -2 ha, which could be erroneously interpreted as very little change, but in fact is the result of two opposing processes: the deforestation of 202 ha compensated by the reforestation of 200 ha. A more detailed analysis of change dynamics can be obtained by cross-tabulating the two maps at two different dates and drawing up a change matrix. The change matrix is a cross-tabulated table indicating the area covered by each change (or permanence) between a category at date 1 and another category at date 2. Many change indices can be obtained from this matrix (see, for example, Sect. 2). 

## Utility ##


## QGIS Exercise ##
Available tools
The indices of change proposed in this document are based on the area statistics for the two maps. These could be efficiently computed using a spreadsheet program. However, we suggest using a simple R script using the QGIS Processing R provider plugin. The script generates a table containing the absolute change (AC) area, the relative change (RC) area (both in hectares), the rates of change based on 

## Requisites ##
All maps must be in raster format and have the same resolution, extent and projection.

## Execution ##
If necessary, install the Processing R provider plugin and download the R script Change_Statistics.rsx into the R scripts folder (processing/rscripts). For more information, see Chapter "About This Book".
Step 1
Then, execute the script and fill in the required parameters (names and dates of the two maps and the output table) as shown in Fig. 
The script generates two tables in CSV format: a table showing the change indices (Table 

## Results and Comments ##
The two land covers with the most significant absolute change are Categories 1 (built-up) and 2 (agriculture). During the period 2000-2018, the built-up area is increased by 1840 ha, and agriculture lost 1987 ha. The built-up area increased by over 50%. The rates of change resulting from the two equations are very similar. The two categories with the largest rates are built-up (Category 1) and water (Category 6) areas. Over the period 2000-2018, the area covered by these categories increased by around 2.5% a year. Categories 2 and 4 (agriculture and scrubs) present a negative net change rate, indicating that their areas have been shrinking. The change matrix gives us more information about the  processes of change. One surprising change is the transition from 1 (built-up) to 6 (water). On closer observation, it was found that pits had been filled with water to create reservoirs.

## Areal and Spatial Agreement Metrics ##
Description Different authors have proposed a series of metrics that evaluate the areal and spatial agreement between two land use/cover maps or between any of their categories. These metrics are obtained from the cross-tabulation matrix and summarize in a single value the agreement between two maps.
The metrics are based either on the comparison of the proportion of total area occupied by a particular category on two maps or on the spatial coincidence of the pixels allocated to any given category on two maps. This review includes some of the most recently developed metrics. 
where X i refers to the number of pixels belonging to category i in map X, Y i refers to the number of pixels belonging to category i in map Y, XY ii refers to the number of pixels belonging to category i in both maps X and Y, N is the number of categories into which the pixels are classified and M is the number of pixels into which the maps are divided.
The overall spatial agreement (A 0 ) and the overall spatial inconsistency (OSI) metrics assess the spatial agreement between the categories in two maps. One metric can be obtained from the other. Whereas A 0 shows the spatial agreement (0-100%), the OSI shows the spatial disagreement (0-100%). Added together, they come to 100. 
where X i refers to the percentage of the total area represented by category i in map X, Y i refers to the percentage of the total area represented by category i in map Y, n is the total number of categories, N is the number of pixels and N i6 ¼j ð Þ is the number of pixels assigned to one category in Map X and a different category in Map Y.
Overall areal inconsistency (OAI) shows the agreement between two maps in terms of category proportions and is expressed in values of between 0 and 100. Users can also assess the areal and spatial agreement/disagreement at a category level through the individual areal inconsistency (AIC) and individual spatial agreement (A i ) metrics. The values for the latter range from 0 to 100, and a value of 100 means perfect agreement.
AIC does not have a standard scale of values, as these depend on the proportion of the total area of the map allocated to the category. It is therefore very difficult to compare the values for this metric between classes, so limiting its usefulness.

## Utility ##
Exercises 1. To validate a map against reference data/map 2. To validate a simulation against a reference map 3. To validate simulated changes against a reference map of changes
The areal and spatial agreement metrics assess the similarity between the two maps. They are obtained from the cross-tabulation matrix and therefore do not provide any additional information, in that the values they provide can also be obtained from the matrix. However, they are standard metrics that allow us to measure the agreement between two maps and summarize it in a single figure. In this sense, they are similar to the user's and producer's accuracy metrics and to Kappa indices. They are also complementary to quantity and allocation (dis)agreement metrics, as they can differentiate between spatial and quantity agreements. These metrics can be used to assess how similar a land use/cover map is to another map used as a reference, i.e. the real situation on the ground. They can also be used to check the similarity between a simulation and the reference map for the same year.

## QGIS Exercises ##
Available tools QGIS has no specific tool for calculating the metrics proposed by 
Exercise 1. To validate a map against reference data/map Aim To validate the CORINE 2011 land use map, take the SIOSE 2011 land use map as a reference. We will be focusing particularly on how the "urban fabric" and "industrial and commercial areas" categories are mapped in CORINE 2011.

## Materials ##


## CORINE Land Use Map Asturias Central Area 2011 SIOSE Land Use Map Asturias Central Area 2011 ##


## Requisites ##
All maps must be rasters and have the same resolution, extent, projection and number of categories. LUC categories must be coded consecutively from 1 to the maximum number of categories considered.

## Execution ##
If necessary, install the plugin Processing R provider and download the R scripts indicated above in the "Available Tools" table. Paste the R scripts into the R scripts folder. For more information, see Chapter "About This Book".
Step 1
Our maps do not comply with one of the requisites of the tools we will be using, in that the categories in our LUC maps are coded from 0 (agricultural areas) to 12 (background). The first step is therefore to reclassify the maps to ensure that all the categories are coded consecutively from 1 to 13. This is done using the Reclassify by table tool (Figs. 
Step 2
Once the maps comply with the requirements of the tools, the different metrics can then be calculated. To test the overall agreement between the assessed and the reference maps, we will calculate the overall spatial agreement (A 0 ), the overall areal inconsistency (OAI) and the overall spatial inconsistency (OSI). For their part, individual areal inconsistency (AIC) and individual spatial agreement (A i ) are used to assess agreement specifically for the "urban fabric" and "industrial and commercial areas" categories.
To calculate all these metrics, open the respective tool and select the maps you want to compare (Fig. 
For class-specific metrics indicate the codes of the classes you want to validate (Fig. 

## Results and Comments ##
After calculating all the different metrics, a numerical output is obtained for each one (Tables 
There is a high overall spatial agreement (close to 90%) between the two maps and low areal inconsistency (around 3%). We can therefore consider the CORINE land cover map for 2011 as validated. The category proportions between CORINE and SIOSE are almost identical and the At the class level, the picture is slightly different. For the two classes we assessed (urban fabric and industrial and commercial areas) spatial agreement between the two maps to be close to 70%. Although this is a high level of agreement, it is much lower than the overall figure. This could be due to the fact that these two classes are more sensitive than others to the scale difference between SIOSE and CORINE.
In order to interpret the AIC metric, we need to first understand the proportion of total area allocated to each class on the two maps. AIC is half of the difference between the two proportions (i.e. if the proportion allocated to one class is 3% on one map and 4% on the other, the difference is 1% and AIC is 0.5). In our case, the AIC value for urban fabric is less than 0.1, which means a high level of agreement between the two maps regarding the proportion of total area allocated to this category (around 3.9%). The proportion allocated to industrial and commercial areas is around 3% in both maps and the AIC value is slightly more than 0.1. This also indicates a high level of agreement, although less than for urban fabric.

## Exercise 2. To validate a simulation against a reference map ##


## Aim ##
To validate the simulation obtained by our land use/cover change modelling exercise. We will focus on the two categories we have modelled actively: "urban fabric" and "industrial and commercial areas".

## Materials ##


## CORINE Land Use Map Asturias Central Area 2011 Simulation CORINE Asturias Central Area 2011 ##
Requisites All maps must be rasters and have the same resolution, extent, projection and number of categories. LUC categories must be coded consecutively from 1 to the maximum number of categories considered.

## Execution ##
Step 1
The first step is to reclassify our maps to make them comply with the requisites of the tools we will be using. These tools require the categories to be consecutively coded from 1. This means that "agricultural areas" (coded 0) must be given a new code (Fig. 
Step 2
Once the maps comply with the requirements of the tools, we can then calculate the different areal and spatial agreement metrics using the tools available in the R toolbox.
To evaluate the global agreement between the simulation and the reference map, we will calculate the overall spatial agreement (A 0 ), the overall areal inconsistency (OAI) and the overall spatial inconsistency (OSI). To evaluate agreement for the categories that we actively modelled, we will calculate the individual areal inconsistency (AIC) and the individual spatial agreement (A i ).
To calculate the metrics, open the corresponding tools and indicate the following: the simulation to be evaluated, the reference map (CORINE 2011), the background value of the maps (13) and the folder where the results will be stored.
For the class-specific metrics, you must also provide the codes of the classes you want to evaluate: in this case 3 (urban fabric) and 4 (industrial and commercial areas) (Fig. 

## Results and Comments ##
Once you have finished the exercise, you will obtain an a CSV file for each metric. The results are summarized in Tables 
The results show almost perfect agreement between our simulation and the reference map. The maps share the same LUC in 99% of their area and the areal inconsistency is insignificant (0.26%). A similar pattern is observed in the actively simulated classes.
These results are misleading. There is perfect agreement between our simulation and the reference map in the persistence areas. However, it is not that high for those areas modelled as changes. Because there are relatively few changes in our study area, the disagreement between the two maps in areas where change is predicted has very little impact on the overall high levels of the agreement created by the correct simulation of permanence areas. To correctly validate the changes that we simulated, we should repeat this exercise, focusing exclusively on the areas that changed in  

## Requisites ##
All maps must be rasters and have the same resolution, extent, projection and number of categories. LUC categories must be coded consecutively from 1 to the maximum number of categories considered.

## Execution ##
Step 1
Our maps do not comply with the requirements for the tools. In the map of simulated changes, the categories are not consecutively coded from 1. In addition, the reference map of changes has many more categories than the map of simulated changes.
Using the Reclassify by table tool we can adjust the number of categories on the two maps to the two categories that appear in both (urban fabric and industrial and commercial areas), plus a third category covering non-changing areas and changes that were not simulated. These categories will be assigned codes 1, 2 and 3, respectively. Figures 
Step 2
After reclassifying the maps, we will calculate the following metrics to validate the simulated changes: individual areal inconsistency (AIC) and individual spatial agreement (A i ).
As we are only comparing two categories, the overall metrics provide the same information as the individual ones.
For each metric, we will open the corresponding tool, indicating the map of simulated changes to be validated (Land use map 1), the reference map of changes (Land use map 2), the background value of the maps (0), the category we are  going to evaluate (urban fabric, 2, Fig. 

## Results and Comments ##
A CSV file will be created for each metric. The results are summarized in Table 
The same amount of changes took place in the reference map of changes as in our simulation. There is no disagreement on this point. However, unlike the previous exercise, the spatial agreement between the simulated and the reference changes was very low. The A i value for the two categories that were actively simulated was quite similar (less than 25%).
These results mean that only a quarter of the simulated changes were allocated in the same places as the changes observed on the reference map. This result, by itself, is not sufficient to consider the simulation invalid. We need to gain a better picture of the location of the changes that were simulated and their pattern. Even if they were not allocated in exactly the same places as on the reference map, they may be allocated in the same general area and follow a similar pattern, indicating that the model has correctly simulated the processes of change. To assess these aspects, we can perform 

## Kappa Indices ##


## Description ##
Kappa indices assess the agreement between two sources of spatial data, corrected by the agreement that is expected by chance. They are typically used to compare the agreement between two maps and to compare one map with reference information (e.g. a collection of validation points).
The first Kappa index (Cohen's Kappa) dates from 1960 
There are many critics of the widespread use of Kappa metrics, especially in LUCC modelling validation. There is now a general consensus that these indices should not be the only validation measures used when evaluating modelling exercises and maps. More information about the limitations of Kappa indices and the criticisms levelled against them can be found in 

## QGIS Exercises ##
Available tools 

## Requisites ##
The two maps must be rasters and have the same extent, spatial resolution, projection and legend. If they do not have the same legend, the user must reclassify the maps in such a way that they comply with this requirement.

## Execution ##
Step 1
Open the r.kappa function and fill in the required parameters: raster to be validated (CORINE map) and reference raster (SIOSE map) (Fig. 

## Results and Comments ##
Once the function has been executed, QGIS creates a new text file (.txt) in the specified folder. Users must manually access this folder to open the text file and see the results of the analysis. These include a cross-tabulation matrix of the maps, together with the Kappa value. For the two maps assessed, we obtained the following Kappa:
where 1 means total agreement, -1 total disagreement and 0 random agreement. A Kappa index value of 0.88 means that the two maps are very similar and therefore that our map has been validated. As a general rule, Kappas above 0.7-0.8 are considered good enough for validation. Kappas above 0.9 indicate very high agreement.
In our case, it is always important to bear in mind that SIOSE is made at a more detailed scale than CORINE. The two maps have different minimum mapping units and minimum mapping widths, which means that perfect agreement is impossible. The SIOSE map will always draw features that are not detected in CORINE because of its coarser scale. Kappa scores of almost 0.9, like this one, show almost perfect agreement between the two sources.
Users can also assess the agreement between CORINE and SIOSE at the category level so as to obtain more information about the similarities and dissimilarities between the two maps. To compute these metrics, they should refer to Exercise 3, using the Semi-Automatic Classification Plugin instead of r.kappa.

## Exercise 2. To validate a simulation against a reference map ##


## Aim ##
To validate the simulation obtained by our land use/cover change modelling exercise. 

## Requisites ##
The two maps being compared must be rasters and have identical resolution, extent, projection and legend. For proper validation, the reference map must refer to the same date for which the landscape was simulated.

## Execution ##
Step 1
Open the r.kappa function and fill in the required parameters: raster to be validated (Simulation) and reference raster (CORINE 2011) (Fig. 

## Results and Comments ##
QGIS will create a text file in the specified folder. This file contains the Kappa value for our simulation: Kappa ¼ 0:99 where 1 means total agreement. The Kappa value indicates that the two maps are almost the same. However, this does not mean that the changes we simulated are the same as the changes that took place on the reference map (CORINE 2011) as compared to the map used as the starting point for our modelling exercise 
In our simulation, most of the landscape remains unchanged. The high Kappa value indicates that we have correctly modelled the persistence of these unchanged areas. However, it is difficult to draw any meaningful conclusions about how closely the changes we simulated fit the changes observed between the CORINE 2011 and 2006 maps. These changes only affect very small parts of the maps and, therefore, do not have a meaningful impact on the Kappa index when evaluating the agreement between the entire area of the maps.
In order to gain a better picture as to how well the simulated changes fit the changes in the reference maps, other complementary metrics also described in this book can be used, such as the quantity and allocation disagreement or the figure of merit (see Sects. 3 and 4 in Chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps"). The agreement between simulated and reference changes can also be assessed using Kappa simulation, although this metric is not currently implemented in any tool in QGIS or in its associated software, such as R.
Users can also evaluate the kappa agreement between the simulation and the reference map at the category level, for which purposes they should refer to the next exercise, Exercise 3. 

## Requisites ##
The two maps to be compared must be rasters and have identical resolution, extent, projection and legend. For proper validation, the reference map must refer to the same date for which the landscape was simulated.

## Execution ##
Step 1
The Kappa index can be calculated at the category level for all the categories in our map using the Semi-Automatic Classification Plugin. To this end, open the plugin and select the Accuracy (Postprocessing) option from the menu. Then choose the rasters to be assessed, i.e. the simulation and the reference map (Fig. 

## Results and Comments ##
After executing the tool, we obtain a raster that cross-tabulates the compared maps and a CSV file with the The Kappa values for the two maps show high levels of agreement at both a general level and for all categories (Table 
The class with the lowest Kappa value is "Built-up areas". This indicates that many of the changes in this category have not been correctly simulated, which is to be expected given the dynamism of this category when compared with others such as forest or water surfaces. It is normally easier to simulate static land categories than changing ones. This explains why "Built-up" areas obtained a very low Kappa score compared to the overall score (Table 
Although these results offer some clues as to how well the changes in some categories were simulated, to obtain a more detailed understanding other methods and metrics should be used, such as the quantity and allocation disagreement and the figure of merit (see Sects. 3 and 4 in Chapter "Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps") or the Kappa simulation metrics. Whereas the Kappa metrics calculated here assess the agreement between persistent and changing areas in the compared and the reference maps, the other tools and methods focus on the specific areas that change between the initial and the final year of the simulation. This is a key element for understanding the success of our simulation, as it is easier to model persistence than change.

## Agreement Between Maps at Overall and Stratum Level ##


## Description ##
The aim is to assess the agreement between map pairs such as a reference map and a simulation map, at different levels:
overall agreement for the whole map, agreement for a given stratum, a smaller area, formed by a particular territory, LUC category or transition or by sample areas according to a gradient such as distance to a road. The purpose of this validation method is encapsulated in the following question: Does a particular item or area of interest show the same prediction score as the whole map?
Utility Exercises 1. To validate simulated changes against a reference map of changes A given map (LUC map, simulation) can be evaluated more precisely at spatial level (specific territory), category level (Is the simulation closer to the real situation for built-up areas or for forests?) or specific transitions (Does the model work better for the transition from forest to agriculture or from forest to pasture?). In this context, the entire area of interest can be used as a guide for interpreting particular simulation scores.

## QGIS Exercise ##
Available tools Agreement between maps at the overall and stratum levels is more a validation approach than a specific method. Accordingly, there are no specific tools available in QGIS to carry out this analysis, as the used tool will depend on what type of analysis will be carried out at the overall and stratum levels.
For general operations, we will make use of the QGIS Raster Calculator. a generic tool performing all kinds of raster calculations. To calculate Kappa indices at the global and stratum levels, we will make use of r.kappa. For more information about this tool, please refer to the previous section.
Exercise 1. To validate simulated changes against a reference map of changes

## Aim ##
To find out if the agreement between an observed (reference map) and a simulated transition varies for several distance-based categories resulting from a driver (e.g. distance to roads). 

## Materials ##


## Requisites ##
All maps must be in raster format with the same resolution, extent and spatial reference system (SRS).

## Execution ##
Step 1
First, we have to obtain the observed and simulated transitions from agriculture and pasture land to built-up areas over the period 2012-2018. Using the raster calculator, we extract the observed ("CLC_2012@1" = 2 AND "CLC_2018@1" = 1) and the simulated ("CLC_2012@1" = 2 AND "CLC_pre-dict_2018@1" = 1) transition from agriculture and pasture land (Category 2) to built-up areas (Category 1). The result is shown in Fig. 
Step 2
The Reclassify by table raster analysis tool is used to transform the map showing the continuous distance from roads into Figure 
The next step is to compute observed and predicted transitions from Category 2 to Category 1 as a function of the road distance classes. To this end, we use the Raster calculator again to calculate: i) the road distance class map multiplied by the observed transition map and ii), the road distance class map multiplied by the simulated transition map. The results can be seen in Fig. 
Step 4 Finally, we compare observed and simulated transitions as a function of distances classes (strata). We use the Raster layer unique values report raster analysis tool to calculate the number of pixels for each road distance category (observed and simulation) for the transition from category 2 to 1 as shown in Fig. 
The results are then converted into percentage as shown in Table 

## Results and Comments ##
The result is that there are almost three times as many observed transitions as predicted transitions. However, the proportion of near-to-road transitions is approximately the same. In conclusion, the model underestimates the quantity of agriculture and pasture land that is transformed into built-up areas, although in the areas close to roads, it accurately predicted what happened in the Ariège Valley between 2012 and 2018. 

## Description ##
The thematic accuracy assessment statistics are a set of parameters that measure the degree of agreement between the LUC map and the reference data (for more details about reference data, see Chapter "Sample Data for Thematic Accuracy Assessment in QGIS"). Overall accuracy, user's accuracy and producer's accuracy are reported in many studies. Some additional accuracy measures such as the standard error of overall accuracy and the confidence intervals for the adjusted areas are also helpful.
All these parameters are mainly derived from the error or confusion matrix (see Chapter "Basic and Multiple-Resolution Cross-Tabulation to Validate Land Use Cover Maps"). This matrix is obtained from a cross-tabulation between the reference data and the thematic map. In the resulting table, the reference data are generally shown in the columns and the map data in the rows (Table 
In Table 
Expressing the error matrix in terms of area proportions instead of sample counts enables the calculation of unbiased area estimators. The area proportions (b p ij ) are defined as follows:
where W i = (Map area of class i)/(Total area of the map).
Based on these area proportions, the overall estimated accuracy ( b O), user's accuracy ( b U i ) and producer's accuracy ( b P j ) are calculated with the following equations:
Errors of commission and omission are complementary concepts of the user's and producer's accuracy metrics, respectively (i.e. error = 1accuracy). An error of commission occurs when a feature is included in a thematic class to which it does not belong. In contrast, an error of omission occurs when a feature is excluded from the thematic class to which it belongs 
A j is the unbiased area estimator or adjusted area. In this case, the area estimator obtained directly from the map (A total ) is then adjusted by a factor obtained from the reference data. If there are more samples labelled as class j in the reference sample than in the map, then b A j will be larger than the area obtained directly by pixel counting.

## Utility ##
Exercises 1. To validate a map against reference data/map
The statistics obtained from the thematic accuracy assessment are not only descriptors of the map quality but also represent a fundamental input for calculating unbiased area estimators. Additionally, they provide the necessary elements to decide whether to increase the number of sampling sites in the reference data, if the precision obtained does not meet the initial mapping objectives.

## QGIS Exercise ##
Available tools

## • MapAccurAssess Plugin ##
In QGIS, several plugins, such as Semi-Automatic Classification, AcATaMa and MapAccurAssess, can be used to calculate the map accuracy statistics. All three plugins provide the overall accuracy, producer's accuracy, user's accuracy and the error matrix, although AcATaMa and MapAccurAssess also report some additional statistics about the adjusted areas and their levels of accuracy.
In this exercise, we use the MapAccurAssess plugin because it can use a shapefile directly with the reference data. The results provided by this plugin, based on 
This plugin is a test version and has not yet been accepted in the official QGIS repositories.
Exercise 1. To validate a map against reference data/map Aim To validate a LUC map for the Marqués de Comillas study area by computing accuracy assessment statistics and the error matrix via cross-tabulation of the reference data and the thematic map.

## Materials ##


## Marqués de Comillas Land Use Cover Map 2019 ##
Photointerpreted reference dataset-Marqués de Comillas 2019 (reference dataset resulting from the exercise in Sect. 2 in Chapter "Sample Sata for Thematic Accuracy Assessment in QGIS")

## Requisites ##
In order to compute the areas, the land cover map must be in raster format (GeoTiff) in any cartographic projection. The reference data must be contained in a shapefile with the same type of projection as the map. The shapefile attribute table must contain at least two columns, showing the value for the thematic class obtained from the land cover map and the value according to field ground-truthing or photointerpretation. Both columns must have the same data type (integer or text) to be comparable. Each row of the table corresponds to one reference site.

## Execution ##
Step 1
Install the MapAccurAssess plugin. Should you need help, please see Chapter "About This Book" and the plugin's documentation.
Step 2
If the plugin has been successfully installed, an icon should appear in the main graphics panel. To start the exercise, click on this icon. Alternatively, go to the Complements menu, select Accuracy Assessment and then Accuracy Assessment again.
Step 3
Select the shapefile with the reference samples (Photo-interpreted reference dataset-Marqués de Comillas 2019) 2 and indicate the column with the reference data and the column with the values for the thematic classes used in the map. After that, select the land cover map you want to assess (Marqués de Comillas Land Use Land Cover Map 2019). If the map is in vector format, indicate the column containing the thematic class values. Finally, select a folder where the results will be saved and click "Accept" (Fig. 

## Results and Comments ##
The output of this plugin consists of two CSV tables. The first contains the error matrix (Table 
According to the data from this exercise, the overall accuracy of the map is 0.91. In other words, there is a high probability (91%) that a randomly selected location on the map will be correctly classified. Note that the thematic class with the lowest accuracy is 130 (Wetland), with a user accuracy of 0.7 and a producer accuracy of 0.21. This class covers a small area (252 ha according to the map). We decided to keep this class to show that illogical situations can occur when there is only a small number of sampling sites, e.g. negative areas. However, we recommend merging class 130 with another class of similar characteristics and recomputing. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## References ##


## Pontius Jr. Methods Based on a Cross-Tabulation Matrix to Validate Land Use Cover Maps ##
Martin Paegelow, Jean-François Mas, Marta Gallardo, María Teresa Camacho Olmedo, and David García-Álvarez

## Abstract ##
Several validation techniques based on the cross-tabulation matrix can be applied to validate Land Use Cover (LUC) maps. The exercises in this chapter focus, in particular, on the cross-tabulation techniques proposed by Robert Gilmore Pontius Jr., who has developed many indices and techniques in this field. Given his major contribution to this family of validation techniques, we have associated his name here with cross-tabulation techniques without this in any way implying that his scientific activity is limited to this field. The null model (Sect. 1) is especially useful for validating simulations, comparing the modelled map to a reference map with full persistence. LUCC budget (Sect. 2) only focusses on changes, which it splits into different components. This method can be used to compare the changes we want to validate with a reference set of changes, so providing interesting information as to how well our maps capture the dynamics of the landscape. Quantity and allocation disagreement (Sect. 3) analyse the differences between the reference map and the map being validated using two indices: disagreement in quantity and disagreement in allocation. The Figure of Merit (FoM) (Sect. 4) technique is used to validate a set of LUC changes by comparing them with a reference, distinguishing between different components of agreement: correctly simulated change, wrongly simulated or missing change. Incidents and States (Sect. 5) allows us to identify illogical transitions in a time series of maps by providing the number of states and transitions that a cell undergoes over the course of the series. Intensity analysis (Sect. 6) and Flow matrix (Sect. 7) also enable us to validate the logic of LUC changes in a time series of maps. Intensity analysis provides information on the speed of changes, identifying those transitions or changes that do not follow a logical trend, while the flow matrix enables us to spot unstable changes in a series of maps. In this chapter, we present examples of how these techniques can be used in different cases: to validate single LUC maps, to validate a series of maps with two or more time points, to validate simulated changes against a reference map of changes and to validate changes simulated by various models. All these techniques are illustrated by exercises using datasets from the Asturias Central Area and the Ariège Valley. 

## Description ##
The null model is a method specifically developed by 
If the agreement between the observed LUC at t 2 and the simulation map at t 2 is higher than that between observed LUC at t 2 and the so-called null model, the simulation has greater predictive power than the hypothesis of complete persistence (no change). The agreement between the null model, the simulation and the reference map is usually assessed using common cross-tabulation techniques and Kappa indices (see Sect. The null model helps to measure the relative success of a simulation compared to persistence in time. The usefulness of this method depends on the spatiotemporal dynamics of the study area.
The method is based on the hypothesis that a simulation is successful if it gets better validation scores than a landscape in which no changes occur. When simulating change in a study area in which little change is taking place, it may be difficult to correctly simulate these changes in the same positions as on the reference map of changes. As a result, the null model may provide better validation scores than the simulation, in that the null model avoids possible errors when allocating changes and always simulates persistence correctly. This is why the null model is especially useful for validating whether an LUCC model simulates persistence correctly. 

## QGIS Exercise ##


## Requisites ##
All maps must be rasters and must have the same resolution, extent and projection.

## Execution ##
Step 1
The first step is to calculate the Kappa indices measuring the agreement between the simulation, the null model and the reference map showing observed LUC in 2018. We use the GRASS r.kappa raster tool to calculate the kappa values for agreement: (i) between observed LUC in 2012 duplicated in 2018 (null model) and observed LUC in 2018 and (ii) between observed LUC in 2018 and simulated LUC in 2018.
Step 2
We then generate the cross-matrices between the simulation, null model and reference map (CLC_2012 against CLC_2018 and CLC_predict_2018 against CLC_2018) using the Cross-classification tool (see Exercise 2 of Sect. 1 in Chapter "Basic and Multiple-Resolution Cross-Tabulation to Validate Land Use Cover Maps"). This method complements the kappa agreement indices and provides additional information about the similarity between the different maps. Step 3
Once the cross-tabulations are obtained, on a spreadsheet we calculate the sum of cells on the diagonal (pixel-to-pixel correspondence).

## Results and Comments ##
The resulting Kappa values are 0.9849 for the simulation (CLC_predict_2018 related to CLC_2018) and 0.9875 for the null model (CLC_2012 related to CLC_2018). The quantity and allocation correspondence (the proportion of diagonal pixels in the cross-matrices) are 98.22% for the simulation and 98.53% for the null model. Therefore, with both techniques, the null model obtains a slightly higher score than the simulation. Interpretation of these results is difficult and has to be done carefully due to the limitations of this technique and the criticisms often levelled against it. The results show that persistence is the dominant process (98.5% of the study area did not change between 2012 and 2018; null model). Taking into account that most models simulate persistence better than change, it would be difficult to obtain a higher prediction score for a study area in which so little land use change is taking place. The low proportion of changes makes it difficult to simulate the changes between land use categories correctly. The slightest error diminishes the performance of the simulation compared to the null model.
Other methods, such as the Figure of Merit (see Sect. 4), can provide a better picture on how the model correctly simulated the change.

## LUCC Budget ##
Description LUCC budget is a technique for analysing land use/cover change (LUCC) using the cross-tabulation matrix obtained by overlaying two maps of the same area at two different dates. For each category, the changes are characterized in four components: gross gains, gross losses, net change and swap 
Gross gains are the areas gained by each category, and gross losses are the areas lost. Net change is the difference between gains and losses. In categories in which gains and losses are occurring in different places, swap is a measure of the real changes taking place which are not revealed by the net change indicator. It measures the total area in which an equivalent amount of gains and losses have taken place, i.e. if in one category there are gains of 5 ha in one place and losses of 3 ha in another, the 3 ha that it losses in one place and recoups in another are the swap (swap = 3 + 3 = 6 ha), while the remaining 2 ha (5-3) are the net change.

## Utility ##
Exercises 1. To validate a series of maps with two or more time points When monitoring landscape changes, the LUCC budget technique helps to identify the most critical land use transitions and should ultimately facilitate linking patterns to process 

## QGIS Exercise ##
Available tools
The components of change computed by the LUCC budget are derived from the cross-tabulation matrix. This matrix can be obtained by overlaying the two maps in QGIS and then calculating the LUCC budget values using a spreadsheet programme. However, we suggest using the LUCCBudget. rsx R script with the QGIS Processing R provider plugin. This script will carry out the entire LUCC budget calculation and will generate a table containing the values for the four components of change.
See Chapter "About this Book" for more detailed information about how to integrate R into QGIS and how to use R scripts such as the one applied in this exercise.

## Exercise 1. To validate a series of maps with two or more time points ##


## Aim ##
To carry out LUCC budget analysis in the Ariege study area using the CORINE Land Use maps dated 2000 and 2018.

## Materials ##


## CORINE Land Cover Map Val d'Ariège 2000 CORINE Land Cover Map Val d'Ariège 2018 ##
Requisites All maps must be in raster format and have the same resolution, extent and projection.

## Execution ##
If necessary, install the Processing R provider plugin, and download the LUCCBudget.rsx R script into the R scripts folder (processing/rscripts). For more details, see Chapter "About this Book".
Step 1
Then, run the script and fill in the required parameters (names of the two maps and the output table) as shown in Fig. 

## Results and Comments ##
The script will generate the cross-tabulation or change matrix as shown in Table 
3 Quantity and Allocation Disagreement Description 
When analysing a time series (or single maps evaluated against a reference map), this method can differentiate between the changes that are due to differences in the relative importance of certain categories (some increase and others decrease) and those derived from changes in the location of the elements that make up these categories. It also identifies the categories that undergo net changes and swaps. As regards differences in location, this method distinguishes between exchanges between classes and changes in the location of two or more classes.

## Utility ##
Exercises 1. To validate a series of maps with two or more time points Quantity and allocation disagreement assess how similar a simulation or simulation is to a reference map, differentiating between (dis)agreement that is due to the quantities of different classes and (dis)agreement caused by the allocation of these classes in different places. By providing the same information, this method can also be used to validate an LUC map against a reference map or to assess the LUC changes in a time series of maps and understand whether or not these changes follow a logical trend.

## QGIS Exercise ##
Available tools For more information about the use of r.cross, r.kappa, SAGA Confusion matrix and SCP, please refer to Chapters 

## Requisites ##
All maps must be in raster format with the same resolution, extent and spatial reference system (SRS).

## Execution ##
Step 1
In order to be able to make this analysis, the CORINE LUC map for 2018 must be polygonized. To this end, use the tool Polygonize.
Step 2
After polygonizing the CORINE raster, the next stage is to cross-tabulate the two maps we are going to compare. To this end, open the SAGA confusion matrix tool and select the CORINE LUC map for 2012 as Classification 1 layer and the CORINE LUC map for 2018 as Classification 2 layer. Then, fill in the parameters for the following lines-Value, Value (Maximum) and Name-into the function. Do not change any default options (the "Report unchanged classes" box must be ticked; output as "cells" and open the results generated) (Fig. 
Step 3
Import the SAGA-generated confusion matrix obtained in the previous stage into a spreadsheet software such as Excel.
Then translate the obtained matrix into percentages (Table 
Step 4
Finally, use the SAGA-generated confusion matrix obtained in Step 2 to calculate the quantity and allocation disagreements in a spreadsheet software such as Excel. For a pixel resolution of 15 Â 15 m, 1 ha corresponds to 44.44 pixels.
Quantity disagreement is calculated by subtracting column total from row total (quantity disagreement = row totalcolumn total) (Table 

## Results and Comments ##
Table 
Table 
Allocation disagreement corresponds to all not-diagonal cell values. These may be expressed as gains (2018-intersection 2012 against 2018) and losses (2012-intersection 2012 against 2018). While in some classes there are net changes (e.g. scrubland is the only category with net losses and no net gains), the changes in agriculture and pasture land are almost all losses (1.05), with just a few small gains (0.08%) from scrubland. This means that quantity disagreement shows a negative net balance for agriculture and  
• CORRECT REJECTIONS (E) = the real maps show persistence and the simulation shows persistence.
Two complementary measures can be obtained using the same components of the Figure of Merit:
• Producer's accuracy: A measure calculated using the ratio B/(A + B + C), which expresses "the proportion of pixels that the model predicts accurately as change, given that the reference maps indicate observed change" 

## Utility ##
Complementary analyses to the Figure of Merit and the Producer's and User's accuracies include spatial metrics, Kappa indices, the Land Use and Cover budget (LUCC budget) technique and Quantity and Allocation disagreement. These indices are described in Sects. 2 and 3 of this chapter. Chapter "Basic and Multiple-Resolution Cross-Tabulation to Validate Land Use Cover Maps", QGIS includes many tools for cross-tabulating spatial data in the GRASS and SAGA toolboxes. The "Semi-Automatic Classification Plugin" also includes cross-tabulation tools.

## QGIS Exercises ##
Of all the tools available in QGIS, in this book, we recommend the "Semi-Automatic Classification Plugin", which is the most efficient, most stable tool of all those assessed. 

## Requisites ##
The maps must have the same extent, spatial resolution, projection and legend. If they do not have the same legend, the maps must be reclassified to meet this requirement. For a proper validation, the latest reference map must refer to the same date as the simulation.

## Execution ##
Step 1
We begin by obtaining two rasters showing the areas that changed in the study area during the period analysed and those that remained the same. This procedure must be done twice: once for the reference map (CORINE 2005-CORINE 2011) and once for the simulated map (CORINE 2005-Simulation 2011).
To obtain these maps, open the "Semi-Automatic Classification Plugin" and the "Postprocessing" tab. Then select Land cover change and fill in the required parameters: the earlier map in the reference classification 
Run the tool to obtain two output maps showing the changes on the reference map (CORINE) and the changes simulated by the model. Both will refer to the same period 
Step 2
The next stage involves cross-tabulating the two maps of changes. To obtain these maps, open the Semi-Automatic Classification Plugin and in the "Postprocessing" tab, select Accuracy. Select the required parameters: classification to assess (simulated changes) and reference raster (CORINE 05-11 changes) (Fig. 

## Results and Comments ##
Step 1 produces two maps of changes, which are stored in the folder specified by the user. The function also generates a matrix for each pair of cross-tabulated maps. These matrices appear in the "output" window, stored in CSV format. They show each possible combination between the two cross-tabulated maps and the code under which each combination is represented in the output raster.
Only four transitions (new codes 3, 4, 16 and 17) are simulated by the model, as expressed in Table 
Most of the changes predicted in the simulation refer to the transition from agricultural areas (Category 0) to urban fabric (Category 2) and to the transition from agricultural areas to industrial and commercial areas (Category 3). Together, they represent 1,546 of the 1,632 pixels simulated. That is, almost 95% of the simulated pixels. In the reference map, these transitions represent 751 and 503 pixels, respectively, a less significant proportion of total change (in italics in Table 
After completing Step 2, we now have a cross-tabulation raster and a table showing every possible combination between the two cross-tabulated maps (Table 
Following the definitions provided by 
The WRONG HITS correspond to combinations where both the reference map and the simulation show change, but to different gaining categories. For example, new code 13 (old codes 3 and 4) refers to areas that were agricultural areas that changed to urban fabric in the simulation and to industrial and commercial areas in the reference map (Tables 
FALSE ALARMS refer to areas that are marked as persistence in the reference map and as change in the simulation. Examples include new code 2 (old codes 0 and 3). Areas with that code refer to pixels that were simulated as urban fabric in the simulation, but do not show change in the reference map. Code 0 does not appear among the codes in Table 
MISSES refer to the areas where the reference map shows change but the simulation shows persistence. Examples include code 16 (old code 4 and 0). Finally, CORRECT REJECTION refers to the pixels marked as persistence in the reference map that were correctly simulated as persistence (new code 1, old codes 0 and 0).
In total, HITS account for 347 pixels, WRONG HITS for 89 pixels, FALSE ALARMS for 1,196 pixels and MISSES for 4,869 pixels (Table 
With all the above information, we can finally calculate the We must also consider that the Figure of Merit compares the simulated changes with all the changes in the reference map. In our simulation, we only modelled two categories actively (urban fabric and industrial and commercial areas). This means that the changes in all the other categories were not even simulated and no agreement can therefore be expected. This limitation must be borne in mind when evaluating the Figure of Merit.
The best way to obtain a Figure of Merit that offers objective information about the validity of our modelling exercise is to repeat the same exercise, focusing exclusively on the actively modelled transitions (from agricultural and vegetation areas to urban fabric and industrial and commercial areas).
Producer's accuracy (B/(A + B + C)) is 6.54% and expresses the number of pixels that the model accurately predicts as change as a proportion of total observed change. For its part, User's accuracy (B/(B + C + D)) measures the number of pixels that the model predicts accurately as change as a proportion of total predicted change, in this case 21.26%.
As regards the four simulated changes, shown in Table 

## Exercise 2. To validate simulated changes against a reference map of changes in a binary format ##


## Aim ##
To validate the change simulated by a model against a reference map of changes for the same simulation period. To do this, we overlay two maps that show change versus non-change over the same period. The initial map in both cases is the CORINE dataset for 2005. The changes from 2005 to 2011 are calculated for the simulation and for the CORINE dataset as reference. In this exercise we do not evaluate the WRONG HITS. 

## Materials ##


## Requisites ##
The maps must have the same extent, spatial resolution, projection and legend. If they do not have the same legend, the maps must be reclassified so as to meet this requirement. For a proper validation, the latest reference map must refer to the same date as the simulation.

## Execution ##
Step 1
The first step is to obtain two rasters showing the areas that changed and those that remained the same over the period being analysed: one for the reference map 
Step 2
Once the two maps have been obtained, they must be reclassified into binary format, i.e. into a map with two possible values: 0 (persistence) and 1 (changes). This is done using the Reclassify by table tool.  
Step 3
Finally, the two binary maps must be cross-tabulated. To do so, open the "Semi-Automatic Classification Plugin" and, in the "Postprocessing" tab, select the Cross-classification option. Fill in the required parameters: classification (binary changes from the simulation) and reference raster (binary changes from CORINE) (Fig. 

## Results and Comments ##
Once we have completed Step 3, the QGIS creates an output raster that shows all possible combinations between the two binary change maps. The function also generates a table showing all possible combinations between the two input maps. This table appears in the "output" window, stored in CSV format. This table also lists the codes with which each combination is represented in the output raster. Table 
The sum of MISSES plus HITS (5,305 pixels) represents the change in the reference map (CORINE) for the period 2005-2011. These pixels cover just 0.9077% of the total study area. Very little change therefore took place in the reference map for our study area.
HITS plus FALSE ALARMS (1,632 pixels) gives all the pixels in which the simulation predicted change. These pixels cover 0.2792% of the total study area. This means that fewer changes were simulated than actually took place on the reference map. This makes sense given that in our simulation we only simulated the transitions from agricultural and vegetation areas to urban fabric and industrial and commercial areas, while the reference map also considered many other changes between all the other categories represented on the map, which were not simulated in our modelling exercise.
The Figure of Merit (B/(A + B + C + D)) for our simulation is very low at 6.7%. This indicates that the simulation did not simulate most of the changes that took place in the reference map correctly. This is partly due to the fact that we only actively modelled two categories, while the reference map showed the changes that took place between all categories. As a result, overlap between the two maps is impossible in many areas. Even so, the general level of overlap between the simulated changes and those observed on the reference maps is still quite low. Other metrics and tools must therefore be used in order to interpret the simulation and the performance of the modelling exercise better.
The Figure of Merit in this exercise is a bit better than in the previous one because we did not take WRONG HITS into account. In this case, we only compared changes, without taking into account the type of change that happened in the simulation period.

## Exercise 3. To validate the changes simulated by various models ##


## Aim ##
To compare and validate the change simulated by two models. For this purpose, we overlay three maps that show change versus non-change over the same interval. The initial map in all cases is the CORINE dataset for 2005. The changes from 2005 to 2011 are calculated for the simulation from model 1, for the simulation from model 2 and for the CORINE dataset as reference. WRONG HITS are not evaluated in this exercise.  

## Requisites ##
The maps must have the same extent, spatial resolution, projection and legend. If they do not have the same legend, the maps must be reclassified so as to meet this requirement. For a proper validation, the latest reference map must refer to the same date as the simulation.

## Execution ##
Step 1
The first step is to obtain three rasters for the study area showing the areas that changed and those that remained the same over the period being analysed. In this way, we obtain: The three output maps will show the change areas and the persistence areas for each of the three maps (the reference CORINE map and the two simulations) under consideration.
Step 2
Once these three maps have been obtained, they must be reclassified into binary maps in which persistence areas are reclassified as 0 and change areas as 1. The maps are reclassified using the Reclassify by table tool.
Step 3
The three binary maps must then be cross-tabulated, so as to be able to assess the congruence between the simulations and the reference map.
To do this, open the "Semi-Automatic Classification Plugin" and the "Postprocessing" tab, and then select Cross-classification. Start by cross-tabulating the two simulations you want to compare. To this end, fill in the following parameters: classification (binary map of changes from simulation 1) and reference raster (binary map of changes from simulation 2) (Fig. 
Step 4
The procedure is repeated again, this time cross-tabulating the raster obtained in the previous step with the reference map. In this case, open the tool and fill in the parameters as follows: classification (raster obtained after running the tool as explained in the previous step) and reference raster (CORINE 05-11 binary map of changes) (Fig. 

## Results and Comments ##
After carrying out Steps 3 and 4, QGIS creates two output rasters. The function also generates a table for each raster, which appears in the "output" window in CSV format. This table shows every possible combination between the values of the cross-tabulated maps. It also lists the codes under which each combination is represented in the output raster.
The raster obtained in Step 3 measures the agreement between the two simulations (Table 
The raster obtained in Step 4 was produced by cross-tabulating a reference change map with the raster obtained after cross-tabulating the change maps produced by the two simulations. This cross-tabulation therefore produces eight possible combinations (Table 
In order to interpret the results of this second cross-tabulation correctly, we need to understand the values of the two rasters that were cross-tabulated. In the reference change map, 0 refers to persistent areas and 1 to areas that changed during the period under consideration. The meanings of the new codes in the raster obtained in Step 3 are detailed in Table 
This enables a better interpretation of the results of the last raster generated. New code 1 (previous codes 0/1) refers to areas in which persistence was observed on the reference map of changes (code 0) and was also simulated by the two   
New code 5 (1/1) corresponds to areas where both models simulated persistence and the reference map showed change (DOUBLE MISSES). New code 8 (1/4) refers to areas where the two models and the reference map also showed change (DOUBLE HITS). Finally, the other four combinations refer to areas where each simulation shows a different agreement with the reference map (Table 
These eight possible combinations are expressed as two maps. The first map (a zoomed area is shown in Fig. 
According to all the above results, it seems that the two simulations are very similar in terms of predictive accuracy. The vast majority of the pixels on the map are DOUBLE CORRECT REJECTIONS, which means that both models are very accurate when predicting persistence. This makes sense in that persistence is very easy to simulate in a highly stable area like the one we simulated. The most challenging task is to correctly simulate change. The best  If we focus exclusively on the areas that changed, the accuracy is very low. 86.1451% of the pixels were DOUBLE MISSES, while in the remaining pixels there were HITS in one or both models. This means that in the vast majority of cases, our models incorrectly simulated change. These simulations cannot therefore be validated, although other validation tools can be used to check whether the simulated pattern is valid. In this regard, even if a hard comparison does not show a high level of agreement between a simulation and the reference map, the pattern of the simulated changing areas may be logical or correct. The models can therefore be considered valid in a qualitative sense.

## Incidents and States ##


## Description ##
Incidents and states are terms proposed by 
Incidents refer to the number of times a pixel changes category over the course of a time series. There can be as many incidents as there are stages in the time series. In a series of 5 maps, there are 4 time-stages. The series may therefore have between 0 and 4 incidents, i.e. the pixel may change category between 0 and 4 times. The number of incidents can also be referred to as "Transition frequency".

## Utility ##
Exercises 1. To validate a series of maps with two or more time points
The number of incidents and states assigned to the pixels in a time series of Land Use Cover maps can help us identify the changes that take place for technical reasons, i.e. erroneous or spurious changes which do not really happen on the ground.
When obtained from satellite imagery classification, Land Use Cover maps usually have important sources of uncertainty. Various different Land Use and Cover categories can have very similar levels of reflectance. If the imagery is obtained at different times of the year, or under different atmospheric conditions, the reflectance of a pixel can vary to a similar extent to the difference in reflectance between two Land Use Cover categories. The same pixel could therefore be classified under different categories over the course of the time series. The number of incidents and states of the pixel can potentially help us to identify these errors.
For example, in a time series of six maps, if a pixel has five incidents, but only two states, it means that it alternates between these two categories at each stage in the time series. If we discover which categories are involved in the transitions we can determine to what extent these changes are logical. Incidents and states can also be used to validate a series of simulations, when working with modelling exercises to obtain scenarios for more than two time points.

## QGIS Exercise ##
Available tools
The GRASS toolbox associated with QGIS has a tool for calculating the number of states in a time series of Land Use Cover maps. QGIS does not provide any specific tool to calculate the number of incidents in the time series, so this metric must be calculated manually. This is done using the raster calculator and a raster reclassification tool. QGIS offers several raster calculators and reclassification tools. Although they are all valid, in this exercise we will be using the ones from the core QGIS toolbox. 

## Aim ##
To find out if technical changes may have taken place in the last series of CORINE Land Cover maps produced for the Asturias Central Area.

## Requisites ##
All maps must be rasters and have the same resolution, extent and projection.

## Execution ##
Step 1
In order to calculate the number of states per pixel, we must open the r.series tool and select all the maps that form part of the series of Land Use Cover maps we are analysing ("Input raster layer(s)"). In this case, we select the three maps in our series: CORINE Land Cover 2005, 2011 and 2018.
In the "Aggregate operation [optional]" option, select "Diversity". This will count the number of different categories to which a pixel is assigned over the course of the time series.
In "Advanced parameters", indicate the range of values of the Land Use Cover maps introduced as input, i.e. the minimum and maximum values. In our case, the minimum value for a category is 0 and the maximum value is 12 (Fig. 
The final stage is to indicate where the new map will be saved.
Step 2
There is no specific tool for calculating the number of incidents in a pixel over the course of a time series. This operation must therefore be carried out manually. The first step is to identify where the changes happened. For each pixel, we must then calculate the number of times it underwent change (or not). To carry out these operations, we have to work with pairs of maps: first 2005 and 2011 and then 2011 and 2018.
To identify where the changes happened, for each pair of rasters we must subtract one raster from the other. If a pixel does not change, the result of the subtraction will be a value of 0 for that pixel. If the pixel changes, the result of the subtraction will be a value other than 0.
The subtraction operation is carried out using the Raster calculator, in which we must write the following subtraction expression for each pair of maps:
We also need to indicate which raster is the reference map that will be used to define the characteristics (extent, spatial resolution and projection) of the new raster obtained after the calculation. In this case, we will be using the first map in our series 
Step 3
Once the previous step has been completed, the maps obtained must be reclassified to enable us to identify the pixels where an incident took place (values other than 0) and the pixels that were incident-free at each stage (0 values).
To identify all pixels in which incidents took place with a value of 1, we reclassify all values other than 0 as 1 using the Reclassify by table tool (Fig. 
That means that all values between -999 and -1 will be reclassified with the value 1. The same will be true for all values between 1 and 999. If as a result of the raster subtraction we get bigger negative values than -999 or bigger positive values than 999 we will need to adjust the values in the reclassification table accordingly.
Step 4
The last step is to count the number of incidents for each pixel over the course of the time series. This is done using the Raster calculator, which adds together the rasters we reclassified in the previous step using the following expression:
The CORINE 2005 map will be used as a reference to define the characteristics of the output raster (Fig. 

## Results and Comments ##
After completing all the operations described above, two different maps will be obtained: one with the number of states per pixel and another with the number of incidents per pixel.
The above maps (Fig. 
During the first stage of this process, the overall rate of land use change over each time interval is analysed to assess whether change was relatively fast or slow. To this end, the average annual rate of change for each time interval is compared with the average annual rate of change for the whole period.
The second stage analyses the intensity of change at category level within each time interval relative to the overall change rate for the interval calculated in stage one. It measures the gross losses and gross gains in area for each category so as to analyse whether the category shows a similar, stable pattern across the various time intervals in terms of the intensity of gains and losses. These observed intensities for each category are compared with an average annual rate of gains/losses that would exist if the changes within each interval were distributed uniformly over the entire time interval. This shows which categories are relatively dormant or active.
The final stage is at transition level. It examines the intensity of a particular transition over a given time interval, taking into account the different sizes of the categories and relative to the results of the category-level analysis. The gains made by a specific category may vary in size and intensity among the different categories from which it makes these gains. By comparing the observed rate of gains from each category with a uniform rate of gains that would exist if the gains were made uniformly from among all the available categories, we can identify those categories that are intensively avoided or targeted. Losses can be analysed in a similar way.
Intensity analysis also allows us to determine whether a particular transition occurs at a stable rate or occurs more intensely over a particular time interval within the series. If the same category is targeted (or avoided) over all the different time intervals, then this transition is said to be stationary.

## Utility ##
Exercises 1. To validate a series of maps with two or more time points Intensity analysis analyses the size and intensity of land changes. It also checks for stationarity and takes the relative size of the categories into account, rather than just the absolute gains or losses they may undergo.
At the interval level, users can identify how quickly or slowly LUC change is taking place during each time interval as compared to the average annual rate of change over the whole time series. At the category level, intensity analysis allows users to identify which categories are dormant versus active in terms of gains or losses in the size of each category. At the transition level, when a given category makes gains or losses, users can identify which other categories are most intensively targeted or avoided.

## QGIS Exercise ##


## Available tools ##
• Aldwaik and Pontius matrix (Excel sheet) https://sites.google.com/site/intensityanalysis/ • R Package Intensity.analysis • Processing R provider Plugin Intensity_analysis.rsx R script There is not any specific tool available in QGIS to make intensity analysis, although this has been implemented in an R package (intensity.analysis) 
See Chapter "About this Book" for more detailed information about how to integrate R into QGIS and how to use R scripts such as the one applied in this exercise. 

## Requisites ##
All maps must be in raster format and have the same resolution, extent and projection.

## Execution ##
If necessary, install the Processing R provider plugin and download the Intensity analysis.rsx R script into the R scripts folder (processing/rscripts). See Chapter "About this Book" of this book for further information about how to use the QGIS R script.
Step 1
The land use maps need to be stacked into a multilayer file in chronological order. The first map is the oldest map. The second map is the next oldest and so on. This can be done with the Merge tool in the Raster tab.
Step 2
Run the script and fill in the required parameters (path and name of the time-series stack, null value, the path to the folder where the results will be saved, the path and name of the output plot) as shown in Fig. 

## Results and Comments ##
The script will generate three files in the results folder: IntervalLevel.csv, CategoryLevel.csv and TransitionLevel. csv. A plot of the interval level is also produced. Plots of both the category and transition level have to be created from the Excel data sheet.
The first Excel file, called IntervalLevel.csv (Fig. 
The automatically generated plot is shown in Fig. 
The CategoryLevel.csv document (Fig. 
This figure shows the intensity of change in the different categories, regardless of their relative size within the study area. The categories with short bars to the left of the blue line representing average, uniform intensity are relatively inactive or dormant, whereas those that extend to the right are relatively active. For example, Category 1 showed the highest intensity in terms of land use gains, while Category 4 underwent more intense gains and losses than the average. At the other end of the scale, Category 3 was relatively Finally, the TransitionLevel.csv (Fig. 
Figure 

## Flow Matrix ##


## Description ##
The Flow Matrix was developed by 
The Flow Matrix is a cross-tabulation matrix that shows the proportion of the study area that transitions from one category to another, excluding persistence. It assumes linear change over time during each time interval. It allows us to calculate: (a) the annual proportion of the study area that changes during each time interval and (b) the uniform annual proportion of the study area that changes over the entire time series, and the proportion of change that would have to be reallocated to different time intervals in order for change to be perfectly stable (R). When change is perfectly stable, R is zero. This value increases as change becomes more unstable.
A vertical bar chart is produced showing the amount of annual land use change during each time interval as compared to the uniform annual change.

## Utility ##
Exercises 1. To validate a series of maps with two or more time points
The Flow Matrix provides an analysis of the temporal extent at which phenomena are stable. It can be used to find out whether land use change takes place at a uniform rate over the course of the entire study period or if more change takes place during certain intervals. It can also be used to detect errors. If one particular interval is very different from the others in terms of its annual change rate, this may be due to errors in the mapping or the methodology.
The Flow Matrix can also be used in the selection of particular calibration intervals when developing future historical trend simulations, as the data should show the greatest possible uniformity in past land use change. It can also be used to assess whether the results of a trend scenario are consistent, i.e. whether the model simulates much more or much less change than actually happened in the historical series.  The first script will generate two tables in CSV format with the stable and unstable data that would exist for the whole study period, respectively. The second script will generate two tables, in CSV format, presenting the annual change for each interval and the uniform rate, respectively. It also produces a plot showing this annual change and the uniform rate for the entire time series. 

## Requisites ##
All maps must be in raster format and have the same resolution, extent and projection.

## Execution ##
If necessary, install the Processing R provider plugin and download the Stable_change_flow_matrix.rsx and Flow_-matrix_graf.rsx R scripts into the R scripts folder (processing/ rscripts). See Chapter "About this Book" of this book for further information about how to use the QGIS R script.
Step 1
Then, run the stable and unstable change script (stable_change_flow_matrix.rsx) and fill in the required parameters: number of time points (in this case, 3), background value (in this case, 0), land use maps and number of years between the time points. Make sure you save the files in the correct folder (Fig. 
Step 2
Now, run the Annual Change Rates script (Flow_matrix_graf.rsx). Fill in the parameters as in the previous section (Fig. 

## Results and Comments ##
Step 1 generates two CSV files containing the data regarding unstable change (Fig. 
Stable change is the percentage of change that is stable in our study area between the first and the second intervals. This data is used to calculate the R value (R = 1stable change). In this case R = 1 -0.94; R = 0.06.
Step 2 produces a chart showing the annual amount of land use change (expressed as a proportion of the study area) during each time interval and the uniform rate that would exist if the annual changes were distributed uniformly across the entire time period. This is shown as a horizontal line in Fig. 
The tool also provides us with data about the annual land use change during each interval, as a percentage of the study area (Fig. 
These results show that land use change did not occur at the same uniform rate over the course of the study period and there was more change in the second interval. It should The maps validated here could be used for simulating future trend scenarios, as there is not much difference between the intervals in terms of the annual rate of land use change.

## References ##
Aldwaik SZ, Pontius RG ( 

## Abstract ##
In Land Use Cover Change (LUCC) modelling, soft maps are often produced to express the propensity of an area to land use change. These maps are generally prepared in raster format, and have values of between 0 and 1, indicating the propensity of each pixel to change. In the literature, they are referred to as suitability, change potential or change probability maps. These maps are sometimes considered as the final product of a model (e.g. map of deforestation risk), but they can also serve as intermediate products that simulate the changes from which a hard-simulated land use/cover map can later be prepared using, for example, a cellular automaton. In both cases, it is essential to evaluate the soft map's ability to identify the areas that are most susceptible to change. One way of assessing this ability is to compare the spatial coincidence between the real changes observed on the ground and the values estimated by the soft map. One would expect real change areas to coincide with high change potential values (near 1) and real no-change areas with low change potential values (near 0). This comparison can be made using various statistical approaches including Correlation Coefficient (Sect. 1), the Receiver Operating Characteristic (ROC) (Sect. 2) and the Difference in Potential (DiP) (Sect. 3). Other measures, such as total uncertainty, quantity uncertainty and allocation uncertainty (Sect. 4), are used exclusively in the analysis of soft maps. In this chapter, we describe the fundamental steps involved in these four statistical approaches to validating the soft maps produced by a model. The four sections are illustrated with specific cases: to validate soft maps produced by the model, to validate soft maps produced by the model against a reference map and to validate soft maps produced by various models against a reference map. We use the Ariège database to validate the different soft maps (change potential and suitability maps) produced by the model by comparing them with real land use maps of the Ariège Valley for two dates 

## Requisites ##
All maps must be raster and have the same resolution, extent and projection.

## Execution ##
Step 1
To create the map of real change, open the "Semi-Automatic Classification Plugin" and, in the tab "Postprocessing", select the option Land cover change. Then, fill in the required parameters: the earlier map in the reference classification (CORINE 2012) and the more recent map in the new classification 
Step 2
The raster obtained in Step 1 is reclassified twice: (i) to represent the areas in which a change was observed from 2 to 1 and those in which there was no change and (ii) to do the same for the transition from 3 to 1.
To reclassify the raster, open the Reclassify by table tool and allocate a new code 1 to ChangeCode 16 (transition from 2 to 1) and a new code 0 to ChangeCodes 17, 18, 19 In Fig. 

## Description ##
Correlation is a statistical measure that evaluates the extent to which two variables are related. This means that when one variable changes in value, the other variable also tends to change. Correlation coefficients are quantitative metrics that measure both the strength and the direction of this tendency of two variables to vary together. The correlation coefficients range from 1 to -1. A coefficient of 1 shows a perfect positive correlation, while a coefficient close to zero indicates that there is no relationship between the variables. A coefficient of minus 1 indicates a perfect negative correlation, that is, as one variable increases, the other decreases.
The Pearson correlation measures the linear correlation between two variables. Spearman's correlation is the non-parametric version of the Pearson correlation and is based on the rank order of the variables rather than on their values. Spearman's correlation is often used to evaluate non-linear relationships or relationships involving ordinal variables. We used Pearson and Spearman correlations to assess the correlation between the map showing real changes and its respective change potential map. The correlation between a binary variable and a continuous variable is known as a Point-Biserial Correlation and measures the strength of association between the two.

## Utility ##


## QGIS Exercise ##
Available tools
We have created an R script to calculate in QGIS the Pearson and Spearman correlation coefficients. This script performs a sampling of the images and calculates both the Pearson and Spearman correlations.
Exercise 1. To validate soft maps produced by the model against a reference map of changes

## Aim ##
To calculate the correlation between the map showing real change from 2 to 1 and the corresponding map of potential change.

## Materials ##
TrueChange2to1 (calculated in the preliminary QGIS exercise in this chapter) Transition potential map from agricultural to artificial areas

## Requisites ##
All maps must be in raster format and have the same resolution, extent and projection.

## Execution ##
If necessary, install the Processing R provider plugin and download the Correlation.rsx R script into the R scripts folder (processing/rscripts). For more details, see Chapter "About this Book".

## Initial maps ##
The initial maps for comparison are the TrueChange2to1 map (see the preliminary QGIS exercise in this chapter) and the map showing the change potential from 2 to 1 (Fig. 
Values range from 0 (black) to 0.977082 (white), the value for the areas with maximum change potential. The areas with a value of 0 (black) are those in which there is no potential for change.
Step 1
Run the script and fill in the required parameters (names of the two maps, proportion of pixels to be sampled, Null value) as shown in Fig. 
The script samples the images in order to speed up the computing of the correlation coefficients. It then displays both the Pearson and Spearman correlation coefficients and a scatterplot in the log files (Figs. 

## Results and Comments ##
As can be seen in Fig. 
ROC applies thresholds to the probability map to produce a sequence of binary predicted event maps and assess the coincidence between predicted and real events. A curve is obtained in which the horizontal axis represents the false positive rate (proportion of no-event cells modelled as an event) and the vertical axis the true positive rate (proportion of true event cells modelled as an event).
A standard metric based on the ROC curve is the area under the curve (AUC). If the actual events coincide perfectly with the highest ranked probabilities, then the AUC is equal to one. A random probability map produces a curve in which the true positive rate equals the false positive rate at all threshold points, and AUC is therefore 0.5. Probability maps that produce a ROC curve below the diagonal (AUC < 0.5) have less predictive accuracy than a random map 

## Utility ##
Exercises 1. To validate soft maps produced by the model against a reference map of changes
The main application of ROC analysis in spatial modelling is in the assessment of maps that predict events such as land use/cover change, species distribution, disease and disaster risks.

## QGIS Exercise ##
Available tools
QGIS does not provide any tool for ROC analysis, although R provides several packages to this end. We implemented the ROCAnalysys.rsx R script in QGIS using the QGIS Processing R provider plugin and the ROCR package to plot the ROC curve and calculate the AUC 
Exercise 1. To validate soft maps produced by the model against a reference map of changes

## Aim ##
To assess the accuracy of a change potential map using ROC analysis.

## Materials ##
TrueChange2to1 (calculated in the preliminary QGIS exercise in this chapter) Transition potential map from agricultural to artificial areas 

## Requisites ##
All maps must be in raster format and have the same resolution, extent and projection.

## Execution ##
If necessary, install the Processing R provider plugin, and download the ROCAnalysis.rsx R script into the R scripts folder (processing/rscripts). For more details, see Chapter "About this Book".
Step 1
Then run the script and fill in the required parameters (Fig. 
The maps have large numbers of both "event" and "non-event" cells, although there are normally more "event" cells than "non-event" cells. The PercentSampled parameter uses random sampling to reduce the number of non-event cells observed.

## Results and Comments ##
The script carries out a sampling of the cells, plots the ROC curve and calculates the AUC. The ROC curve (Fig. 
We assessed the change potential map for the transition from Category 2 (agriculture) to Category 1 (built-up) using ROC analysis. An AUC of 0.74 was obtained. We can therefore conclude that this predictive map was reasonably successful at identifying the agricultural areas that were most likely to be converted to built-up over the period 2012-18.

## Difference in Potential (DiP) ##


## Description ##
DiP is based on the Peirce Skill Score (PSS): In DiP, proposed by 

## Utility ##
Compared to other assessment techniques such as ROC analysis (see previous section), which is based on a relative threshold, DiP analysis is a measure of absolute threshold. As 

## QGIS Exercises ##
Available tools The Difference in Potential is a simple subtraction between average values from two maps. The required functions are related to zonal statistics which is why in these exercises we will be using the Raster layer zonal statistics tool.
Exercise 1. To validate soft maps produced by the model against a reference map of changes

## Aim ##
To validate and compare two change potential maps (soft maps), obtained from the same model, against a reference map-CORINE Land Use map of real changes (from 2012 to 2018).

## Materials ##
TrueChange2to1 (calculated in the preliminary QGIS exercise in this chapter) 

## Requisites ##
All maps must be raster and have the same resolution, extent and projection.

## Execution ##


## Initial maps ##
In this exercise, we will be using the TrueChange2to1 and the TrueChange3to1 maps (see the preliminary QGIS exercise in this chapter), the change potential map from 2 to 1 (see Sect. 1) and the change potential map from 3 to 1 (Fig. 
Step 1
We open Raster layer zonal statistics (located in the Processing Toolbox) to extract the mean values from the change potential map from 2 to 1 (Input layer) using the TrueChange2to1 map as the Zones layer (Fig. 
We then repeat the exercise with the change potential map from 3 to 1 (Input layer) using the TrueChange3to1 map as the Zones layer.

## Results and Comments ##
The mean value for change potential from 2 to 1 in the candidate areas that actually change to Category 1 is 0.43, while in the candidate areas that did not change, the mean value is 0.20. Therefore, the Difference in Potential is 0.23.
In spite of the fact that the change potential is twice as high in the areas that changed to Category 1 than in those that did not change, the absolute potential (about 0.43) is quite low.
As regards the change from 3 to 1, the mean value for change potential in the candidate areas that change to Category 1 is 0.31, while in the candidate areas that did not change, the mean value is 0.02. Therefore, the Difference in Potential is 0.29. In spite of the fact that the absolute difference is quite low, it is important to highlight that the change potential value in the candidate areas that did not change is almost zero. From this point of view DiP throws up interesting results.
The fact that these soft maps have similar DiP values means that they have similar predictive capacity. This is slightly higher in the map charting potential change from 3 to 1, although we should also bear in mind that the change from 3 to 1 affects just one small, contiguous area.

## Exercise 2. To validate soft maps produced by various models against a reference map of changes Aim ##
To validate and compare two soft maps obtained from two different models against a reference map-CORINE Land Use map of real changes (from 2012 to 2018).

## Materials ##
TrueChange2to1 (calculated in the preliminary QGIS exercise of this chapter) Transition potential map from agricultural to artificial areas Markovian probability map for artificial areas Ariège Valley

## Requisites ##
All maps must be raster and have the same resolution, extent and projection.

## Execution ##


## Initial maps ##
In this exercise, we will be using the TrueChange2to1 (see the preliminary QGIS exercise in this chapter), the change potential map from 2 to 1 (see Sect. 10.1) and the Markovian probability map for Category 1 (Fig. 

## Step 1 ##
In order to obtain the mean values from the change potential map for the transition from 2 to 1, follow the process set out in Exercise 1 of this section. Step 2
We now use the Raster layer zonal statistics tool to extract the mean values from the probability map for Category 1 (Input layer) using the TrueChange2to1 map as zones (Zones layer). In other words, in both soft maps (change potential map and probability map) we extract the mean values using the same map as zones.

## Results and Comments ##
As commented in Exercise 1 of this section, the mean value for change potential from 2 to 1 in the candidate areas that did actually change to Category 1 is 0.43; while in the candidate areas that did not change, the mean value is 0.20. This means that the Difference in Potential is 0.23. In spite of the fact that the change potential is twice as high in the areas that changed to Category 1 than in those that did not change, the absolute potential (about 0.43) is quite low.
The mean value for the probability of Category 1 in the candidate areas that did actually change from Category 2 to 1 is 0.013, while in the candidate areas that did not change, the mean value is 0.0098. The Difference in Potential is therefore 0.0032. This very small difference means that the only Markovian-generated probability map has no predictive value.
The two soft maps, each generated by a different model to predict the changes in land use and cover, produce highly varying results: some areas considered to have high change potential by one model are attributed low change potential by the other.
In this case, it is important to remember that we are comparing two quite different change potential maps. Firstly, a change potential map in which only one specific transition is evaluated (in this case from 2 to 1) and therefore only one source category (Category 2) is considered for its potential for change to the target category (Category 1). Secondly, a suitability map, which generates the probability of any part of the study area belonging to a particular target category (in this case Category 1) at the end of the period regardless of its original source category. However, when comparing the outputs of these models, we evaluated the same transition in both soft maps and validated them against the same real change.
The second main difference is that the change potential map is based not only on two LUC maps but also on selected drivers, while the Markov Probability map is computed without additional knowledge (drivers). The conclusion is that when comparing different maps, it is important to bear in mind that the data may have been obtained in different ways.
4 Total Uncertainty, Quantity Uncertainty, Allocation Uncertainty

## Description ##
In an exhaustive state of the art on the accuracy of model outputs, 
where PM = the average for the values less than 0.5 (pixel values equal to or higher than 0.5 are previously set to zero); PF = average of soft prediction map where values less than 0.5 are set to zero while values equal to or higher than 0.5 are converted into their complement to 1 (0.8 becomes 0.2; 0.51 becomes 0.49). The uncertainty indices proposed by 

## Utility ##


## QGIS Exercise ##
Available tools

## • Raster Raster Calculator Raster Layer Statistics ##
There is not any specific tool implemented in QGIS or R that allows to directly calculate the uncertainty indices proposed by 

## Exercise 1. To validate soft maps produced by the model Aim ##
To validate the soft map produced by the LCM model for the Ariège Valley case study.

## Materials ##


## Soft prediction LCM Val d'Ariège 2018 ##


## Requisites ##
The map must be raster.

## Execution ##


## Initial maps ##
Figure 
To calculate the PM map (probability of being a miss), we use the Raster Calculator twice. First, we generate an intermediate map in which all pixel values less than 0.5 are coded as 1: calculator expression = "CLC_predict_2018_-soft_UTM@1" < 0.5. Then, we multiply this mask (intermediate map named "TMP_1") by the soft prediction map: calculator expression = "TMP_1@1" * "CLC_pre-dict_2018_soft_UTM@1". As a result, we obtain the PM map (Fig. 
Step 2
To calculate the PF (probability of being a false alarm) map, we need to use the Raster Calculator again. With the calculator, we can first compute an intermediate map in which all pixel values equal to or greater than 0.5 are coded as 1: calculator expression = "CLC_predict_2018_soft_UTM@1" > = 0.5. Then, we subtract the values of the soft prediction map from 1 before multiplying it by the mask (intermediate map, here named "TMP_2"): calculator expression = (1-"CLC_predict_2018_soft_UTM@1") * "TMP_2@1". As a result, we obtain the PF map (Fig. 
Step 3
Finally, we use the Raster Layer statistics tool to calculate the average PM and PF values from the corresponding maps.
2018 PM average ¼ 0:00963 2018 PF average ¼ 0:00577
Step 4
Once we have obtained the PM and PF values, we can calculate the Quantity Uncertainty (QU), Allocation Uncertainty (AU) and Total Uncertainty (TU) following the formulas provided by 

## Spatial Metrics ##
Description Spatial metrics are a set of indices or metrics that were first developed within the field of landscape ecology 
Spatial metrics were initially developed for raster data, although some of them have also been adapted for calculation with vector data, for which the polygon is the unit of measurement. For raster data, the reference concept for calculating the metrics is the patch.
A patch is defined as a contiguous area of pixels belonging to the same category. The number and shape of the patches in a raster will depend on the neighbourhood rule applied (Fig. 
Spatial metrics can be calculated at three different levels: per patch, per category or for the whole map (landscape level). In the first case, each metric is calculated for every single patch. In the second case, the metrics are calculated for all the patches belonging to every single category on the map. In the last case, the metrics are calculated for the map as a whole. Not all metrics can be calculated for the three levels of analysis, but some of them are only available for certain levels of analysis.
There is a wide variety of metrics available, and new ones are regularly being proposed (Botequilha 
Spatial metrics are usually classified into groups according to the information they provide: area, density and edge metrics; shape metrics; connectivity metrics and diversity metrics. The first group (area, density and edge) provides information about the area and perimeter of the patches. Shape metrics assess the complexity of the shape of the patches, based on their area and perimeter, while connectivity metrics quantify the degree to which patches relate to each other (how connected they are) and are usually calculated at the category level. Finally, diversity metrics quantify the heterogeneity of the map and can only be computed at a landscape level.
For an overview of the range of metrics available and a description, please see Botequilha 

## Utility ##
Exercises 1. To validate a map against reference data / map 2. To validate a simulation against a reference map 3. To validate simulated changes against a reference map of changes 4. To validate a series of maps with two or more time points 5. To validate a series of maps with two or more time points (vector) 6. To validate a series of maps with two or more time points (raster) Spatial metrics are some of the most popular tools for analysing the pattern of categorical maps. Using the wide diversity of spatial metrics currently available, we can obtain numerous quantitative measurements of the fragmentation, shape complexity and heterogeneity of the landscape.
Spatial metrics can be calculated for the whole map or for certain specific features. In the case of Land Use Cover Change analyses, including LUCC modelling, spatial metrics can be specifically used to characterize the pattern of the elements that change.
Spatial metrics are usually highly dependent on the scale of analysis 
For maps at the same scale, spatial metrics can be used to assess to what extent their patterns differ. In other words, they assess the relative complexity of their shapes and perimeters, the degree to which they are fragmented, or how close patches belonging to the same categories are to each other.

## QGIS Exercises ##
As mentioned earlier, there are a lot of spatial metrics available, many of which are highly correlated. Despite this, there is a wide range of different metrics that characterize map patterns in different ways.
It would be impossible to present example exercises for all the available spatial metrics in the literature, as there would be enough material to fill an entire book. This is why, in the exercises proposed here, we focus on the metrics most commonly used for validating maps or analysing their uncertainties. These metrics are also suitable for many other exercises that users may typically wish to perform. However, they should be aware that other metrics are available which may be more suitable or useful in certain specific cases. Despite the widespread use of spatial metrics, QGIS offers few tools for calculating them. For vector maps, we have the Polygon shape indices tool, which characterizes the area, perimeter and shape compactness of polygons. Metrics are calculated for each polygon, i.e. at patch level.
Of the tools available for raster maps, we highlight two: the LecoS plugin 
The SAGA tool only allows the user to calculate a few metrics (relative richness, diversity, dominance, fragmentation, number of different classes, centre versus neighbours), although these are not amongst the most frequently used when comparing map patterns. These metrics can only be calculated for the entire landscape or study area and are not available at patch or class level. In addition, although the user may select the window at which the spatial metrics are calculated (3 Â 3, 5 Â 5 or 7 Â 7), the 8-cell neighbourhood rule is applied by default and cannot be changed.
The "LecoS" plugin offers a wider set of metrics and two levels of analysis: per class and for the entire map. It also provides a few extra tools with which to manipulate the maps and extract specific elements that may be of interest to users. The plugin also allows us to calculate the metrics for specific areas of the map that overlay a vector layer defined by the user. Nonetheless, these spatial metrics cannot be calculated per patch and the 8-cell neighbourhood used by default for the calculation cannot be changed. For full information about the plugin and the various possibilities it offers, readers should consult the Lecos website and the paper by 
The R package "landscapemetrics"
Although the R package offers us all the options currently available for calculating spatial metrics, in this chapter we will be focusing exclusively on the LecoS plugin. This is because it provides enough tools for the exercises we propose, and is a tested, efficient software which allows us to perform these analyses easily and quickly.

## Exercise 1. To validate a map against reference data/map ##


## Aim ##
To assess to what extent the pattern of the CORINE map is similar to the pattern of the reference SIOSE map, which charts the real situation on the ground.

## Materials ##


## SIOSE Land Use Map Asturias Central Area 2011 CORINE Land Use Map Asturias Central Area 2011 ##


## Requisites ##
The two maps must be raster. The background class must be 0 or no data.

## Execution ##
Step 1
One of the requisites of the "LecoS" plugin is that no category, apart from the background, is coded with the number 0. In our maps, the category "agricultural areas" is coded 0. The first step is therefore to reclassify the maps, so the background is coded 0 (currently it is coded 12) and all other categories have different codes other than 0.
The maps are reclassified using the Reclassify by table tool (Processing toolbox > Raster analysis > Reclassify by table ). After opening the tool, indicate the map you want to reclassify (CORINE map) and fill in the "Reclassification table" with the values that will replace the existing values in the raster (Fig. 
Bearing these criteria in mind, fill in the reclassification table and run the tool (Fig. 
Step 2
After running the tool, you will obtain a reclassified map that meets the requirements of the LecoS plugin. You are now in a position to calculate the spatial metrics. This is done by accessing the Landscape statistics option of the "LecoS" plugin via the following route: Raster > Landscape ecology > Landscape statistics.
Once there, in the "Landcover grid" box indicate the raster for which you want to calculate the spatial metrics (CORINE reclassified), the "No-data" value (0, which is the background) and the spatial resolution of the raster (50 m, which you can check in the layer properties). You must also select the particular metrics you want to obtain (Fig. 
Several spatial metrics can be selected at the same time, using the "Select multiple metrics" tab. In this case, we selected the following: Land cover; Landscape proportion; Number of patches; Greatest patch area; Smallest patch area; Mean patch area; Median patch area; Fractal dimension index; Like adjacencies; Patch cohesion index. Once you have done this, run the function.
If your computer is unable to calculate all the metrics at the same time, split the task into two (e.g. two groups of five metrics). In this case, after running the tool for the second time, the results must be gathered together in a single file, as the plugin creates one file for each time you run the tool.
Step 3
The last step is to repeat the whole workflow for the reference raster, i.e. for the SIOSE map. In this case, you will probably need to split the spatial metrics calculation into different steps as the plugin may be not able to handle all the information at once. As the SIOSE map is made up of a larger number of patches, the plugin will need more time to make all the calculations.

## Results and Comments ##
Once the spatial metrics for each of the maps have been calculated, the results of the analysis will be stored in CSV files in the folder of your choice.
You will have one file for each time you have run the tool. The first step will therefore be to gather all the information together in one file to make it easier to compare the spatial metrics for the two maps (Table 
The "Land cover" and "Landscape proportion" metrics (Table 
The "Land cover" metric indicates the surface area in square metres occupied by each category. The "Landscape proportion" gives the proportion of the entire map (out of 1) occupied by each category. If the two maps have the same extent, both metrics will provide the same information, albeit in different units (square metres and percentage). Comparing maps with different extents is not recommended and could lead to important issues in the interpretation of the analysis.
In our case, the landscape composition of the two maps is very similar. All the categories are represented in similar proportions. Nonetheless, some differences were observed in the case of mineral extraction sites (Category 5 after reclassification), dump sites (Category 6) or road and rail networks (Category 7), among others.
The "Number of patches" (Table 
Unlike landscape composition, important differences can be observed between the two maps in terms of landscape configuration. The SIOSE map is much more fragmented than the CORINE one. This difference is very significant for example in the road and rail networks category (Category 7   after reclassification). Whereas in CORINE this class is made up of just 28 patches, in SIOSE it is much more fragmented with 2,464 patches (Table 
Those land use categories that usually appear on the ground in small areas, such as small dump sites, or with linear features such as most of the road network, are not represented on the CORINE map, although they do appear in SIOSE. This explains the differences between the two maps in terms of the areas or proportions of certain classes referred to above.
It would be wrong therefore to conclude that CORINE does not map these areas of disagreement between the two maps well. They do not appear in CORINE simply because it has different MMU and MMW rules.
The "Greatest patch area" and "Smallest patch area" metrics (Table 
These two metrics highlight CORINE's simpler pattern and higher level of generalization. With a few exceptions, the largest patch in CORINE is usually larger than its counterpart in SIOSE. For the smallest patch, there are small differences between the maps. In most cases, the smallest patch occupies 2,500 m 2 in both maps. In other words, the smallest patch covers a single pixel with a 50 m edge (50 Â 50 = 2,500 m 2 ). It does not comply with the MMU and MMW rules of CORINE. This may be due to the presence of isolated pixels on the edge of the map after clipping it or due to the rasterization process.
The "Mean patch area" and "Median patch area" metrics (Table 
The "Fractal dimension index" (Table 
Contrary to what might be expected, and with the exception of the port areas (Category 9 after reclassification), patch shapes were more complex in CORINE than SIOSE. This seems illogical given that SIOSE is made at a finer scale (1:25,000) than CORINE (1:100,000) and delimits land use areas more accurately.
In our case, CORINE has more complex patch shapes than SIOSE because of the rasterization of the CORINE and SIOSE vector databases, which reduced the complexity of the SIOSE polygons, resulting in more regular shapes. Finally, "Like adjacencies" and the "Patch cohesion index" (Table 
The "Like adjacencies" metric is based on the number of adjacencies between pixels, whereas the "Patch cohesion index" is obtained by calculating the ratio between the area and the perimeter of the patches. This means that although they provide information on a similar subject (compactness), they complement each other.
These metrics show that land uses are represented in a more compact (more clustered) manner in the CORINE database. This makes sense because of the lower degree of fragmentation and the greater generalization of CORINE compared to SIOSE.
All in all, even if important differences between the two maps could be identified in terms of landscape configuration, most of these are due to the different criteria used in the drawing of each map. This also applies to the small differences in terms of landscape composition. Our CORINE map must therefore be considered validated after comparison with SIOSE.
However, in order to be able to validate CORINE with certainty and to interpret the results of the spatial metrics more effectively, we should always compare the maps via visual inspection. In this case, visual inspection reveals that the differences identified by the spatial metrics are mostly due to the different criteria used in the drawing of each map, and not because they interpret land use in different ways.
Complementary tools must therefore be used to contextualize the results of our validation or uncertainty analysis. If this is not done, there is a high chance that we will make incorrect assumptions due to not having all the relevant information.
Exercise 2. To validate a simulation against a reference map

## Aim ##
To assess to what extent the pattern of our simulation is similar to the pattern of a reference map for the same year, which accurately reflects the real situation on the ground.

## Materials ##
Simulation CORINE Asturias Central Area 2011 CORINE Land Use Map Asturias Central Area 2011

## Requisites ##
The two maps must be raster. The background class must be 0 or no data. For a proper validation, the reference map and the simulation must refer to the same year.

## Execution ##
Step 1
In order to comply with the requirements of the "LecoS" plugin, which assumes that pixels with the value 0 are No Data or background, we must first reclassify the two maps we are going to compare. The background, which is coded as 12, must be reclassified as 0. Agricultural areas, which were coded as 0, must be reclassified as 1. All the remaining classes must be reclassified following the same criteria (new code = original code + 1). The Reclassify by table (Processing toolbox > Raster analysis > Reclassify by table) tool will be used to reclassify the maps (Fig. 
Step 2
Once the two maps have been reclassified, the next stage is to calculate the spatial metrics for each map: first for the simulation and then for the reference map. This is done using the Landscape statistics option in the "LecoS" plugin (Raster > Landscape ecology > Landscape statistics) (Fig. 
In "Landcover grid" select the raster for which you want to obtain the spatial metrics. You must also indicate the value of the background (No-data) and its spatial resolution (Cellsize). Finally, select the spatial metrics you are going to calculate.
Several spatial metrics can be selected at the same time using the "Select multiple metrics" tab. In this case, we selected the following metrics: Land cover; Landscape 

## Results and Comments ##
Once the spatial metrics for each of the maps have been calculated, the results of the analysis will be stored in CSV files in the folder of your choice. To make it easier to interpret and compare the spatial metrics, the two files must be merged into one. This can be done using a spreadsheet program such as OpenOffice Calc or Microsoft Excel. This will display the results in a table similar to Table 
Even so, some differences can be observed. Agricultural areas (Category 1 after reclassification) and vegetation areas (Category 2) are made up of a larger number of patches in the simulation than in the reference map (Table 
These trends may indicate that the changes simulated as transitions to urban fabric and to industrial and commercial areas have made these classes more compact (patches that were not previously connected have now become connected with the simulated changes). That is, these classes did not grow in an isolated way, but via the expansion of previously existing patches. The slight differences between the reference map and the simulation in the "Like adjacencies" and "Patch cohesion index" metrics for industrial and commercial areas (Category 4 after reclassification) also point in this direction.
In the process of expansion of urban fabric and industrial areas, some patches of agricultural and vegetation areas could become isolated, so increasing the fragmentation of the category. This would explain why there are more patches in these categories in the simulation than in the reference map.
The difference in pattern between the simulation and the reference map can best be calculated using spreadsheet software, as described in the example for Table 
In our simulation, we did not actively model the vacant classes. Thus, whereas according to the reference map there were many vegetation areas that changed to agricultural areas, in our simulation this did not happen. As a consequence, our simulation has more vegetation areas, but less agricultural areas than the reference map.
The "Greatest patch area" metric shows that we did not model one of the biggest industrial developments in the study area correctly. The largest patch in our simulation is 450,000 m 2 smaller than the one in the reference map. The opposite was true in the case of urban fabric. According to the model, many pixels were considered to have changed as a result of the expansion of large pre-existing patches, when this trend was in fact not that strong according to the reference map.
If we focus on the "Mean patch area" metric for the two categories we modelled actively (3 and 4) we can see how in both cases the mean area of patches is always bigger in the simulation than in the reference map. This may be due to the same process as in urban fabric, i.e. most of the changes are simulated as expansions of pre-existing large patches.
In all other categories apart from the first 4 (1, 2, 3, 4), there are important differences between the two maps. However, as changes in these categories were not modelled in the simulation (they remained invariant), the differences between the maps are due to changes that took place in the reference map but were not simulated.
To sum up, it is difficult with the information available to us to understand whether the pattern of the changes we simulated is valid or not. We have various clues about the pattern of the changes (more compact and connected than in the reference map), but these trends are best confirmed by visual inspection. Calculating the spatial metrics solely for the areas that changed is also highly recommended and can provide additional insight.
Exercise 3. To validate simulated changes against a reference map of changes

## Aim ##
To assess to what extent the pattern of the changes we simulated is similar to the pattern of a reference map of changes for the same year, which accurately reflects the real situation on the ground.

## Materials ##


## CORINE Land Use Changes Asturias Central Area 2005-2011 Simulated CORINE changes Asturias Central Area 2005-2011 ##


## Requisites ##
The two maps must be raster. The background class must be 0 or no data. For a proper validation, the changes in the reference map must refer to the same time period as the simulation period.

## Execution ##
Step 1
Given that the background is already coded 0 in the two maps charting changes, we do not need to take any preliminary steps prior to calculating the spatial metrics. This can be done directly using the Landscape statistics option in the "LecoS" plugin (Raster > Landscape ecology > Landscape statistics).
In the tool, we must indicate the raster for which we want to calculate the spatial metrics (Landcover grid), the value of the background in our maps (No-Data) and their spatial resolution (Fig. 
In this analysis, we will be calculating the following metrics: Land cover; Number of patches; Greatest patch area; Smallest patch area; Mean patch area; Median patch area; Fractal dimension index; Like adjacencies; Patch cohesion index.
Step 2
We repeat this process for the second map.

## Results and Comments ##
Once we have run the tool twice, once for each map, we will have two CSV files with the metrics for each of the change maps. These will be saved in the specified folder.
The reference map of changes includes land use changes for many categories 
The changes we simulated are quantitatively the same as the reference changes (Table 
On the other hand, the pattern of the simulated changes seems to be very different from the pattern of the reference map of changes. In the reference map, the changes took place in just a few patches and most of the pixels that changed are allocated close to each other. In the simulation, the changes are fragmented in many different patches (Table 
When working with Cellular Automata models, change usually takes place organically as an expansion of existing patches. In the real world, however, changes in urban and industrial areas tend to happen at the same time over entire cadastral parcels. Often, these parcels are quite big, comprising a large number of pixels. However, as CA models usually simulate change at the pixel level, they are not normally capable of simulating big patches of change covering large numbers of pixels. Our model therefore behaves differently from the real processes taking place on the ground, hence the disagreements in the pattern of simulated changes.
Other metrics, such as "Like adjacencies" and "Patch cohesion index" confirm this behaviour. The pixels in the reference map are better grouped than those in the simulated map (Table 
In conclusion, the pattern of changes we simulated is very different to the pattern of changes in the reference map. However, this does not mean that the changes we simulated have altered the pattern of the simulated landscape. On the contrary, as we discovered in the previous exercise, the pattern of the whole landscape remains very similar.
It is important to remember here that we are only calculating the pattern of the areas that changed, without viewing them in any larger context. By contrast, when we calculate the spatial metrics for the whole map, we also consider the context and can therefore assess whether the changes have altered the pattern of the map. Thus, both analyses are complementary. We recommend users to carry out both analyses when validating the pattern of their simulations.
Finally, a qualitative validation through visual inspection is highly recommended for contextualizing the results and understanding them better. 

## Requisites ##
The two maps must be raster. The background class must be 0 or no data.

## Execution ##
Step 1
In order to comply with the requirements of the "LecoS" plugin, the maps must be reclassified to ensure that the background code is 0 and all other categories have a positive code different from 0. This is done using the Reclassify by table tool (Processing toolbox > Raster analysis > Reclassify by table) (Fig. 
After opening the tool, we indicate the map we want to reclassify and then fill in the "Reclassification table" with the new category codes that will replace the existing ones in the raster (Fig. 
Step 2
Once the categories have been reclassified, the spatial metrics for each map can be calculated using the Landscape statistics option in the "LecoS" plugin (Raster > Landscape ecology > Landscape statistics) (Fig. 
After opening the tool, we select the raster for which we wish to obtain the metrics (Landcover grid), the background value of the raster (No-Data) and its spatial resolution (cellsize). We then select the different metrics we want to calculate in the "Select multiple metrics" tab. In this case we selected the following: Land cover; Landscape proportion; Number of patches; Greatest patch area; Smallest patch area; Mean patch area; Median patch area; Fractal dimension index; Like adjacencies; Patch cohesion index.

## Results and Comments ##
After running the tool, the metrics are displayed in two CSV files which are saved in the specified folder.  The metrics reveal important differences between the two maps in terms of landscape configuration, i.e. the way land uses are allocated on each map.
The categories in the CORINE 2011 map are made up of many more patches than the same categories in the CORINE 2005 map (Table 
The "Like adjacencies" and "Patch cohesion index" metrics also show slight differences between the maps. This is unusual when comparing a time series of land use maps, as these metrics are not usually sensitive to small changes in the landscape. With the exception of highly dynamic environments, in most of the study areas we might wish to assess, change affects less than 5% of the landscape. We should not therefore expect meaningful differences in the spatial metrics that characterize the landscape over a short period such as that used in our example 
The "Land cover" metrics show big differences between the maps in terms of the areas covered by each category (Table 
The "Greatest patch area" and "Mean patch area" metrics also differ greatly for the two maps in the time series (Table 
These conclusions were confirmed by a visual inspection of the two maps, an additional check that is highly recommended to complement the results of this analysis.
Exercise 5. To validate a series of maps with two or more time points (vector)

## Aim ##
To study the pattern of a specific transition (from scrubland to forest) in our study area (Ariège Valley) for a given period 

## Materials ##


## CORINE Land Cover Map Val d'Ariège 2000 CORINE Land Cover Map Val d'Ariège 2018 ##


## Requisites ##
All raster maps must have the same resolution, extent and projection.

## Execution ##
Step 1
We begin by extracting the changes we want to study (transition from scrub to forest) with the Raster Calculator (Fig. 
This produces a raster showing the areas that underwent this transition (Fig. 
Step 2
Once the raster for this transition has been obtained, it must be converted into vector format (polygons) using the Polygonize GDAL tool. When making this conversion, the "Use 8-connectedness" option must be selected (Fig. 
Step 3
Once the polygons that undergo this transition have been obtained in vector format, we can then calculate their spatial metrics using the SAGA Polygon Shape Indices tool (Fig. 
Step 4
In order to better interpret the general pattern of all the polygons that undergo this transition, the results of the metrics can be exported to a spreadsheet where statistics such as the mean, standard deviation, minimum and maximum can be calculated (Table 

## Results and Comments ##
The pattern of the areas that transition from scrubland 
The perimeter / area (P/A) ratio is a measure of the compactness of the patches. Lower P/A values mean more compact polygons, whereas higher P/A values mean elongated or less compact polygons. The maximum distance metric indicates the longest segment of a polygon. The maximum distance / area (D/A) ratio is a measure of how Lower values indicate more compact, less elongated polygons, whereas higher values mean the opposite. Finally, the shape index measures the shape complexity of a patch, using the following formula: Perimeter/(2 * Square Root(PI * Area).
The metrics calculated in this exercise can be compared with the metrics obtained and analysed in Exercise 6 below, which carries out the same analysis with raster data. The comparison will offer an insight into how data format (vector or raster) can affect the results of a pattern analysis. Exercise 6. To validate a series of maps with two or more time points (raster)

## Aim ##
To study the pattern of a specific transition (scrub into forest) in our study area (Ariège Valley) for a given period 

## Requisites ##
All maps must be rasters and have the same resolution, extent and projection.

## Execution ##
Step 1
We begin by extracting the specific changes we want to study from our series of maps, i.e. the pixels that transitioned from scrub (Category 4) to forest (Category 3). We do this by introducing the following expression in the Raster Calculator: "CLC_2000@1" = 4 AND "CLC_2018@1" = 3 (Fig. 
Step 2
Once the raster with the areas that changed from scrub to forest has been obtained, we then calculate their spatial metrics using the Landscape statistics option from the "LecoS" plugin (Raster > Landscape ecology > Landscape statistics) (Fig. 

## Results and Comments ##
Once the spatial metrics have been calculated, the plugin creates a CSV file in the output folder with the results.  The results show that 37 different patches underwent the transition from scrub to forest, as shown in the "Number of patches" metric in Table 
The "Landscape proportion" metric indicates the percentage of the studied landscape occupied by the category in question. As we are only considering one category in our analysis (the areas that transition from scrubland to forests), this category occupies 100% of the studied landscape and therefore has a landscape proportion value of 1 (Table 
The landscape division, patch cohesion and splitting indices assess the compactness or fragmentation of the patches that make up a class, i.e. how well aggregated they are. A "Landscape division" value close to 1 means a very fragmented landscape, whereas values close to 0 indicate a landscape made up of a single patch. A "Patch cohesion" value of 0 means one isolated patch, whereas values closer to 100 mean more aggregated patches. A "Splitting index" value of 1 indicates a landscape made up of a single patch, while splitting index values of more than 1 indicate a progressively more fragmented landscape.
If we compare these results to those obtained in vector format (Exercise 5), we can see that the same values were obtained for comparable measures (e.g. mean area, greatest / smallest area), while other measures use different formulas. These include the shape and compacity indices (standardized or not, area-weighted or not, completed by a constant or not). The LecoS plugin also offers complementary indices which are not calculated in vector format, such as the fractal dimension or the splitting index. In addition, whereas the spatial metrics in vector can be calculated individually for each patch or polygon (Exercise 5), this is not possible in raster format when using the LecoS plugin. The plugin usually calculates the mean values of all the patches for each metric.
Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http:// creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## 228 ##
D. García-Álvarez and M. Paegelow

## Advanced Pattern Analysis to Validate Land Use Cover Maps ##
Martin Paegelow and David García-Álvarez

## Abstract ##
In this chapter we explore pattern analysis for categorical LUC maps as a means of validating land use cover maps, land change and land change simulations. In addition to those described in Chap. "Spatial Metrics to Validate Land Use Cover Maps", we present three complementary methods and techniques: a Goodness of Fit metric to measure the agreement between two maps in terms of pattern (Map Curves), the focus on changes on pattern borders as a method for validating on-border processes and a technique quantifying the magnitude of distance error. Map Curves (Sect. 1) offers a universal pattern-based index, called Goodness of Fit (GOF), which measures the spatial concordance between categorical rasters or vector layers.
Complementary to this pattern validation metric, the following Sect. 2 focuses specifically on the changes that take place on pattern borders. This enables changes to be divided into those that take place on the borders of existing features and those that form new, disconnected features. Bringing this chapter on landscape patterns to a close, Sect. 3 presents a technique for quantifying allocation errors in simulation maps and more precisely on the minimum distance between the allocation errors in simulation maps and the nearest patch belonging to the same category on the reference map. The comparison between a raster-based and a vector-based approach brings us back to the differences in measurement inherent in the representation of entities in raster and vector mode. These techniques are applied to two datasets. Section 1 uses the Asturias Central Area database, where CORINE maps are compared to SIOSE maps and simulation outputs. For their part, the techniques described in Sects. 2 and 3 are applied to the Ariège Valley database. CORINE maps for 2000 and 2018 are used as reference maps in comparisons with simulated land covers.

## Keywords ##
Allocation distance error Á Change on pattern borders Á Map Curves Á Pattern shape and size indices

## Map Curves ##
Description This is a quantitative method proposed by 
GOF values range from 0 to 1. Maximum GOF ( 
When comparing pairs of maps, the GOF value may vary depending on whether the assessed map is evaluated against the reference map or the reference map is evaluated against the assessed map. Map Curves calculates the GOF values for both these operations. It then uses the highest of these two GOF values in the comparison.
GOF values may be obtained either for the whole dataset or for the set of patches or polygons that make up each category on the map. Although it is technically possible to calculate a GOF for each individual polygon or patch, it is computationally very demanding and is not normally done.
Based on the GOF metrics at the category level, the results of the map comparison may be expressed in a graph, which shows the percentage of the categories in the map that have a specific GOF value. For example, if there are 10 categories and 2 of these have a GOF value of ! 0.8, the graph will show that 20% of the categories have GOF values of ! 0.8.

## Utility ##
Exercises 1. To validate a map against reference data/map 2. To validate a simulation against a reference map 3. To validate simulated changes against a reference map of changes 4. To validate a series of maps with two or more time points Map Curves provides a simple metric for assessing the extent to which two datasets share the same spatial structure, i.e. the same number and shape of polygons or patches. Unlike many other metrics, GOF evaluates the spatial agreement between maps at a polygon or patch level. In most cases, this type of analysis is based on raster data and comparisons are made at cell level. However, polygons or patches reflect the real structure of a landscape better than cells. GOF therefore provides a better, more realistic method for validating the similarity between maps than cell-based metrics.
GOF provides a standard and, therefore, comparable metric. The GOF value in one validation exercise may be compared with the GOF value obtained in another. Consequently, when using this metric to assess validity, we can establish a general minimum acceptable GOF threshold above which the map can be considered valid.
Map Curves gives an overview of the pattern agreement for the whole landscape and at category level. However, it does not provide information about the agreement per polygon. This means that a few polygons that do not show good overlap when comparing the maps could be hidden in the general analysis. Thus, as currently implemented, this technique only provides information on spatial agreement at a category level and does not shed light on disagreements occurring at more detailed scales of analysis.
The fact that GOF is unaffected by the spatial resolution used in the analysis should be considered an important strength, as spatial resolution is one of the main sources of uncertainty associated with any validation exercise. Nonetheless, at very coarse spatial resolutions, the area and shape of some polygons and patches can become very distorted, and this could affect the results of the analysis. Therefore, when used with rasters, GOF can be considered independent of spatial resolution below a certain threshold.
We do not recommend validating the spatial structure of a map by comparing it with another map obtained at a different resolution. Changes in spatial resolution or scale will always result in changes in the spatial structure of the maps. The results of the analysis will highlight not only the differences between the original maps in the way they represent LUC in the landscape, but also the differences produced by changes in the spatial resolution.
Although Map Curves could be a useful tool for comparing the agreement of the spatial pattern between different maps, its results must be treated with caution when validating the pattern of the maps. This is because Map Curves only assesses the degree of overlap between the patches or polygons belonging to each category in the two maps compared. If the overlap is low, the GOF score obtained by Map Curves analysis will also be low. However, this only means that their classes do not overlap well and does not imply that the two maps being compared have completely different patterns.
Spatial metrics (see Chap. "Spatial Metrics to Validate Land Use Cover Maps") are more suitable for validating the pattern of the map. Even if there is no spatial overlap, they provide objective information about the fragmentation of the landscape or the complexity of the polygons/patches, which can be used when comparing two maps. Spatial metrics therefore allow us to compare pattern agreement between maps, even if they do not locate land uses in the same positions.

## QGIS Exercises ##
Available tools
There is no default tool in QGIS for carrying out Map Curves analysis. It is however implemented in R. We have developed two R tools for QGIS to perform the Map Curves analysis for either raster or vector data. To learn how to configure QGIS to work with R scripts, see Chap. "About This Book" of this book. This also explains how to install the different R scripts required to do some of the exercises presented in the book. The Map Curves raster script is based on the code developed by Professor Emiel van Loon from the University of Amsterdam. 
The Map Curves vector script, which can only be employed to compare vector maps, is based on the "Sabre" R package. 
The Map Curves raster script provides more information than the Map Curves vector script. It is also much faster and more efficient. We therefore recommend that this analysis be carried out with raster data.
Exercise 1. To validate a map against reference data/map

## Aim ##
To check the agreement between the SIOSE and CORINE maps, considering SIOSE as a valid reference. We will assess to what extent the spatial structure of the CORINE map (number of polygons, shape) is similar to the SIOSE map.

## Materials ##


## SIOSE Land Use Map Asturias Central Area 2011 CORINE Land Use Map Asturias Central Area 2011 ##


## Requisites ##
The two maps must be raster and have the same projection. Although the tool does work with raster maps at different extents and with different thematic resolutions, we recommend comparing rasters with the same or very similar extents and thematic resolutions, so as to avoid results that may not be particularly meaningful.

## Execution ##
If necessary, install the Processing R provider plugin, and download the MapCurves_raster.rsx R script into the R scripts folder (processing/rscripts). For more details, see Chap. "About This Book" of this book.
Step 1
Open the Map Curves Raster function and fill in the required parameters. These are basically the two LUC maps to be compared: "Land Use map 1" (SIOSE) and "Land Use map 2" (CORINE) (Fig. 

## Results and Comments ##
After running the function, we obtain two tables and one graph. All the information, with the exception of the graph, will also be displayed in the "Log" window (Fig. 
The GOF value is a measure of the general agreement between the two maps being compared. This value ranges from 0 to 1, with 0 meaning no agreement and 1 total agreement. The GOF value for our comparison (0.54) indicates that the agreement between the two maps is significant, although not very high. The patches of the same categories partially overlap.
The reference map ($Refmap) value informs us as to which map was used as the reference when obtaining the GOF value. If value "A" is obtained, it means that "Land use map 1" was used as the reference map in the comparison. If value "B" appears, it means that "Land use map 2" was used. Therefore, in our case, a GOF of 0.54 was obtained when comparing SIOSE and CORINE and taking CORINE as the reference. If SIOSE had been taken as the reference, agreement (GOF value) would have been lower.
The GOF table details the GOF value for agreement per category, so providing a measure of how similar the pattern for a particular category is in the two maps. It therefore answers the following question: to what extent do the patches that make up a particular category overlap in the two maps being compared?
In our case, the category that shows the greatest pattern agreement between the two maps is water bodies (Category 11), with a GOF value of 0.968. Agricultural areas (Category 0; GOF 0.783) and vegetation areas (Category 1; GOF 0.800) also show high levels of agreement. By contrast, agreement between the two maps is very low for road and rail networks (Category 6; GOF 0.112).
If we observe the two maps, most of the agreement and disagreement is due to the fact that they follow different Minimum Mapping Unit (MMU) and Minimum Mapping Width (MMW) criteria. Thus, if a patch is larger than the MMU and MMW of both maps, it will be similarly mapped in both cases. However, if a patch is drawn in SIOSE, but is too small for the MMU and MMW of CORINE, this will lead to disagreement between the two maps.
This explains the results for Category 6 (road and rail networks). Whereas many patches representing road and rail networks are mapped in SIOSE, most of them are not mapped in CORINE because they are less than 100 m wide and therefore do not comply with its MMW criterion (Fig. 
In this exercise, the GOF values for the different categories did not indicate a high degree of similarity between the category patterns on the two maps. On the contrary, they indicated different patterns of fragmentation for each category because of the different MMU and MMW rules applied in each map.
In addition to the overall GOF and the GOF table detailing the GOF agreement per category, the Map Curves function also produces two extra tables: the $BMC_A2B and the $BMC_B2A (Fig. 
Unlike the other two tables, these tables are only displayed in the "Log" window and are not stored in any folder. For each category, they indicate the category with which it shows most agreement (GOF) on the other map. Whereas, the information in the first table ($BMC_A2B) was obtained using map A (Land use map 1) as the reference, the reach or exceed a specific GOF threshold. Thus, all the categories (100%) always have a GOF score higher than 0. However, only around 40% of the categories in this map have a GOF score of over 0.5 and none of the categories show perfect agreement (0% of the categories have a GOF score of 1) (Fig. 
The graph provides the GOF scores using either Land use map 1 (A) or Land use map 2 (B) as a reference. It is therefore a good summary of the pattern agreement between the two maps.
In summary, in this exercise we have noted that although the GOF value is not very high, CORINE has a very similar pattern to SIOSE. The lower GOF is the result of different pattern fragmentation in the two maps: SIOSE maps have many small patches that do not appear in CORINE. However, if we look at the maps, the polygons from the same category usually overlap very well and have a similar pattern structure. In addition, thematic agreement, as we noted in the $BMC_A2B and $BMC_B2A tables, seems to be very high. 

## Aim ##
To assess the similarity between the spatial structure of a simulation and the spatial structure of a map used as a reference.

## Materials ##
Simulation CORINE Asturias Central Area 2011 CORINE Land Use Map Asturias Central Area 2011

## Requisites ##
The two maps must be raster and have the same projection. Although the tool works with raster maps at different extents and with different thematic resolutions, we recommend that raster maps with the same or very similar extents and thematic resolutions be compared so as to avoid results that may be not fully informative. For a proper validation, the reference map must be for the same year as the simulation.

## Execution ##
If necessary, install the Processing R provider plugin and download the MapCurves_raster.rsx R script into the R scripts folder (processing/rscripts). For more details, see Chap. "About This Book".
Step 1
Open the Map Curves Raster function and fill in the required parameters: "Land Use map 1" (CORINE simulation) and "Land Use map 2" (CORINE reference map) (Fig. 

## Results and Comments ##
After running the tool, a GOF value was obtained for the whole maps compared and broken down per pair of classes (GOF table ). The GOF values are stored in different tables and displayed in the "Log" window ($GOF, $GOFtable). The GOF values per pair of classes are also represented in the Map Curves graph, which is stored in the specified folder (R Plots).
The GOF value for our comparison is very high (0.92). This is logical given that most of the simulated landscape did not change over the simulation period and, therefore, remained the same. Permanence is one of the easiest processes to simulate in LUC modelling. This means that the reference and the simulated maps look very similar. The patterns of the two maps are very similar because most of the pattern remains unchanged over the simulation period and was correctly simulated as such.
The agreement (GOF) per category was always very high. The minimum scores were for port areas (0.669) and mineral extraction sites (0.708). In the modelling exercise, these categories were treated as features (categories that remained invariant during the simulation) and were therefore not simulated. However, a few changes did in fact occur in these categories in the reference map. As a result, the Map Curves analysis produced a relatively poor fit for these categories when comparing the simulation with the reference map. Whereas no change occurred in these categories in the simulation, a few changes did take place in the reference map. Given that these categories consist of a very small number of patches, even a small number of changes can reduce the GOF values substantially.
All in all, this analysis is not particularly meaningful. It confirms that the two compared maps have very similar patterns because most of the landscape was correctly simulated as permanence. However, more meaningful results could be obtained by focusing exclusively on the areas that were simulated as change. Hence, for a proper validation of the simulation, the simulated changes must be compared with the changes observed on the reference maps.
Exercise 3. To validate simulated changes against a reference map of changes

## Aim ##
To evaluate how similar the changes we simulated in our modelling exercise are to those observed on the reference map.

## Materials ##


## CORINE Land Use Changes Asturias Central Area 2005-2011 Simulated CORINE changes Asturias Central Area 2005-2011 ##


## Requisites ##
The two maps must be raster and have the same projection. Although the tool does work with raster maps at different extents and with different thematic resolutions, we recommend comparing rasters with the same or very similar extents and thematic resolutions, so as to avoid results that may not be very meaningful. For a proper validation, the simulation and the reference map must refer to the same time period. In both cases, the maps must only display the changes that occurred during the study period, showing all other areas as 0 or some other suitable code.

## Execution ##
If necessary, install the Processing R provider plugin and download the MapCurves_raster.rsx R script into the R scripts folder (processing/rscripts). For more details, see Chap. "About This Book".
Step 1
Open the Map Curves Raster function and fill in the required parameters: "Land Use map 1" (Simulated CORINE changes) and "Land Use map 2" (CORINE changes) (Fig. 

## Results and Comments ##
After running the function, we get the overall GOF ($GOF) value, the GOF value per category ($GOFtable) and the Map Curves graph (R Plots). In this case, the only results that might be useful for interpreting the validity of the simulated changes are the results per category.
The general GOF value is 0.3, but this is artificially high due to the almost perfect overlap of class 0 (areas with no change) which has a GOF value of 0.993 (Table 
The spatial overlap between these two categories in the two maps is very low. The GOF value for urban fabric (Category 3 in the maps) is only 0.05. In the case of industrial and commercial maps (Category 4) it is even lower: 0.039.
This means that the spatial structure of the simulated changes is very different to that of the changes used as a reference for the same period. Thus, even though the Map Curves analysis for the whole simulation (persistence and changes) obtained good results, the simulated changes overlap poorly with the changes mapped in the reference data.
We cannot draw final conclusions about the different patterns of simulated and reference changes. Even if there is no overlap between them, their shape or fragmentation could be similar. For a clearer picture of these aspects, other tools, such as spatial metrics, must be used (see Chap. 

## Requisites ##
The two maps must be raster and have the same projection. It is also recommended that they have similar extents and thematic resolutions.

## Execution ##
If necessary, install the Processing R provider plugin and download the MapCurves_raster.rsx R script into the R scripts folder (processing/rscripts). For more details, see Chap. "About This Book".
Step 1
Open the Map Curves Raster function and fill in the required parameters: "Land Use map 1" (CORINE 2005) and "Land Use map 2" (CORINE 2011) (Fig. 

## Results and Comments ##
The results show the level of overall agreement between the pair of maps compared ($GOF), the agreement per category ($GOFtable), the best matches between categories ($BMC_A2B, $BMC_B2A) and the Map Curves graph (R plots). All results are displayed in the "Log" window and stored in the preselected folders. The overall agreement between our maps is 0.5, which is not high. This means that there is only partial overlap between the categories in the two maps. In a series of two or more Land Use maps, persistence is the norm and one would expect almost perfect overlap between the maps for most of the landscape. Landscapes must be very dynamic to experience changes affecting more than 10% of the study area.
The Asturias Central Area is not a dynamic landscape of this kind. The low GOF score therefore suggests that a lot of the differences between the two maps are due to technical changes or errors.
When agreement was assessed at the category level, the only very high values were for water bodies (Category 11), with a GOF of 0.961 (Fig. 
The agricultural areas (0.709), vegetation areas (0.704) and airports (0.778) show a high level of agreement between the two maps. However, there are still important differences between them that cannot be explained solely by the normal land use dynamism of the study area, in which only small changes usually take place. For all the other categories, agreement is low or very low. Nonetheless, there is no evidence of systematic confusion between one category on the first map and a different category on the second. This is confirmed by the tables showing the best matches between categories (Fig. 
The low agreement or overlap between the categories in the two maps is also summarized in the Map Curves graph (Fig. 
All in all, we can conclude that the time series we assessed has many errors and uncertainties and is therefore affected by many erroneous or spurious changes. These are changes that did not really happen on the ground and arose due to technical reasons, such as different production methods. In a coherent time series of LUC maps, high GOF scores of 0.9 or over would be expected.
The low agreement in our exercise is due to the change in the methodology used to produce the Spanish CORINE Land Cover maps between 2006 and 2011. The CORINE 2005 map (v.00) used in this exercise was obtained using photointerpretation of satellite imagery. However, from 2011 onwards the CORINE maps were obtained by generalizing more detailed Land Use maps (SIOSE). This change in the production method resulted in LUC maps with important differences from their predecessors. In order to solve this problem, the Copernicus service produced another CORINE map for 2005 in Spain according to the new methodology, which was consistent and comparable with the CORINE 2011 map. This more recent version of the CORINE 2005 map is the one normally used in the different exercises of this book.

## Change on Pattern Borders ##


## Description ##
In pairs of maps or time series, this technique is used to identify the changes taking place on the edges of patches. The allocation of changes (on the edge of an existing patch or a new disconnected one) provides useful information about the nature of change dynamics: the expanding or shrinking of existing boundaries or the appearance of new land use patches.

## Utility ##
Exercises 1. To validate a series of maps with two or more time points By detecting the changes taking place on the edges of the patches, we can assess both the type of landscape dynamics taking place and the data errors resulting from different data sources, classifiers or spectral responses.  For the sake of simplicity, we will only be presenting the tools used in this exercise, although we are aware that there are many other tools that could be used to carry out this analysis.
Exercise 1. To validate a series of maps with two or more time points

## Aim ##
To focus on gains taking place on the edges of patches for a specific land use/cover category. We can then assess the proportion of change taking place on the edges of existing patches compared to the change that appears in new, disconnected areas. 

## Materials ##


## Requisites ##
All maps must be rasters and have the same resolution, extent and projection.

## Execution ##
Step 1
First, we extract forests in 2000 (Fig. 
We then vectorize the binary raster maps computed in Step 1 using the Polygonize Raster Conversion function with no specific parameters.
Step 3
We now isolate the forest gains on the edge of the pattern. The aim is to distinguish between new areas of forest in 2018 (i.e. that did not exist in 2000) which are contiguous with forests that existed in 2000 and others that are not. For this purpose, we use the Extract by location Vector Selection tool with the 'touch' operator (Fig. 
Figure 
Step 4
In this step we will isolate the new forests that are not connected to forests that existed in 2000. This step is optional insofar as new forest patches not connected to forests that existed in 2000 can be obtained simply by subtracting new connected forests from the total area for new forests.
To get an independent layer of new forest in 2018 that is not connected to forests that existed in 2000, we use the same Extract by location tool, opting this time for the 'disjoint' operator 
The next step is to calculate the area covered by new connected/unconnected forests. We use the Vector table Field Calculator tool to create a new attribute called area_ha (decimal number), selecting the $area operator, divided by 10,000 to calculate the area in ha (Fig. 
Step 6
Of the various tools available to summarize the characteristics of the assessed patches, we use the Basic statistics for fields vector analysis tool. On the left of Fig. 
As can be seen in Table 

## Utility ##
Exercises 1. To validate a simulation against a reference map (vector) 2. To validate a simulation against a reference map (raster) Simulation accuracy can be measured in different ways, such as quantity agreement, allocation agreement, landscape structure agreement, etc. 

## QGIS Exercises ##
Available tools GRASS and SAGA toolboxes offer several algorithms for measuring the distance inside a raster grid (r.grow.distance; SAGA distance) or the minimum distance between pixels/patches belonging to two different grid layers (r.distance). Their use inside QGIS may be unstable.
Vector analysis tools require converting raster layers into vector format and then calculate the centroids of the polygons obtained. The Distance to nearest centre (points) tool creates a points layer whose table contains minimum distances between the points in one layer to the nearest point in the second layer.
Both tools (raster and vector) are used in the next two exercises because they provide complementary results.
Exercise 1. To validate a simulation against a reference map (vector) 

## Aim ##
To calculate the seriousness (degree) of allocation errors for a specific LUC category, expressed as the minimum mean distance between all the pixels wrongly allocated to this category in the simulation and the nearest patch belonging to the same category on the reference map. 

## Requisites ##
Maps can be raster or vector. They must have the same resolution, extent and projection. If using vector maps, readers can skip the first steps detailed in the execution.

## Execution ##
Step 1
We extract real built-up areas in 2018 (Fig. 
The right map (A) in Fig. 
Step 2
The two raster layers obtained in Step 1 are now polygonized into vector layers. This is done using the Polygonize function in the Raster-Conversion menu (Fig. 
The above map (Fig. 
Step 3
We then calculate the centroids for each of these vector layers with the Centroids tool (Vector-Geometric tools) (Fig. 
Step 4
Once we have obtained the two centroids maps (built-up areas in 2018 and built-up allocation errors), we use the Distance to nearest hub (points) tool available in the Processing Toolbox (QGIS Vector). The source points layer is the point layer containing allocation errors and the destination hubs layer is the layer containing the built-up centroids from the reference map (Fig. 
To obtain the desired statistics about the allocation error distance for wrongly simulated built-up areas, we use the Basic statistics for fields tool (Processing Toolbox, Vectoranalysis) by selecting the field containing the calculated distance to the nearest hub (Fig. 

## Results and Comments ##
The resulting points layer contains the same number of points as the allocation error polygons at the same location.
The corresponding table contains the minimum distance between each allocation error (centroid) and the nearest existing built-up area (centroid) on the reference map (Fig. 
A summary of the statistics appears in the log of the Basic statistics for fields function (Fig. 
As we can see, the mean distance for 132 allocation errors is about 1,236 m. This is quite close to the median value 
The mean allocation error distance of about 1.2 km should be put into context by comparing it with the spatial extent of the layer, which is about 31 Â 62 km. It may also be useful to compare this value with the mean allocation error distances for other LUC categories and the mean value for all the allocation errors. 

## Requisites ##
All maps must be rasters and have the same resolution, extent and projection.

## Execution ##
Step 1
First, we compute a raster distance map up from built-up areas using the QGIS raster function Proximity (Fig. 
The values obtained in this exercise differ greatly from those obtained in Exercise 1. During Exercise 1 we calculated the distances between the centroids of polygons. This may result in longer distances than those generated by the technique used in Exercise 2, which measures the mean or minimum distance. The two techniques can produce different results, depending on the number, the extent and the shape of the features being analysed.
Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http:// creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. Overall accuracy is expressed as the proportion of the map that has been correctly classified. User's accuracy indicates the probability that a pixel from a specific category on the classified map correctly represents the real situation on the ground or reference map. Producer's accuracy indicates the probability that a reference pixel belonging to a specific category has been correctly allocated to that category 
GWR is a statistical technique in which regression points are estimated on the basis of the spatial distribution of data points. A moving window analyses the data points it collects to estimate the coefficients of the selected regression point. This window, or kernel, weights each data point according to the distance within the window and the assigned weighting function (gaussian, exponential, bisquare, tricube, boxcar). Its maximum weighting value is 1 and this decreases as the distance between the observation and calibration data points increases. The size of the kernel is defined by the bandwidth, which indicates the number of data points that will be included in the local calculation for each regression point. This can consider either a fixed or a variable number of reference data points. If a fixed number of points are considered, a specific number will be obtained, while in the case of a variable number, a distance value is given. The number of reference data points therefore varies according to their distribution. It is important to select a suitable bandwidth so as to minimise the cross-validation prediction error. According to 
where b 0 is the intercept, b n is the coefficient, x n is the value of the explanatory variable, and u i ; v i are the coordinates of the data point (Fig. 
This geographically weighted method was adapted for the calculation of local accuracy assessment statistics by 
where P(A = 1) is the probability that the agreement between the classified data and the reference data is equal to 1. This value is 0 when there is no agreement and 1 when there is agreement.
To estimate user's accuracy, it is necessary to analyse the reference data against the classified data. This metric indicates the probability that the reference LUC class y i and is correctly predicted by the classified data x i .

## User's accuracy ##
To estimate producer's accuracy, it is necessary to analyse the classified data against reference data. This indicates the probability that the classified data x i correctly represents reference LUC class y i . Finally, in order to obtain the accuracy values, the coefficients have to be adjusted. To this end, the coefficients are added together, and an alogit function (inverse logit) is applied.

## Producer's accuracy ##


## Utility ##
Exercises 1. To validate a map against reference data/map Geographically Weighted methods can be used to validate single LUC maps by analysing spatial variations in the agreement between reference data and classified remotely sensed data, so enabling us to analyse the spatial non-stationarity of LUC data error and accuracy. They allow to explore the spatial relationships between the reference data and the classified data, exposing possible clusters of land cover errors, and reporting the values for each data point in contrast to global accuracy assessment statistics, which only provide a global value for the entire map.
This technique allows us not only to discover what proportion of the map has been correctly classified but also to estimate in which areas the classification fits best and to analyse possible trends that are only visible spatially. In this way, the spatial distribution of the overall, user's and producer's accuracy metrics can be visualized on a map so as to enable a better understanding of classification uncertainty. the University of Leicester,

## QGIS Exercise ##
First, to estimate local OA values, the tool calculates internally, for each data point, the agreement between the reference data and the classified data, where 0 represents disagreement and 1 represents agreement. Agreement is automatically selected as dependent variable [y] and "1" is selected as independent variable [x], where P(A = 1) is the probability that agreement is equal to 1.
To estimate local UA values, the tool generates a new data frame and obtains two columns. One column shows the presence (1)/absence (0) of the chosen category for the reference data, while the other column shows the same for the classified data. The reference data (RD) is selected as dependent variable [y], and the classified data (CD) is selected as independent variable [x], where P(RD = 1|CD = 1). The procedure for producer's accuracy is very similar. The classified data for the chosen category is selected as dependent variable [y], and the reference data is selected as independent variable [x], where P(CD = 1|RD = 1).
In order to ensure that the tool works correctly, various parameters must be configured. Selecting an appropriate bandwidth is therefore crucial. A small bandwidth would include too few data points in the local sample, making it unreliable for calibrating the model, while a large bandwidth would include too many data points, so reducing the local analysis capacity. A spatially distributed data sample is also required.
The fact that the parameters must be configured and the need for more in-depth knowledge to interpret the results could be considered a disadvantage when choosing these validation methods. Another important consideration is that using large data samples can lead to long runtimes.
Exercise 1. To validate a map against reference data/map Aim To assess the spatial variation of accuracy assessment measures (overall, user's and producer's accuracy) when validating the Marqués de Comillas LUC map against a reference set of points.

## Materials ##


## Marqués de Comillas random sample points from Mexico (2019) Boundary of Marques de Comillas ##


## Requisites ##
The data points must be projected in their corresponding reference system. The vector point file must include two attributes, one corresponding to reference LUC data and one to classified LUC data. It is recommended that the data points have an appropriate random distribution. Sample size should not be overly large, as this could lead to long runtimes.

## Execution ##
If necessary, install the Processing R provider plugin, and download the Local accuracy assessment statistics.rsx R script into the R scripts folder (processing/ rscripts). For more details, see chapter "About This Book" of this book.
Step 1
Open the Local accuracy assessment statistics function and fill in the required parameters (see Fig. 
Step 2
The parameter configuration for calculating User's Accuracy is very similar. Select the corresponding accuracy assessment statistic in the "Accuracy" option ("User") and the category you want to assess in the "Category" option, (see Fig. 
Step 3
To estimate the producer's accuracy values, the same steps must be followed (see Fig. 
Step 4
Finally, the coefficients adjusted by the Local accuracy assessment statistics tool were interpolated using the Inverse Distance Weighted method (IDW interpolation tool in QGIS) (see Fig. 
The names of the column or attribute obtained as a result of applying the tool and indicating the local overall, user's and producer's accuracy values are "g__SDF_", "coefs_u" and "coefs_p" respectively. This column must be specified in the "Interpolation attribute" option in line with the accuracy metric being analysed.
Step 5
As an additional, optional step, the raster images obtained by interpolation can be clipped by mask using the Marques de Comillas boundary (Clip raster by mask layer tool in QGIS) in order to provide a better visual representation. In addition, a discrete colour scale using six classes was chosen in order to make interpretation of the data more straightforward.

## Results and Comments ##
After the execution of the previous steps, we obtain a new attribute column with the estimated local values for OA, UA and PA respectively, and the interpolated distribution maps for these accuracy measures. Another output of the tool is a new layer that includes the estimated Overall Accuracy value for each data point. In addition, a summary of the local and overall values calculated is displayed in the log window (Fig. 
The IDW interpolation method is used to generate an area that visually represents the distribution of the values obtained, offering a more detailed spatial representation of the distribution of accuracy and error than that provided by a single overall accuracy value. Figure 
Figure 
The last part of this exercise focuses on Producer's Accuracy. In this case, it describes omission errors related to the tropical rain forest class. User's accuracy varies from 0.56 to 0.89 (variation of 0.33), despite the global value for the entire area of 0.74 (Fig. 
Figure 
Producer's accuracy has the highest range of variation, with User's accuracy close behind. By contrast, Overall accuracy has a relatively small range, indicating low levels of spatial variation. Despite this, the maximum Overall accuracy value (0.84) is below the value proposed by 
In conclusion, Local accuracy assessment statistics should be considered as a useful complement to the cross-tabulation matrix and its global accuracy statistics in that they provide more detailed information that can help improve classification techniques by locating possible error clusters with greater precision. It is also important to stress that a visual interpretation can enable better decisions to be taken when evaluating and validating LUC maps.   The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## Part IV Land Use Cover Datasets: A Review ##
Global General Land Use Cover Datasets with a Single Date David García-Álvarez, Javier Lara Hinojosa, and Jaime Quintero Villaraso

## Abstract ##
Global general Land Use and Land Cover (LUC) datasets map all land uses and covers across the globe, without focusing on any specific use or cover. This chapter only reviews those datasets available for one single date, which have not been updated over time. Seven different datasets are described in detail. Two other ones were identified, but are not included in this review, because of its coarsens, which limits their utility: Mathews Global Vegetation/Land Use and GMRCA LULC. The first experiences in global LUC mapping date back to the 1990s, when leading research groups in the field produced the first global LUC maps at fine scales of 1 km spatial resolution: the UMD LC Classification and the Global Land Cover Characterization. Not long afterwards, in an attempt to build on these experiences and take them a stage further, an international partnership produced GLC2000 for the reference year 2000. These initial LUC mapping projects produced maps for just one reference year and were not continued or updated over time. Subsequent projects have mostly focused on the production of timeseries of global LUC maps, which allow us to study LUC change over time (see Chapter "Global General Land Use Cover Datasets with a Time Series of Maps"). As a result, there are relatively few single-date global LUC maps for recent years of reference. The latest projects and initiatives producing global LUC maps for single dates have focused on improving the accuracy of global LUC mapping and the use of crowdsourcing production strategies. The Geo-Wiki Hybrid and GLC-SHARE datasets built on the previous research in a bid to obtain more accurate global LUC maps by merging the data from existing datasets. OSM LULC is an ongoing test project that is trying to produce a global LUC map cheaply, using crowdsourced information provided by the Open Street Maps community. The other dataset reviewed here is the LADA LUC Map, which was developed for a specific thematic project (Land Degradation Assessment in Dryland). This dataset is not comparable to the others reviewed in this chapter in terms of its purpose and nature, as is clear from its coarse spatial resolution (5 arc minutes). We therefore believe that this dataset should not be considered part of initiatives to produce more accurate, more detailed land use maps at a global level.

## Project ##
The Department of Geography of the University of Maryland hosted one of the first research groups to use the classification of satellite imagery for global LUC mapping. They initially produced an LUC map at a spatial resolution of 1 degree for the year of reference 1987. This was followed sometime later by the production of a finer map at 8 km for 1984. Finally, the project delivered a map at 1 km, which at that time was the finest resolution at which global LUC mapping had ever been carried out.
The Global Land Cover Facility that hosted all this data recently went offline. This means that there is currently no official website that supports the datasets and provides information about their particular specifications. The map at 1 km can however be downloaded from external sites. The earlier maps at coarser resolutions are no longer available.

## Production method ##
The UMD LC was obtained through supervised classification with a decision tree algorithm of imagery captured by the AVHRR sensor. Urban and built-up areas were not mapped, nor were water covers. Instead, they were extracted from auxiliary sources. The classification obtained in this way was then improved in a post-classification stage by expert regional labelling, based on inconsistencies that were identified by the experts.

## Product description ##
Users can download the UMD LC Classification in two formats (.lan, .img), which are available in the section "GIS-Compatible Formats". The download is not easy and does only include the raster file with LUC information. 

## Downloads ##


## Practical considerations ##
There is no official website hosting this dataset, which makes it more difficult to access and understand. Users must bear in mind that this was one of the first global LUC datasets ever developed and it can therefore be considered outdated in technical terms.
Coarser versions of the 1 km map, resampled at 0.25, 0.5 and 1 degree of spatial resolution, are also available. 
Information about the codification and the meaning of all the other classification schemes can be found in the technical documentation included in the downloaded product, as well as in the documentation available on the project's website. 

## Practical considerations ##
For more information about the product, users are referred to its readme file,

## Project ##
GLC2000 was a project run by the Joint Research Centre (JRC) of the European Commission in collaboration with regional teams across the globe. The objective of the project was to create a homogeneous, coherent global LUC map that was suitable for environmental monitoring. The reference year 2000 was chosen because of its particular significance for that purpose. One of the most successful aspects of the project was the coordination of different teams across the globe to produce a global LUC map. To this end, GLC2000 provides a global dataset, together with a set of more detailed regional datasets adapted to the specificities of each territory.

## Production method ##
GLC2000 was produced by different work teams across the globe. To this end, the world was split into 18 different regions, with each team mapping either a specific region or an area of special interest within a region.
A LUC map for each region was obtained through unsupervised classification of imagery captured by the VEGETATION sensor. The classifications obtained were then labelled by each regional team according to their local expertise in the area. Input for the classification varied in line with the particular characteristics of each region.
Regional LUC maps were merged into the global product, which is a coherent and homogeneous generalized mosaic of the set of regional maps. However, these regional maps provide more detail than the global one.

## Product description ##
GLC2000 consists of two main products: the harmonized global LUC dataset covering the whole earth and the set of detailed regional LUC datasets. The Global LUC map can be downloaded in four different formats (ESRI, Binary, Tiff, Img), whereas the regional maps are only available in two (ESRI, Binary). The product for download includes a file to symbolize the raster LUC map as well as auxiliary information to interpret the legend. -Folder with raster file of the regional LUC map (glc_eu_v2) -Colormap file to symbolize the raster in ArcGIS (.clr) -DBF file with the map legend

## Downloads ##


## Legend and codification ##
Code Label 

## Practical considerations ##
Information about map metadata is easily available on the project's website together with technical documents describing the products. This information can help users gain a better understanding of the maps and all their specific characteristics, advantages and disadvantages. GLC2000 has also been widely analysed in the scientific literature. Users can find out more about the particular characteristics and the accuracy of the database by consulting some of the references of interest cited above.

## Project ##
This project aimed to merge available global LUC maps to create a new, more accurate dataset, in a bid to enable more accurate global LUC mapping. Reference LUC data collected by the Geo-Wiki platform via crowdsourcing was employed in the fusion process, so pioneering a practice that has become more common in recent years. The dataset obtained in this way was one of the first, best-known examples of data fusion for global LUC mapping.

## Production method ##
The hybrid map of the Geo-Wiki project was produced by merging three global LUC datasets: GLC2000, GlobCover and MODIS LC. Whereas GLC2000 shows the LUC state of the world for the reference year 2000, the other two sources provide LUC information for the reference year 2005. The spatial resolution of the hybrid map is the same as applied in the dataset with the highest resolution: GlobCover (300 m). The other two datasets, which had a spatial resolution of 1 km, were resampled to fit this resolution.
For each dataset, a probability layer was produced indicating the probability of that source representing the correct LUC class on the ground. These layers were obtained by regressing the datasets with validation points created through Geo-Wiki campaigns. A Geographically Weighted Regression (GWR) algorithm was employed to this end.
The probability layers were later merged in two different ways, delivering two LUC maps. For Hybrid Map 1, the LUC category from the dataset with the highest probability in the probability layers was selected. For Hybrid Map 2, when two LUC datasets agreed on a LUC category, this was selected. When the LUC datasets disagreed, the LUC category from the dataset with the highest probability in the probability layers was chosen.

## Product description ##
Users can download the hybrid map in a compressed folder (.rar) which also contains the raster layers that store the LUC information. No other auxiliary information is provided.

## Downloads ##


## Geo-Wiki Hybrid (folder) ##
-A raster file with LUC information (.img) 

## Practical considerations ##
The Hybrid map is available online through the Geo-Wiki platform. 

## Project ##
Land Degradation Assessment in Dryland (LADA) is a project led by the Food and Agriculture Organization (FAO) of the United Nations that aims to assess and map land degradation at different scales and levels, so as to understand its impact on land use. As part of the datasets created in the project, a map of the world's Land Use Systems (LUS) was developed. Many other datasets were also created within the framework of this project, which may be of interest to users.

## Production method ##
The dataset was obtained after the interpretation of LUC units over a spatial dataset generated by the overlay of different spatial thematic layers: the GLC2000 LUC map, cropland LUC maps, livestock distribution data, ecosystem and ecological indicators and socioeconomic factors such as population density.

## Product description ##
The LADA LUC map can be downloaded in two different formats (ESRI GRID or TIF). In each case, users download the raster files containing the LUC information, together with a layer style file to symbolize the dataset in a GIS.

## Downloads ##


## ESRI GRID folder ##
-Folder with raster files including LUC information ("lus") -Folder with product metadata ("info") -Layer style file for ArcGIS (.lyr) TIF folder -Raster file with LUC map (.tiff) -Layer style file for ArcGIS (.lyr)

## Legend and codification ##
Code Label Although the GLC-SHARE was produced in 2014, it was conceived as a living database that could integrate new LUC datasets as they were released or updated. Its production method has been made public, so enabling product replication.
As GLC-SHARE was produced by merging data from multiple databases, it has no specific date of reference. There are different dates for each part of the world, according to the main product that was used to map them.

## Production method ##
GLC-SHARE was produced by merging and integrating high-quality LUC data for different areas of the world. LUC data at all scales (global, national, sub-national, regional) was used to produce the map.
In order to merge the various LUC datasets into a single product, their legends had to be harmonized. When different products were available for the same area, the one with the most detailed, most accurate data was chosen. If no products were available at detailed or national scales, global LUC datasets (Globcover 2009, MODIS VCF 2010 and Cropland database 2012) were used instead. The main areas not covered by high-resolution datasets included Latin America, West Africa, Indonesia and important parts of Asia, such as Thailand and the Arabian Peninsula.
An initial map for each of the 11 land cover classes that make up the classification legend of the GLC-SHARE was obtained. Each map shows the proportion that each land cover occupies in each pixel of the GLC-SHARE grid. Finally, from the 11 thematic rasters created, a general raster was obtained indicating the dominant land cover type in each pixel.

## Product description ##
GLC-SHARE products can be downloaded in raster format or as a kml file to upload in Google Earth or any other GIS software. GLC-SHARE maps are also available through a WMS web service.
Users can download the global GLC-SHARE LUC map, which indicates the dominant land cover type in each pixel, or individual LUC rasters showing the proportions of each LUC type in each pixel. In these rasters, the pixel value refers to the proportion (0-100) at which each category is represented in the pixel. A pixel covered exclusively by artificial surfaces would have a value of 100 in the "GLC-Share -Artificial surfaces" raster.
Users can also download auxiliary information about the dataset from the website. This includes a technical report about the product (GLC-Share report) as well as a raster and an excel spreadsheet explaining which dataset was used to map each area of the world (GLC-Share-Sources). 

## Practical considerations ##
GLC-SHARE is a single product with no information about changes in LUC over time. It was created in 2014, which may therefore be considered the reference year for the dataset. However, this date may vary a great deal between the different parts of the world. GLC-SHARE is therefore not recommended for studies or analyses of LUC change.
Although the dataset was conceived as a live map, it has not been further updated with the inclusion of new LULC datasets since 2014.

## Project ##
OSM Landuse/Landcover (LULC) is a LUC dataset created as part of the H2020 project "LandSense", which aims to engage citizens in the production of LUC information. The OSM LULC has been developed above all by the GIScience research group from Heidelberg University.
OSM LULC is an attempt to exploit the LUC information contained in the OpenStreetMaps (OSM) database. It is a test project and therefore cannot be regarded as a final product with full global coverage. Nevertheless, the project has developed a workflow to obtain LUC information from the OSM database as well as a methodology for obtaining an LUC map with full coverage over a specific test area (Heidelberg), filling the gaps in the OSM via classification of satellite imagery.

## Production method ##
OSM LULC was produced using a very simple method. Authors downloaded the OSM database and translated the tags that define the features stored in the database into LUC terms (the legend for the Corine Land Cover (CLC) survey was used as a reference). An equivalence table between the OSM tags and the CLC level 2 legend was created.
The OSM LUC information, in vector, was generalized in a 30 m pixel side grid. In the event of feature overlap when aggregating information, preference was given to the smaller features.
Gap areas not covered by the OSM database were filled with the LUC information obtained by a supervised classification of Landsat imagery with the random forest classifier. This process was only carried out for a European test area, leaving important information gaps in the rest of the global map.
Due to the particular characteristics of the OSM database, LUC information is not provided for a single date. Each feature of the database has a different date. This makes it difficult to determine the date of reference for each pixel in the dataset.

## Product description ##
The product was initially distributed in tiles. However, users can also request a specific file for their area of interest by email. These files contain the LUC map and an Excel spreadsheet with the pixel count for each category. They do not include the qualitative meaning of the category codes. 

## Downloads ##


## Practical considerations ##
The website for this database includes a form for those who want to download the map. However, interested users are recommended to contact the map producers directly, as the first approach does not always work. Contact details for the map producers are available at the project's website. 

## Global General Land Use Cover Datasets with a Time Series of Maps ##
David García-Álvarez, Javier Lara Hinojosa, Francisco José Jurado Pérez, and Jaime Quintero Villaraso Abstract General Land Use Cover (LUC) datasets provide a holistic picture of all the land uses and covers on Earth, without focusing specifically on any individual land use category. As opposed to the LUC maps which are only available for one date or year, reviewed in Chap. "Global General Land Use Cover Datasets with a Single Date", the maps with time series allow users to study LUC change over time. Time series of general LUC datasets at a global scale is useful for understanding global patterns of LUC change and their relation with global processes such as climate change or the loss of biodiversity. MCD12Q1, also known as MODIS Land Cover, was the first time series of LUC maps to be produced on a global scale. When it was first launched in 2002, there were already many organizations and researchers working on accurate, detailed global LUC maps, although these were all one-off editions for single years. The MCD12Q1 dataset continues to be updated today, providing a series of maps for the period 2001-2018. Since the launch of MCD12Q1, many other historical series of LUC maps have been produced, especially in the last decade. This has resulted in the LUC map series covering a longer time period at higher spatial resolution. Recent efforts have focused on producing consistent time series of maps that can track LUC changes over time with low levels of uncertainty. GLCNMO (500 m), GlobCover (300 m) and GLC250 (250 m) provide time series of LUC maps at similar spatial resolutions to MCD12Q1 (500 m), although for fewer reference years. GLCNMO provides information for the years 
The project was launched in 2009 and has been developed in different phases. The initial idea was to create a LUC product covering three time periods 
Apart from LUC maps, other interesting products have also been created as part of the Climate Change Initiative: weekly image composites of the AVHRR (1992-1999, 1 km), 

## Production method ##
The LC-CCI LUC map series is based on a single base LUC map that is progressively updated and backdated. The base LUC layer was created by classifying a series of composite MERIS imagery for the period 2003-2012. A different classification was carried out for each year of this period, and the map finally obtained was a combination of all these classifications. This allowed them to differentiate between land cover states (i.e. those land features that remain stable over time) and land cover seasonality (i.e. natural, seasonal variability of land cover features that do not imply a change in the cover itself).
The classification method combined the GlobCover unsupervised classification chain with a machine learning algorithm. During the classification process, a series of spectrotemporal classes were identified. These were later labelled to LUC classes with the help of experts. The classification was regionalized to account for regional diversity and local heterogeneity of land cover characteristics.
Change detection for updating and backdating the base map was carried out with imagery from different sensors (AVHRR, SPOT, MERIS and PROVA), according to image availability. Changes were detected at a spatial resolution of 1 km, and since 2013 have been delineated at 300 m. Previously, delineation of changes at finer spatial resolutions had been impossible due to the lack of available images.
As a general rule, the only changes studied were those between six wide categories, which are not semantically close to each other: agriculture, forest, grassland, wetland, settlement and others. These changes had to persist for at least two years to be considered. The purpose of these rules was to try to ensure the stability over time of the LUC map series, avoiding technical changes and noise.

## Product description ##
The LC-CCI dataset is distributed in different ways. This gives users the flexibility to download the product that best suits their needs. A single LUC map in either GeoTIFF or NetCDF4 may be downloaded for each year of the period 1992-2015. For the most recent years (2016-2018), these are only available in NetCDF4 format. Additionally, the whole time series of maps for the period 1992-2015 can be downloaded as a single raster with multiple bands, in either of the two formats available.
When downloading the LUC maps, users only gain access to the rasters with LUC information. However, other supplementary information is available on the project's website. This includes a CSV file with the legend description; layer style files for displaying the rasters in common GIS software (ArcGIS, ENVI and QGIS); GeoTIFF files with information about the quality and uncertainty of the LUC maps time series (Quality flags); and a data package for users working with the Sen2Cor classification software. 

## Downloads ##


## Practical considerations ##
The project is aimed at the climate change research community and therefore provides the LUC data in the NetCDF4 raster file format commonly used by this community. However, .nc files are much heavier than .tiff files.
LUC maps for single years are easily displayed in QGIS. However, raster files storing the whole series of LUC maps for the period 1992-2015 are very heavy and are difficult to display in QGIS without a computer with good processing power.

## Project ##
GlobeLand30 (GLC30) is a project funded and promoted by the Chinese government and the National Science Foundation of China. It aims to coherently map the land uses and covers on the world's surface at a detailed scale, using images from the Landsat satellite imagery archive.
The project initially focused on analysing the best methods and procedures to carry out such an ambitious task. It then produced a global LUC map at 30 m for the reference years 2000 and 2010. An update of the dataset for the year 2020 was recently released, in which Antarctica was mapped for the first time.

## Production method ##
GLC30 was obtained after classifying Landsat imagery using a pixel-object-knowledge-based (POK-based) classification approach. Other sources of complementary imagery were also used for the reference years 2010 (HJ-1-China Environment and Disaster Reduction Satellite) and 2020 (GF-1-China High Resolution Satellite).
The classification was carried out independently for each of the mapped categories. Water bodies were mapped first, followed by wetlands, snow and ice, artificial surfaces, cultivated land, forest, scrubland, grassland, barren land and finally tundra. Once a LUC category had been classified, the pixels assigned to that category were masked for the following classifications.
Each category was classified according to a specific approach, adapted to the characteristics of the features being mapped. For most of the categories, the classification approach consisted of three main steps: a pixel-based classifier, image segmentation and knowledge-based verification. For this last step, different sources of auxiliary information were used via their integration in a web-based data platform.

## Product description ##
GLC30 is distributed in tiles. Users can separately download a LUC map for each tile and year of reference. The download includes the LUC map in raster format, a metadata file and a vector file with information about the satellite imagery used to obtain the map. 

## Downloads ##


## Practical considerations ##
The GLC30 LUC maps for 2000, 2010 and 2020 can also be accessed online through the project website, 
There are no technical documents describing the latest update of the map for the year 2020. Methodological changes in the production of the map could have been implemented which could lead to errors when comparing with previous editions.
The project website is not always maintained. It has been unattended for many months over recent years. If the website is not maintained, it is possible that the dataset may be not accessible in the future.

## Project ##
This product forms part of the project led by Tsinghua University to effectively map land uses and covers across the world, which mainly focused on FROM-GLC and the production of thematic LUC databases. Several of these datasets were used in the production of GLC250. The classification legend for GLC250 was also taken from FROM-GLC.

## Production method ##
GLC250 was obtained after the classification of MODIS imagery (MOD13Q1) with a random forest classifier fed with auxiliary data: slope, latitude, MODIS vegetation indexes. For each year of reference 
The three probability maps obtained after the classification carried out for each year of reference were processed through a spatial-temporal consistency model (MAP-MRF) to improve the LUC classification. The final LUC map was improved in a post-classification phase through a rule-based label adjustment method using auxiliary data from MODIS Vegetation Continuous Fields (MOD44B), slope and Enhanced Vegetation Index series.

## Product description ##
A map for each year of reference can be downloaded in a single compressed file. Each file contains all the raster files that make up the LUC map for each year of reference. To this end, the global map is split into multiple tiles following the MODIS tile grid. 

## Legend and codification ##
The GLC250 classification scheme is the same as that developed for FROM-GLC. A complementary product at coarser resolution has been developed as part of the same project: MCD12C1 (0.05 Deg).

## Production method ##
MCD12Q1 was obtained by means of supervised classification (Random Forests) of MODIS imagery for the period 2001-2020. Once the classification had been obtained for each year, it was adjusted with the aid of auxiliary data: C5 MCD12Q1, C6 MODIS Land Water mask, C5 MODIS Vegetation Continuous Fields (VCF), WorldClim dataset, a global urban layer and global crop type information compiled from census data.
As a result of the classification, class probability rasters were obtained for each LUC category. These inform about the probability of each pixel belonging to a specific LUC category. These probability layers provided a base on which to map LUC covers according to six different classification schemes: IGBP, UMD, LAI, BGC, PFT and FAO-LCCS. In order to ensure the consistency of the classification over time, a hidden Markov model (HMM) was applied to the adjusted classification to reduce spurious changes over time.

## Product description ##
MCD12Q1 may be downloaded through different servers or tools: AppEEARS, Data Pool, NASA Earthdata Search, USGS EarthExplorer, OPeNDAP, DAAC2Disk Utility and LDOPE. Depending on the server or tool chosen, users can download the product as a single file for each year of reference or in tiles for specific areas of interest.
The download includes the raster file with LUC data in six different classification schemes and PDF documents with the technical specifications for the product. 

## Downloads ##


## Practical considerations ##
As there are no auxiliary datasets or documentation, users who require more detailed information about the characteristics of the dataset should consult the scientific papers cited above (14.6 Technical Documentation). 

## Overall accuracy ##
Expected to be >78.0% 

## Production method ##
GlobCover maps were obtained by classifying imagery captured by the MERIS sensor. Urban and wetland areas, which are not well represented, were classified using a supervised classifier. The remaining categories were classified in a series of spectro-temporal classes through an unsupervised classifier. Once classified, the spectro-temporal classes were labelled automatically according to the information provided by the reference datasets. For the 2005 map, the reference datasets were the GLC2000 global LUC map (see Sect. 3 in Chap. "Global General Land Use Cover Datasets with a Single Date" Global General Land Use Cover Datasets with a Single Date) and other high-quality regional LUC maps. For the 2009 map, the GlobCover 2005 map was used as a reference.
The area for classification was divided into different regions, to account for the ecological and reflectance diversity of the world. Once labelled after classification, the LUC map was finally edited to account for inaccuracies in the representation of certain features.
For the 2005 version, regional maps with a more detailed legend were also produced following the same classification procedure.

## Product description ##
A zipped file is available for each GlobCover map. It contains the raster layer with the LUC information and all the auxiliary data that users may need to correctly interpret the dataset. This includes the classification legend, technical and data quality information, and files with the layer style of the map to automatically symbolize the raster in GIS software. A complementary raster detailing the source of the LUC information for each pixel (MERIS sensor classification (value = null) or a land cover database (value = 1)) is also provided. In a separate file, users can also download a raster for a coloured version of the LUC map. 

## Downloads ##


## Legend and codification ##
A specific two-level classification scheme legend was initially developed for the FROM-GLC project in 2010. This was updated with various changes for the FROM-GLC map for 2015. The map for 2017 has the simplest, least detailed classification legend (Level 1). In each case, we include the most detailed classification scheme available for each year. Users can consult the correspondence between level 2 and level 1 of the classification scheme for the years 2010 and 2015 at the project website. 

## Practical considerations ##
The project website, where all the information is stored and available for download, is not user-friendly. It is not easy to find the information the user is looking for. Users may also struggle to download datasets for their area of interest according to latitude and longitude information. When available, we recommend using the kmz file with Google Earth for this purpose.
There is little additional information. For a complete description of the characteristics of the different maps, we recommend users to read the scientific papers cited in the introduction to this dataset above (14.8. Technical Documentation).
7 http://data.ess.tsinghua.edu.cn/.

## 306 ##
D. García-Álvarez et al.

## Project ##
CGLS-LC100 is one of the deliverables produced as part of the Copernicus Global Land Service (CGLS), which aims to provide a series of bio-geophysical products to monitor land surface at a global scale. In addition to this LUC package, the programme produces other relevant variables, such as the Leaf Area Index (LAI), the Fraction of Absorbed Photosynthetically Active Radiation (FAPAR), the Land Surface Temperature, soil moisture and other vegetation indices.
The first version of CGLS-LC100 was released in 2017, mapping LUC for Africa. Since then, several updates of the product have improved the production methodology and extended its temporal and geographical coverage. The last version of the product (Collection 3), released in 2021, covers the whole world for the period 2015-2019. It includes a method for detecting land cover change that addresses the main sources of technical uncertainty when studying change in a time series of LUC maps.
In addition to the LUC map described here, the product also includes a series of continuous field layers or "fraction maps" for the basic LUC classes mapped. Future updates of the product are expected on an annual basis, using the imagery provided by the Sentinel satellite missions.

## Production method ##
The Copernicus Global Land Service Dynamic Land Cover map is produced through a multistep processing framework. First, PROBA-V satellite images are pre-processed and merged following a Sentinel-2 tiling grid to create a 3-year epoch mosaic for each reference year. Second, a series of metrics (spectral and textural metrics, descriptive statistics) are extracted from each epoch mosaic. Third, imagery for all the epochs is classified using a regression algorithm, which delivers a cover fraction layer for each basic LUC class and reference year, and a supervised classification algorithm, which delivers a LUC map for each reference year.
Various auxiliary data sources are used in the classification phase, i.e. seven different data masks and three extra datasets: biome clusters, water cover fractions and built-up cover fractions.
In order to ensure the temporal consistency of the LUC map series, it was decided to include a temporal postprocessing phase in the production of the dataset. This consists of a BFAST break detection algorithm and a Hidden Markov Model. The former is used to detect changes in an independent time series of MODIS NIRv imagery, while the latter is used to rule out technical changes in the classified epoch images.

## Product description ##
CGLS-LC100 is distributed in tiles, following the Sentinel-2 tiling grid (110 Â 110 km). For each tile, users can download many different layers: the discrete classification containing the LUC map for the selected area; a layer with the classification probability; layers of cover fractions for each of the basic LUC classes mapped; a layer showing the level of confidence for the change measured between the different years in each pixel; and two extra layers: forest types and input data density.
The download of the LUC map only includes the raster file with the LUC information. Each reference year must be downloaded separately.  

## Downloads ##


## Practical considerations ##
Because of the large number of datasets available through this project, users are encouraged to make use of the different layers of LUC information available. This will give them a better understanding of the uncertainties and limitations of the product. Users can download the product covering the whole globe, which is distributed through the files in the Zenodo repository. 8  Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http:// creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.  

## Production method ##
Historic LUC maps for the HILDA project were obtained through an extensive workflow involving various steps. First, gross and net LUC changes per decade were obtained for the period 1950-2010 from a set of sources providing historic LUC information: UNFCCC national reporting data, CORINE Land Cover, Historisch Grondgebruik Nederland (HGN) for the Netherlands, FAO-RSS data and BioPress data with classified aerial photographs of 73 sample sites across Europe. Later, LUC data was spatially allocated by the HILDA model. Four categories were spatially allocated at this stage. A fifth category (other land) remained static throughout the time series. Water was a subclass of the "other land" category, which was only separated in the final maps for visualization purposes.
The model allocates the LUC categories using a series of probability maps. A specific probability map for each category was created on the basis of historical LUC maps and a range of socioeconomic and physical (soil properties, climate and terrain) factors. The categories were allocated hierarchically according to their socioeconomic value: settlements were allocated first, followed by croplands, forest and grasslands.
Once the model had been run for the 1950-2010 timeframe, four extra maps were obtained for the period 1900-1950 based on historical LUC statistics and an extrapolation of the change matrix. The pre-1950 maps therefore assume stable transition rates for the period 1950-2010. This could be an important source of uncertainty in these maps.

## Product description ##
The product is delivered in four different packages, two of which include the series of LUC maps . Of these, one considers the net changes over the course of each decade, while the other considers the gross changes. The other two packages detail the specific transitions that take place between the different categories, one charting net changes and the other gross changes.
Each package can be downloaded in three different file formats (ESRI Grid, TIFF, ASCII). Each download includes a raster with LUC information for each decade and a supplementary file with the technical description of the product. The number of countries taking part in the project has been increasing since its inception, from the initial group of 26 countries that created the CLC 1990 to the 39 countries that participated in the most recent edition 2 . In the meantime, the production of CLC has undergone several technical and methodological changes. The fact that CLC is produced at a national level means that methods vary from one country to the next.

## Downloads ##
Because of its long life, detail, consistency and wide range of applications, CLC is one of the most renowned LUC mapping initiatives worldwide. Various European countries have developed national LUC products based on CLC. In some cases, these products are new CLC layers with an extended legend, adapted to the specificities of the country. In other cases, they are new CLC layers for different dates to those used in the main Europe-wide project.

## Production method ##
The production of CLC is coordinated by the European Environment Agency (EEA). Each participant country is responsible for mapping its own territory according to the general guidelines developed by the EEA.
The method of production may vary from country to country. Initially, CLC was mapped at national scales based on the photointerpretation of Landsat imagery. In the following editions, most of the countries decided to stick to this method, using different satellite imagery according to EEA prescriptions: Landsat, SPOT; ITS P6, RapidEye, LISS III, Sentinel. In the latest editions, the production method has varied in some cases. A few countries, like Germany or Spain, produce the CLC database by generalizing national LUC databases at finer scales. This has introduced important changes in the way land uses and covers are mapped over time for these countries. For both production methods, photointerpretation and map generalization, the CLC map obtained is then subject to expert review to ensure its consistency and validity.
The first CLC map was produced for the reference year 1990 and the subsequent editions have been updates of this initial map. The national teams do not draw a new map for each new reference year. Instead, they map the changes for the analysed period 
In addition to the maps for each reference year, CLC produces change layers for each period between reference years 

## Product description ##
CLC is made up of two spatial layers: a Land Use Cover map for each reference year 
CLC layers are provided in either vector (ESRI or GeoPackage databases) or raster (.tiff) formats. As might be expected, the vector data is much heavier than the raster data, because of its higher definition.
Together with the LUC layers, the CLC product includes all the auxiliary information required to understand the LUC information provided by the CLC layers: a style layer for the raster, the legend description, technical information and other relevant metadata. LUC maps for the French overseas departments (Guadeloupe, French Guinea, Martinique, Mayotte and Reunion) are also provided in auxiliary layers.

## Downloads ##
The base layers with LUC maps for each reference year (CLC) have the same structure and group of files, as do the change layers for each period of analysis (CHA). This is why we only describe the file structure once for each type of format.
CLC 2018 (Geodatabase)/CHA 2012-2018 (Geodatabase) -Geodatabase files with CLC vector layers (DATA folder) -Folder with CLC vector data for French overseas departments -Layer style files for ArcGIS (.lyr), QGIS (.qml) and any other GIS software (.sld) (Legend folder) (continued) 2 https://land.copernicus.eu/pan-european/corine-land-cover. -Code_18: LUC code for the year 2018.
-Remark -Area_Ha: Area of the polygon, in hectares.
-ID: Unique identifier for each polygon.
-Shape_Length: Perimeter of the polygon, in metres.
-Shape_Area: Area of the polygon, in square metres.
-C18: LUC code for the year 2018.

## Database ##
General Land Use Cover Datasets for Europe  

## Practical considerations ##
CLC was originally mapped in vector format. This format provides higher precision and detail and is therefore recommended when working at local and regional scales. At national and supranational scales, raster data can be more suitable, as vector data is too heavy and may be difficult to handle in desktop computers with insufficient processing power. Users can download the vector CLC to rasterize the database to the spatial resolution they require. The 100 m offered is the reference resolution provided by the EEA, but it is not the only one at which the map could be used.
Users should be aware that different mapping methodologies were used in different countries, and in some countries, at different times. This could result in significant differences in the way the landscape is mapped and conceptualised, which could introduce important sources of uncertainty in our studies and analyses. The same category could be interpreted differently in different countries, and even within the same country, a particular category could be mapped differently at different times if the production method changes. Those wishing to analyse LUC change should therefore use the change layers rather than the maps.
Project PELCOM (Pan-European Land Cover Monitoring) was a research project funded by the European Union that ran from 1996 to 1999. The main purpose of the project was to develop a consistent methodology to create a continental LUC map for Europe from remote sensing sources. Users were consulted about their needs and requirements and revealed that they would like to have LUC data at coarser and finer spatial resolutions than CLC, and that CLC could be updated more frequently. They also made clear that a dataset of this kind would be useful for environmental modelling and monitoring purposes.
At the time the project was launched, no consistent continental LUC maps were available at high spatial resolution (at least 1 km). The map created through the project sought to provide a high-resolution continental LUC dataset that could later be updated frequently. However, despite these original intentions, the PELCOM map has not been updated since the project came to an end.

## Production method ##
The classification carried out for the PELCOM map was based on AVHRR imagery and NDVI composites from the DLR archive of the JRC. An improved stratified, integrated classification methodology was specifically developed by the creators of this map. To this end, Europe was divided into different strata according to similarities in LULC patterns and phenology.
The classification process consisted of several steps, in which users played an important role. Both supervised and unsupervised classifiers were employed. Some classes (forest, water bodies, urban areas) were mapped through specific workflows, using masks and other strategies, to improve the uncertainty and errors associated with their classification.

## Product description ##
PELCOM may be downloaded in three different formats: ESRI-grid, ERDAS-Image and ENVI. The download includes the raster with the LUC map and, depending on the format chosen, auxiliary information about the product (readme and symbology files).
Detailed technical documentation about the map and its production method is also available from the download site. 

## Downloads ##


## Practical considerations ##
The product is no longer available for download from the official website of the EEA. The only edition that can still be obtained is the map for 2005, which is available through the geoportal of the Université Catholique de Louvain, one of the producers of the dataset. The map can also be consulted online at the same website, without having to download it. While the GlobCorine classification legend focuses particularly on land use, GlobCover centres on land cover. GlobCorine can therefore be regarded as a complementary dataset to GlobCover.

## Project ##
N2K was developed as part of the Copernicus Land Monitoring programme. It maps land uses and covers in the areas that form part of the Natura 2000 network, plus a 2 km buffer zone around their perimeters. Natura 2000 is a network that protects natural areas with rare and threatened species or with rare types of natural habitat.
The dataset first appeared in 2015. A reviewed edition was issued in 2017 with a new classification legend that made it compatible with other European local reference LUC datasets: Riparian Zones, N2K and the Coastal Zone product.

## Production method ##
N2K is obtained by photointerpretation of high-resolution imagery. Various auxiliary datasets are used in the photointerpretation process, namely CORINE Land Cover, Urban Atlas, High Resolution Layers, topographic maps, national WMS services and COTS navigation data. The changes are also photointerpreted by comparing satellite images at two different points in time.

## Product description ##
N2K is distributed as a single vector file covering all mapped Nature 2000 areas. Two formats are available: ESRI Geodatabase and Geopackage. Downloads include the layers with LUC information, a style file to symbolize the layers in GIS and a pdf with the product classification scheme. 

## Downloads ##


## Database ##
N2K was produced according to a hierarchical classification legend made up of four different levels, the most detailed of which is provided here (MAES L3). Information about the other levels of classification and their codes can be found in the technical documents accompanying the dataset. 

## Practical considerations ##
N2K files are very heavy (over 2gb), which means that they may be difficult to use for those without powerful computers.
The map can also be consulted online in a viewer included in the download website of the product. Together with the LUC map of riparian zones, two extra complementary products are also provided: a delineation of Riparian Zones based on a fuzzy modelling approach and an inventory of the Green Linear Elements (hedgerows and lines of trees) growing in those riparian areas.

## Production method ##
The RZ map was obtained through semi-automatic classification of very high-resolution imagery captured by the SPOT and Pleiades satellites 

## Product description ##
A different vector file is provided for each riparian area mapped. Downloads include the vector file with LUC information and pdf documents with information about the product.  

## Downloads ##


## Practical considerations ##
The map can also be consulted online in a viewer available on the download site (see link above).

## Project ##
The Coastal Zones Land Cover/Land Use dataset is produced by the European Environment Agency (EEA) as part of the Copernicus Land Monitoring Service (CLMS). The dataset has been developed in collaboration with the Copernicus Marine Environment Monitoring Service (CMEMS) and representatives from the potential community of users.
It is specifically intended for monitoring coastal areas and provides an important source of information for all EU policies dealing with coastal management and maritime spatial planning.
The dataset maps, at very detailed scale, the land uses and covers in coastal areas in the 39 countries belonging to the EEA. The coastal area mapped is defined by a 10 km inland buffer zone and the Corine Land Cover (CLC) seawards buffer zone. Relevant estuaries, coastal lowlands and nature reserves that extend beyond the buffer zone have also been included.
The dataset's classification legend has been specifically designed to fit the needs of its user community. It is based on the Mapping and Assessment of Ecosystems and their Services (MAES) ecosystem typology and makes the product compatible with other CLMS local monitoring datasets, such as Urban Atlas, Riparian Zones and N2K.
The dataset is composed of two LUC maps for the reference years 2012 and 2018, plus a change layer for the period 2012-2018. The dataset will be updated every 6 years, in accordance with the CLC production timeline.

## Production method ##
The Coastal Zones Land Cover/Land Use dataset is produced via computer-assisted photointerpretation of very high spatial resolution 

## Product description ##
Users can download the Coastal Zones dataset in two different formats: Geodatabase and GeoPackage. Different download files are available for each year of reference 

## Project ##
Sentinel-2 Global Land Cover (S2GLC) was a project funded by the European Space Agency (ESA) in order to create an automatic methodology to globally map LUC at high resolution from Sentinel-2 imagery. The project was led by the Space Research Centre of the Polish Academy of Sciences (CBK PAN). Its main output is the S2GLC 2017 map.
The project was developed in two phases. In the first phase, the proposed methodology was tested in five prototype sites: Germany, Italy, China, Columbia and Namibia. In the second phase, the methodology was adjusted to map LUC for the whole of Europe, except Russia, Belarus and Ukraine.
Production method S2GLC was obtained by classifying Sentinel-2 imagery. Each Sentinel-2 scene was individually classified using a set of multi-temporal images through a random forest classifier. Training data was automatically extracted from existing datasets, such as CORINE Land Cover. A set of probability rasters were obtained from the random forest classifier, and the class finally selected for each pixel was the one with the highest probability over the whole time series. A post-classification step was applied for those pixels with low probabilities.

## Product description ##
S2GLC 2017 can be downloaded as a single file or in tiles. In the first case, users can choose to download the raster LUC file, either symbolized (RGB GeoTiff file) or not (GeoTiff file). Users who opt to download a tile from the map will automatically download both types of rasters. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.  

## Downloads ##


## Practical considerations ##
The maps for each country were usually produced at different dates, so making inter-country comparison difficult.

## Project ##
The SADC Land Cover Database is fruit of a project funded by the South African Department of Arts, Culture, Science and Technology (DACST) through the Regional Science and Technology Programme. It was coordinated by the Council for Scientific and Industrial Research (CSIR) in South Africa, with the participation of organizations from the different countries being mapped. The objective of the project was to deliver a coherent Land Use Cover map covering the Southern African Development Community (SADC) region. The project builds on earlier LUC mapping work carried out at national and regional scales for each of the mapped countries.
The map covers those SADC countries that already had a LUC dataset available for their territory: Lesotho, Malawi, Mozambique, South Africa, Swaziland, Tanzania, Zimbabwe. The other countries in the region are not included in the map.

## Production method ##
The SADC Land Cover Database was obtained by harmonizing and fusing the different national and regional LUC datasets. All the datasets were originally obtained by classification or photointerpretation of Landsat imagery, although the reference years vary from country to country.
The maps were combined by resampling to a spatial resolution of 1 km, before being reclassified according to the same classification system. This reduced the detail of the original maps, a deliberate action to avoid copyright and commercialisation issues.

## Product description ##
The dataset is downloaded as a single compressed file (.zip), which includes the vector LUC map, a metadata file and a complete map (i.e. with colours, graphics, scale and legend) in jpg format that is ready to print out. 

## Downloads ##


## SADC ##


## Legend and codification ##


## Practical considerations ##
A detailed description of the map categories is available in the dataset's metadata. The map's production method entails certain limitations and uncertainties, in that each country has been mapped by a different team, using different sources of imagery for different reference years. Inconsistencies may therefore arise when comparing information between countries.
Project AFRICOVER was a project led and coordinated by the Food and Agriculture Organization (FAO) of the United Nations, which aimed to create georeferenced data for the African continent. The FAO helped the different countries and regions to develop their reference maps, establishing the standards for the final product. Twelve countries participated in the project (Burundi, Democratic Republic of Congo, Egypt, Eritrea, Kenya, Rwanda, Somalia, Sudan, Tanzania, Uganda, Libya and Malawi), which therefore required extensive coordination of many national and regional teams across Africa.
A keystone of the project was the production of LUC maps for Africa. In addition to LUC maps, other georeferenced data were created for a range of themes: hydrology, geomorphology, demography…

## Production method ##
The production of AFRICOVER was decentralised at a national and regional level. Although the FAO defined the guidelines and standards for the product, national and regional teams from each country were responsible for its execution. This meant that although a set of common characteristics regarding the production of AFRICOVER had been established for all the countries involved, certain specificities could also arise.
AFRICOVER LUC maps were mainly obtained through photointerpretation of satellite imagery, of which Landsat was the main source. The photointerpretation scale was 1:200,000. When drawing LUC polygons, the FAO LCSS classification scheme was followed. The FAO provided national and regional teams with specific software and training to carry out LUC mapping according to this approach.

## Product description ##
AFRICOVER LUC maps are distributed at a national level. A compressed file can be downloaded for each country. This includes the vector LUC map and a legend description to help users interpret it. Practical considerations AFRICOVER LUC maps have been created following the FAO LCSS classification scheme. This means that each LUC polygon is described through a specific code that identifies the general cover of the polygon and characterizes it through a series of labels. Users may find this system difficult to understand, as it does not follow a common hierarchical classification legend in which each polygon is defined by a single category. 

## Downloads ##


## Practical considerations ##
The map is distributed as a single, very heavy file (6 Gb). Users with limited computer and internet capacities may find it difficult to download and work with this product. Nonetheless, a preview tool is available online for any user wishing to consult the map. 

## Practical considerations ##
Users can consult the LUC map online on the Université Catholique de Louvain website (http://maps.elie.ucl.ac.be/ geoportail/).

## Project ##
The Large-Scale Biosphere-Atmosphere Experiment in the Amazon (LBA) was an international project launched by the Brazilian scientific community in 1993. The main objectives were to study Amazonia and its role in the earth's ecosystem as well as to understand LUC changes in the area and their environmental consequences.
As part of the project, a global LUC map covering South America was produced from imagery and data of the period 1987/91. Vegetation and soil maps for Brazil were also digitalized on the basis of previous resources. These maps are also available for any interested user as part of the same dataset.

## Production method ##
The LBA LUC map was produced after unsupervised classification of AVHRR imagery, postprocessing and labelling of the classification results. Different sources of auxiliary data were used in the production of the dataset to overcome the limitations of the imagery, including a Global Vegetation Index (GVI) layer, the UNESCO's Vegetation Map of South America, the Hueck's Vegetationsskarte Von Sudamerika and a potential vegetation map of South America based on the Holdridge bioclimatic scheme.

## Production description ##
Users can download the LUC map as a single raster file including the LUC information or as part of a data package including all the products produced within the LBA project. As part of these, we find different vegetation and soil maps for Brazil. In all cases, the download only includes the raster files and no auxiliary information is provided. The change layers include a qualitative description of the classes at the two different points in time. In addition, the pixel values are formed by combining the class code for the land use at point 1 in time with the class code for the new land use at point 2. e.g. the code 1011 refers to a pixel that was Temperate or sub-polar grassland (10) on the first date assessed and had changed to Sub-polar or polar shrubland-lichen-moss (11) on the second. 

## Downloads ##


## Practical considerations ##
Maps at 30 m and 250 m were obtained following a different workflow and are not comparable. The maps for Mexico for 2010 and 2015 were obtained from different imagery sources, which means that changes cannot be calculated by subtracting one map from the other and should only be studied using the change layer distributed by the production team.
No information is offered about the uncertainty of the change layers. They may be subject to important sources of uncertainty and may include a lot of technical or spurious changes that did not actually happen on the ground.
NALCMS is one of the products in the North American Environmental Atlas. Users can consult the different NALCMS layers online, together with a lot of other relevant geospatial information for North America, as part of the Atlas website at http://www.cec.org/files/atlas/. Users can also download any of the displayed layers, including the LUC maps, from the same website. 

## Practical considerations ##
This dataset is not directly available for download. Users wishing to access it must contact the JRC team that produced it (Hugh.EVA@ec.europa.eu, Rene.BEUCHLE@ec.europa. eu).
Although the dataset has been used to assess LUC changes by comparing it with GLC2000, this exercise has many limitations and uncertainties and is therefore not recommended.

## 368 ##
D. García-Álvarez and J. Lara Hinojosa

## Project ##
The Himalaya Regional Land Cover database was developed within the context of the Global Land Cover Network-Regional Harmonization Programme, promoted by the Food and Agriculture Organization of the United Nations (FAO) and UN Environment in collaboration with the Geographic Information for Sustainable Development (GISD) global partnership. The programme aimed to produce reliable, harmonized global land cover information, providing guidance and methodologies for the production of LUC information at national, regional and global levels.

## Production method ##
The database was obtained by automatic segmentation of Landsat imagery for the reference year 2000 plus visual interpretation. The initial classification was refined by interpreting high resolution imagery from Google Earth. A layer of LUC changes was obtained by assessing the base map (2000) against historical imagery for the periods 

## Product description ##
The database is distributed at regional level in vector format for each of the countries and regions that make up the Himalayan region: Afghanistan, Bhutan, China-Yunnan Sheng, China-Xizang Zizhiqu, India, Nepal, Pakistan, Aksai Chin, Arunachal Pradesh, China/India, Jammu Kashmir and Myanmar. An additional vector layer with LUC changes for the period 1970-2007 is also included. The downloaded products consist solely of the vector layers with LUC data. No other auxiliary information is provided with the downloaded file.
A detailed legend for the product can be downloaded separately in Excel or mdb formats. A layer with the boundaries of the region and its administrative units is also available for download. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

## Project ##
The World's Forests 2000 map was one of the products generated within the context of the Global Forest Resources Assessment (FRA) for the year 2000. FRA is a project run by the Food and Agriculture Organization (FAO) that dates back to the year 1946. A new edition is issued every five years on average.
The project, which is carried out in collaboration with the different countries that form part of the FAO, aims to assess the state of the world's forests and understand the changes that they undergo over time. Satellite imagery and remote sensing techniques were used for the first time in the FRA2000 survey. A global map of forests was produced as part of the project. The U.S. Geological Survey (USGS) EROS Data Center (EDC) was in charge of map production. Two extra maps were also produced as part of the project: an ecological zoning map and a map of protected forests.

## Production method ##
The World's Forests 2000 map was produced in two stages. In the first stage, closed forest and open or fragmented forest categories were mapped on the basis of a classification of AVHRR imagery for the period 1995-1996. A complex methodology based on a mixture analysis model and a geographical stratification to account for regional variation in the mapped features was employed to calculate the fraction cover per pixel. The two LUC categories were extracted from these layers based on the tree cover percentages defined by the FAO: 40-100% for closed forest and 10-40% for open or fragmented forest.
In the second stage, the Global Land Cover Characteristics Database (GLCC), obtained from a classification of AVHRR imagery for the period 1992/93, was used to map the remaining categories: other wooded land, other land cover and water. The fact that the different input data (AVHRR and GLCC) had different reference dates led to temporal inconsistency between forest and non-forest categories.
Some auxiliary datasets were also used in the production of the map, such as ecoregion maps and digital elevation models. These helped to merge and split the different categories being mapped.

## Product description ##
The map can be downloaded as a zipped file containing the raster with the LUC information and other auxiliary information. The download includes two versions of the LUC map, one classifying the land covers in a range of values from 1 to 6 and the other classifying the land covers in a range of values from 100 to 600. 

## Downloads ##


## Practical considerations ##
SYNMAP was designed to satisfy the needs of a very specific community: carbon cycle and vegetation modellers. The dataset can be consulted online via a web application. 

## Practical considerations ##
The dataset can be easily visualized and consulted through a web-based visualization tool. 

## Practical considerations ##
Users must bear in mind that although the dataset is distributed as a single raster file, this includes multiple layers with different, complementary information. Nonetheless, the core of the product is the band storing information about the percentage of tree cover. The dataset can be also consulted online through a Web Map Service (WMS).

## Project ##
The Percent Tree Cover Global version is a dataset created within the context of the Global Mapping Project, which aimed to create a global reference database of geospatial information. The project was promoted by the International Steering Committee for Global Mapping (ISCGM) in cooperation with National Geospatial Information Authorities (NGIAs) from different countries and regions across the world. It came to an end in 2016, when the ISCGM decided to wind up the project and transfer all the data to the Geospatial Information Section of the United Nations.
The PTC map was generated by a group of researchers from the Geospatial Information Authority of Japan (GSI) and Chiba University. Two versions of the map were produced: one for the reference year 2003 and another for the reference year 2008.

## Production method ##
The map was obtained via the classification of MODIS imagery. No other information is available about how the PTC Global version was produced.

## Product description ##
A single download containing the map for the entire globe is available for the year 2003. For the year 2008, the map is distributed in 12 different tiles. Each tile covers an area of 90 degrees of latitude and 60 degrees of longitude. The downloads only include the raster files with LUC information. There are no auxiliary data. 

## Downloads ##


## Practical considerations ##
This dataset lacks auxiliary and technical information about specific characteristics and possible limitations, including data about its accuracy. It must therefore be used with caution.
General information about the Global Mapping Project can be found at https://www.gsi.go.jp/kankyochiri/gm_ report_e.html. More information about the project within which the dataset was created can be found at this website.

## 390 ##
D. García-Álvarez and J. Lara Hinojosa 

## Practical considerations ##
This dataset was produced by means of a complex production method that is difficult to understand for those without specialist knowledge of radar data. Those wishing to find out more about this dataset should read the guide cited in the specifications above and other information about the dataset available at https://geoservice.dlr.de/web/dataguide/fnf50/. 

## Change detection ##
No (only one date)

## Overall accuracy ##
Expected to be >69%

## Website of reference Website Language ##


## Not available English ##
Download site http://ftp-earth.bu.edu/public/friedl/GRIPCmap/?C=S;O=A

## Project ##
Global Rainfed, Irrigated and Paddy Croplands (GRIPC) is a map developed by researchers from German and American universities, who aimed to overcome some of the limitations of previous datasets focusing on irrigated croplands. At the time it was released, the dataset offered an up-to-date representation of irrigated croplands across the world at the highest spatial resolution available. It could be useful for those studying agricultural productivity, agricultural hydrology and food security in general.

## Production method ##
The GRIPC map is made up of 4 different categories.
Uncropped areas were extracted from the non-cropland categories of the MODIS Land Cover database for the period 2004-2006. Paddy croplands were independently mapped from different sources, such as crop inventories, due to the challenges involved in classifying cloudy imagery in the tropics. Rainfed and irrigated cropland were mapped using a decision-tree classification algorithm (C4.5) and the "boosting" machine learning technique. MODIS imagery was used as the input for the classification. Climate and agroecozones data were also used as auxiliary datasets. Probability layers obtained from the classification were combined with information from national and subnational cropland inventory-based datasets to finally map the rainfed and irrigated cropland areas. The information from these datasets served to define the probabilities of each category occupying a pixel. Then, the classification results were combined with these probabilities using a Bayes' rule to obtain the final map.

## Product description ##
GRIPC is distributed in 273 tiles, according to the MODIS tile grid. 

## Downloads ##


## Practical considerations ##
GRIPC does not map various important irrigated cropland categories, such as deficit irrigation (irrigation occurring less than once a year), permanent crops (orchards and vineyards) and unharvested pastures. As there is no official website describing the GRIPC and its characteristics, users wishing to find out more about this dataset should consult the scientific paper in which it was presented 

## GFSAD1KCM and GFSAD1KCD ##
According to the authors, data about cropping intensity can be obtained from this product using a time-series of Normalized Difference Vegetation Index (NDVI) data.

## Project ##
The Global Synergy Cropland Map is a dataset created within the framework of the Spatial Production Allocation Model (SPAM), which maps agriculture production across the world. It is a joint effort involving different institutions and universities across the world: AGRIRS, IFPRI Chinese Academy of Agricultural Sciences and Victoria University of Wellington.
The project team aimed to create a more accurate cropland dataset that would be useful for agricultural monitoring and food security policies and studies. The obtained map is a critical input of SPAM.

## Production method ##
A self-adapting statistics allocation model (SASAM) is used to generate the Global Synergy Cropland Map, using LUC datasets at global, supranational and national scales as input, as well as FAO agricultural statistics at national and subnational levels.
Two layers were generated by the model. Firstly, an agreement layer, which shows the level of agreement of all the datasets regarding the location of cropland areas, and secondly, an average cropland percentage layer, obtained by calculating the average of all the input maps. For the agreement layer, datasets with a higher accuracy are given more weight. This accuracy is based on the agreement between each input dataset and the FAO statistics. For the cropland percentage layer, the cropland category definitions in the input maps were translated into cropland percentages.
The final cropland map was obtained after executing the SASAM model, which allocated cropland in the areas with the highest probability in the agreement layer until the total surface area for cropland according to FAO statistics for each country was reached.

## Product description ##
The raster file showing the cropland percentage can be downloaded separately. However, we recommend the full download, which also contains additional information about the dataset, such as its level of confidence.

## Downloads ##
Global synergy cropland map (full download) -Raster file with cropland percentage (.tiff) -Raster file with information about the confidence level of the cropland map (.tiff) -A text file with information about the downloaded product

## Legend and codification ##
Code Label 0-1 Cropland extent percent (0-100%)

## Practical considerations ##
More information about the associated SPAM project is available at www.mapspam.info. The website includes all the spatial datasets about agricultural production generated as part of the project. These complement the information provided by the cropland map reviewed here.
6 UCL-Unified Cropland Layer Project Global Food Security-support Analysis Data 30 metre (GFSAD30) was a project aimed at producing high-resolution cropland maps to inform global food and water security studies and policies. The project sought to overcome some of the limitations presented by previous cropland datasets, such as sources of uncertainty, insufficient precision in the allocation of cropped areas, and a lack of information regarding the intensity and irrigation status of cropland areas. GFSAD30 was the continuation of earlier projects (the GFSAD1KCM and GFSAD1KCD datasets described above) with similar purposes. They all formed part of the MEa-SUREs (Making Earth System Data Records for Use in Research Environments) programme, which promotes the use of data from NASA missions to produce innovative products that are useful for research and policy-making.
Various different US institutions (USGS, BAER Institute, U.S. Department of Agriculture, U.S. Environmental Protection Agency) and universities (New Hampshire, California, Wisconsin, Northern Arizona) took part in the project, together with Google and institutions from other countries (ICRISAT, IAARD).
A global map of cropland extent at a spatial resolution of 30 m for the reference year 2015 was delivered as part of the project. The global map was obtained after merging different maps that had been independently produced for seven different regions across the world. The map for North America was produced for the reference year 2010, instead of 2015.

## Production method ##
GFSAD30 is made up of 7 datasets which were independently produced for different regions across the world: Europe, Middle East, Russia and Central Asia; Africa; Australia, New Zealand, China, and Mongolia; Southeast and Northeast Asia; North America; and South America. Each dataset was produced following a specific production method, although they all share certain common features.
The same imagery source (Landsat) was used for all 7 datasets. Sentinel-2 imagery was also used to map the extent of cropland in Africa. Other auxiliary data, such as elevation data from the SRTM radar, were used for the production of several datasets. In all cases, the extent of cropland was computed using the Google Earth Engine (GEE) platform.
The classification workflow varies in each case. The most frequent classification method was the random forest algorithm. For some datasets, like Africa, additional classifiers (support vector machines, an object-based classifier) were also used. In addition, in order to take the geographical variability within the mapped area into account, producers usually split the classification into agro-ecological zones (AEZs).

## Product description ##
GFSAD30 is distributed in tiles with a 10º edge for each of the mapped regions. Datasets are available from different servers or tools, including Data Pool, NASA Earthdata Search, USGS EarthExplorer and the DAAC2Disk Utility. We recommend users to download the dataset through NASA Earthdata Search and USGS EarthExplorer, on which the geographical coverage of each tile can be visualized.
In most cases, the download only includes a raster file with the extent of cropland in .tiff format. Nonetheless, the download from the Data Pool server also includes a metadata file and a preview image of the product. 

## Downloads ##


## Practical considerations ##
The global map obtained after merging the 7 GFSAD30 datasets can be consulted online at the project's website. 

## Practical considerations ##
Although not directly available for download, access to the original map at a spatial resolution of 250m is possible on request to the members of the ASAP Team. 6 Previous versions of the dataset for Africa developed by 
6 https://mars.jrc.ec.europa.eu/asap/about.php.
1 Global Urban Land

## Product ##


## LULC thematic ##
Dates 

## Project ##
Global Urban Land, also referred to as Multi-temporal Global Impervious Surface (MGIS), is a project developed by researchers from different Chinese universities (Sun Yat-sen, East China Normal, Guangzhou and Jiangsu Normal) to create a high-resolution multi-temporal urban land dataset. They aimed to provide high-resolution data about urban areas at multiple dates, which could be useful for those studying urbanization and the impact of artificial surfaces and human activities on the environment. In this dataset, urban land is understood as an impervious surface. It can therefore be assimilated to all the datasets mapping artificial or impervious surfaces, such as GAIA. Initially, the dataset was produced for the period 1990-2010, with maps every 5 years. However, it has since been updated, with new data for the years 1980 and 2015.

## Production method ##
Global Urban Land is obtained through an index-based method that automatically predicts urban land: the Normalized Urban Areas Composite Index (NUACI). The index, implemented through the Google Earth Engine (GEE) platform, uses Landsat imagery and DMSP-OLS nighttime lights images as inputs.
To calibrate the index, the world was stratified into different urban ecoregion categories, according to the particular physical and socioeconomic characteristics of each urban region. Three indexes (NDWI, NDVI and NDBI) were extracted from Landsat imagery to calculate the NUACI. In addition, a binary mask was obtained by segmenting DMSP-OLS nighttime lights images into urban and non-urban by applying a specific threshold. On the basis of these data, the NUACI index was calculated, obtaining a raster showing the percentage of impervious surface area per pixel.
The final Global Urban Land dataset was obtained after applying region-specific segmentation thresholds to the NUACI images showing the degree of imperviousness. After this step, a binary urban/non-urban map was generated.
For the calibration of the NUACI index, as well as for the application of segmentation thresholds, cities were randomly assigned to three equal-sized groups: centroid sites, threshold sites and testing sites. Different criteria for index calibration and threshold segmentation were decided for each type of site.

## Product description ##
The Global Urban Land dataset can be downloaded from three different servers: Baidu Drive, Google Drive and FTP. From them, users will be able to separately download the dataset for each of the available years of reference. For each year, there is a compressed folder (.zip) containing the whole dataset distributed in tiles.
An auxiliary vector file (.shp) is provided to help users identify the number of the files corresponding to their area of interest (field "grid_id"). The scientific paper presenting the dataset is also available for download, together with a text file with relevant technical information about the product and the reference data used to produce the dataset for the initial period 1990-2010.

## Downloads ##
Global Urban Land 2010 -Raster files with the extent of the artificial surfaces for each tile into which the dataset was divided (.tiff)

## Project ##
The dataset on Global Urban Expansion is the result of the work carried out by a group of researchers from the Beijing Normal University, the China University of Geosciences and Murray State University in the USA. Their aim was to create a new dataset on urban expansion using fully convolutional network (FCN)-based methods, which would be able to overcome some of the limitations of previous datasets on the same topic: outdated datasets, low spatial resolutions and low levels of accuracy.
The dataset provides useful information for studies addressing global urbanization and its impacts on the environment. It considers as urban all those built-up areas where human-constructed or artificial elements cover more than half of the area or pixel.

## Production method ##
A specific fully convolutional network (FCN) was developed to map the urban areas in the Global Urban Expansion dataset. FCN are deep learning structures based on convolutional neural networks (CNN) that employ pixel-to-pixel image recognition.
The FCN was fed with different sources of input data: Nighttime Light (NTL) imagery from NOAA and NPP-VIIRS, as well as Normalized Difference Vegetation Index (NDVI) and Land Surface Temperature (LST) data from MODIS. Other auxiliary data sources were also employed to obtain the Global Urban Expansion dataset: urban population statistics, Landsat imagery and the GHS and LC-CCI LUC datasets. LST data is only available for the period 2000-2016 and was not used to map the urban areas in 1992 and 1996.
The FCN was calibrated with data from MODIS Land Cover, differentiating urban from non-urban areas. The calibration provided the weights of the FCN, which were then used to obtain the final Global Urban Expansion dataset.
A post-classification stage using population density data was carried out to ensure the consistency over time of the maps obtained.

## Product description ##
The dataset can be downloaded as a single compressed file (. zip), including the raster files showing the urban expansion for each available year. No auxiliary information is provided with the dataset.

## Downloads ##


## Global Urban Expansion ##
-Raster files with urban expansion data for each mapped year (.tiff)

## Legend and codification ##
Other complementary products are also available for download: a layer of standard error for the production of the GMIS dataset and an HBASE probability layer. 

## Practical considerations ##
Users can explore the different datasets available online, 4 including the complementary layer about the standard error of the Impervious Surface Percentage raster and the HBASE probability layer. Full metadata for GMIS and HBASE is also available online. 5  GMIS and HBASE have some limitations associated with their production methodology. For example, they may present areas of missing information due to cloud cover or other factors. The technical documents for the product (cited below) provide a detailed description of all these limitations.
As part of the same project, Landsat imagery composites for 66 urban areas are also available for download. 6 4 https://sedac.ciesin.columbia.edu/mapping/gmis-hbase/explore-view/. 5 https://sedac.ciesin.columbia.edu/data/set/ulandsat-hbase-v1/metadata .https://sedac.ciesin.columbia.edu/data/set/ulandsat-gmis-v1/metadata. 6 https://sedac.ciesin.columbia.edu/data/set/ulandsat-cities-from-space. 

## Practical considerations ##
WSF is considered by the authors to be the most accurate dataset of its type. It is part of the U-TEP tool, which also distributes many other datasets for characterizing urban areas that may be of interest to users. Users can access an online visualization of the dataset on the U-TEP tool website. 

## GISM-Global Impervious Surface Map ##


## Overall accuracy ##
Expected to be >95% 

## Practical considerations ##
A full characterization of the dataset is provided in the technical report published by the European Commission and in the technical documentation cited above. The map comes with several limitations: a few seasonal monsoon forests in Sulawesi, New Guinea and Philippines were not mapped as an individual category, while degraded forest cover and mature stages of forest regrowth were sometimes mapped as forest. Water and Wetness HRL

## Congo Basin Monitoring Maps ##
The Water and Wetness HRL is made up of a main product mapping the different types of water and wetness covers in Europe. Users can also download an additional layer (Expert products) showing the probability of each pixel being water or wetness. Two extra technical layers are also available as expert products: one informs about the confidence of the 2018 status map (WACL) while the other studies the differences in the mapping of water and wetness covers between 2015 and 2018 (WAWCSL).

## Project ##
The European Settlement Map (ESM) is part of the Global Human Settlement Layer (GHSL) project, supported by the European Commission through the Joint Research Centre (JRC) and the Directorate General for Regional and Urban Policy (DG REGIO). ESM complements the GHSL global products by providing an urban settlement map for Europe at a very detailed spatial resolution: 2-2.5 m versus 30 m for the GHSL. Both products share similar automatic methods for extracting LUC information from satellite imagery. ESM was initially released in 2014, with successive updates in 2016, 2017 and 2019. In 2014, a dataset was created for the reference year 2012, showing the percentage of the surface area that was built up. This was revised with a new production methodology in 2016 and again in 2017. The first update improved the accuracy of the product and its consistency with population data. The spatial resolution was also improved: from 100 to 10 m. The second update increased the spatial and thematic detail of the product, at 2.5 m and differentiating between 12 classes. A new dataset at 2 m for the year 2015 was released in 2019, using a different production methodology. Unlike previous editions, this map only shows the extent of built-up areas, without providing further information about the built-up fraction per pixel.
In addition to the base layer delineating built-up areas, the latest edition of the product (2019) includes a classification differentiating residential from non-residential areas at a spatial resolution of 10 m.

## Production method ##
The ESM production method has changed over time, although it has always been fully automatic. The latest edition (2019) was produced at 2 m on the basis of the Copernicus VHR_IMAGE_2015 imagery dataset, made up of images captured by the satellites Pleiades, Deimos-02, WorldView-2, WorldView-3, GeoEye-01 and Spot 6/7. The imagery was classified through a scene-based classification algorithm: Symbolic Machine Learning (SML).
The first three editions of ESM were obtained at 100, 10 and 2.5 m through a textural and morphological technique of unsupervised built-up area detection. Spot 6/7 imagery was used as an input. In the third edition (2017), auxiliary data sources (Open Street Map, Urban Atlas…) were also used to provide more thematic detail, distinguishing between 13 LUC categories, instead of just between built-up and non-built-up areas.

## Product description ##
The ESM for each of the available editions can be downloaded separately as a single file. If more than one spatial resolution is available, users must separately download the specific product for the spatial resolution they require.
The ESM layers at 100 m for the 2014 and 2016 editions are distributed as a single European file. For the 2017 edition of ESM at 100 m, users must download a different file covering the entire mapped area for each of the categories (13 in total). The 2016 edition at 10 m is also distributed in 400 Â 400 km tiles. Finally, the ESM layers at 2-2.5 and 10 m are distributed in 100 Â 100 km tiles for the 2017 and 2019 editions of the product. In all cases, users can find out which tile or tiles fall within their area of interest by consulting the viewer available on the ESM website.

## Downloads ##
Due to the complexity of this product, with different editions available for the same years of reference at different spatial resolutions, in the following table we present an overview of all the available maps, classified according to the year they were released, their spatial resolution and the year of reference, i.e. the year for which they map the LUC covers. The different files available for download are described below the table. 

================================================================================
REFERENCES:
================================================================================
1. Photogrammetry/Aerial Photography
   Authors: 
   Date: 2009

2. What is land cover? Environ Plan B Plan Des
   Authors: , , 
   Date: 2005

3. The separation of land cover from land use using data primitives
   Authors: 
   Date: 2008

4. Global land cover mapping: A review and uncertainty analysis
   Authors: , , 
   Date: 2014

5. The impact of class resolution in land use change models
   Authors: 
   Date: 2009

6. Introduction to agent-based modelling
   Authors: 
   Date: 2000. 2012

7. Effects of a century of land cover and climate change on the hydrology of the Puget Sound basin
   Authors: , , , 
   Date: 2009

8. Spatial Data Quality: Concepts
   Authors: , 
   Date: 2006

9. Land Cover Classification System (LCCS): Classification Concepts and User Manual
   Authors: , , , , , 
   Date: 1998. 2003

10. Land Cover and Land Use Indicators: Review of available data
   Authors: , 
   Date: 2016

11. A Short Presentation of CA_MAR-KOV
   Authors: , 
   Date: 2018

12. Introduction to Satellite Remote Sensing
   Authors: , 
   Date: 2017

13. LUCC Scenarios
   Authors: , , 
   Date: 2018

14. Key issues in making and using satellite-based maps in ecology: A primer
   Authors: , , 
   Date: 2006

15. Corine land cover change detection in Europe (case studies of the Netherlands and Slovakia)
   Authors: , , , 
   Date: 2007. 2006

16. Reducing uncertainties in land cover change models using sensitivity analysis
   Authors: , , 
   Date: 2017

17. Land Use and land cover: Contradiction or Complement
   Authors: , 
   Date: 2005

18. Usability of VGI for validation of land cover maps
   Authors: , , 
   Date: 2015

19. Mapping global cropland and field size
   Authors: , , 
   Date: 2015

20. The land cover map of Great Britain: an automated classification of Landsat TM data
   Authors: , , 
   Date: 1994

21. 
   Authors: 
   Date: 2014. 2018

22. The influence of scale in LULC modelling. A comparison between two different LULC maps (SIOSE and CORINE)
   Authors: 
   Date: 2018

23. Changes in the methodology used in the production of the Spanish CORINE: Uncertainty analysis of the new maps
   Authors: , , 
   Date: 2017

24. Sensitivity of a common Land Use Cover Change (LUCC) model to the Minimum Mapping Unit (MMU) and Minimum Mapping Width (MMW) of input maps
   Authors: , , , 
   Date: 2019

25. Uncertainty Challenge in Geospatial Analysis: An Approximation from the Land Use Cover Change Modelling Perspective
   Authors: , , , , 
   Date: 2019

26. Socializing the Pixel" and Pixelizing the Social" in Land-Use and Land-Cover Change
   Authors: 
   Date: 1998

27. Remote sensing of land use and land cover: Principles and applications Giri CP (2016b) Brief overview of remote sensing of land cover
   Authors: 
   Date: 2016

28. A new research paradigm for global land cover mapping
   Authors: , , 
   Date: 2016

29. Linking Disciplines across Space and Time: Useful Concepts and Approaches for Land-Cover Change Studies
   Authors: , , 
   Date: 2005

30. An overview of 21 global and 43 regional land-cover mapping products
   Authors: , , 
   Date: 2015

31. Cellular Automata Modeling of Land-Use/Land-Cover Dynamics: Questioning the Reliability of Data Sources and Classification Methods
   Authors: , , 
   Date: 2016

32. Towards monitoring land-cover and land-use changes at a global scale: The global land survey
   Authors: , , 
   Date: 2008. 2005

33. Land Change Science. Observing, Monitoring and Understanding Trajectories of Change on the Earth's Surface
   Authors: , , , 
   Date: 2012

34. The NASA Land Cover and Land Use Change Program
   Authors: , , , 
   Date: 2012

35. ed) Remote 30 D. García-Álvarez et al. Sensing of Land Use and Land Cover. Principles and Applications
   Authors: , , 
   Date: 2012

36. Towards an integrated global land cover monitoring and mapping system
   Authors: , , , 
   Date: 2016

37. Participatory land use modelling, pathways to an integrated approach
   Authors: , , 
   Date: 2014

38. Evaluating the spatial uncertainty of future land abandonment in a mountain valley
   Authors: , , 
   Date: 2015

39. Impact of land use and land cover changes on ecosystem services in Menglun
   Authors: , , 
   Date: 2008

40. Occurrence and Distribution of Anopheles (Diptera: Culicidae) Larval Habitats on Land Cover Change Sites in Urban Kisumu and Urban Malindi, Kenya
   Authors: , , 
   Date: 2003

41. Research Directions in Land-Cover and Land-Use Change
   Authors: 
   Date: 2012

42. Evaluation of measurement data -Guide to the expression of uncertainty in measurement
   Authors: 
   Date: 2008

43. Quantitative multiresolution characterization of landscape patterns for assessing the status of ecosystem health in watershed management areas
   Authors: , 
   Date: 1998

44. A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: General guidelines for practitioners and future research
   Authors: , , 
   Date: 2016

45. Evaluating drivers of land-use change and transition potential models in a complex landscape in Southern Mexico
   Authors: , , 
   Date: 2013

46. The effects of habitat resolution on models of avian diversity and distributions: A comparison of two land-cover classifications
   Authors: , , , , 
   Date: 2016. 2004

47. Simulating the biogeochemical and biogeophysical impacts of transient land cover change and wood harvest in the Community Climate System Model (CCSM4) from 1850 to 2100
   Authors: , , 
   Date: 2012

48. A conceptual framework for uncertainty investigation in map-based land cover change modelling
   Authors: , , 
   Date: 2005

49. Uncertainty analysis in ecological studies: an overview
   Authors: , 
   Date: 2006

50. Exploring Spatial Scale in Geography
   Authors: , , , , , , , 
   Date: 2014. 2011. 2016

51. Change detection techniques
   Authors: , , , 
   Date: 2004

52. Land Use and Land Cover Mapping in Europe. Practices & Trends
   Authors: , 
   Date: 2014

53. The impact of anthropogenic land-cover change on the Florida Peninsula Sea Breezes and warm season sensible weather
   Authors: , , , 
   Date: 2004

54. Inductive pattern-based land use/cover change models: A comparison of four software packages
   Authors: , , 
   Date: 2014

55. A Suite of Tools for Assessing Thematic Map Accuracy
   Authors: , , , 
   Date: 2014

56. LUCC modeling approaches to calibration
   Authors: , , , 
   Date: 2018

57. Global Land Cover Mapping: Current Status and Future Trends
   Authors: , , , 
   Date: 2014

58. A brief history of remote sensing applications, with emphasis on Landsat
   Authors: 
   Date: 1998

59. Trends in Remote Sensing Accuracy Assessment Approaches in the Context of Natural Resources
   Authors: , , , 
   Date: 2019

60. The Development of the International Land-Use and Land-Cover Change (LUCC) Research Program and Its Links to NASA's Land-Cover and Land-Use Change (LCLUC) Initiative
   Authors: , , 
   Date: 2012

61. Current and future challenges in land-use science
   Authors: , 
   Date: 2014

62. Advancing Land Change Modeling: Opportunities and Research Requirements
   Authors: 
   Date: 2012. 2014

63. Thematic resolution in conservation monitoring -Assessment of the impact of classification detail on landscape analysis using the example of a biosphere reserve
   Authors: , 
   Date: 2016

64. Spatial Simulation: Exploring Pattern and Process
   Authors: , 
   Date: 2013

65. Making better use of accuracy data in land change studies: Estimating accuracy and area and quantifying uncertainty using stratified
   Authors: , , , 
   Date: 2013

66. Land Use Cover Mapping, Modelling and Validation. A Background estimation

67. Policy Relevant Modelling: Relationships Between Water, Land Use, and Farmer Decision Processes
   Authors: , , 
   Date: 2002

68. Advances in geomatic simulations for environmental dynamics
   Authors: , , 
   Date: 2008

69. 
   Authors: , 
   Date: 2010

70. Useful techniques of validation for spatially explicit land-change models
   Authors: , , 
   Date: 2004

71. Comparison of the structure and accuracy of two land change models
   Authors: , 
   Date: 2005

72. Hotspots of uncertainty in land-use and land-cover change projections: a global-scale model comparison
   Authors: , , 
   Date: 2016

73. Developing a science of land change: challenges and methodological issues
   Authors: , , 
   Date: 2004

74. Quelles sources cartographiques pour la définition des usages anciens du sol en France ? Rev For Française
   Authors: , , 
   Date: 2017

75. Simulation of land use spatial pattern of towns and villages based on CA-Markov model
   Authors: , , 
   Date: 2011

76. Markov Land Cover Change Modeling Using Pairs of Time-Series Satellite Images
   Authors: , 
   Date: 2013

77. Modelling conservation in the Amazon basin
   Authors: , , 
   Date: 2006

78. Role of Remote Sensing for Land-Use and Land-Cover Change Modeling
   Authors: , 
   Date: 2012

79. Divergent projections of future land use in the United States arising from different models and scenarios
   Authors: , , 
   Date: 2016

80. Producing global land cover maps consistent over time to respond the needs of the climate modelling community. 2011 6th Int Work Anal Multi-Temporal Remote Sens Images, Multi-Temp 2011 -Proc
   Authors: , , 
   Date: 2011

81. Key issues in rigorous accuracy assessment of land cover products
   Authors: , 
   Date: 2019

82. Use of air photographs for interpreting and mapping rural land use in the United States
   Authors: 
   Date: 1965

83. Global Land Cover Validation: Recommendations for Evaluation and Accuracy Assessment of Global Land Cover Maps Strand GH (2013) The Norwegian area frame survey of land cover and outfield land resources
   Authors: , , 
   Date: 2006

84. Addressing the need for improved land cover map products for policy support
   Authors: , , 
   Date: 2020

85. Intensity analysis to unify measurements of size and stationarity of land changes by interval, category and transition
   Authors: , , 
   Date: 2013. 2012

86. Land-use classification schemes used in selected recent geographic applications of remote sensing
   Authors: 
   Date: 1971

87. Geographic information systems for geoscientists
   Authors: 
   Date: 1994

88. Measuring landscapes: a planner's handbook
   Authors: , , , , 
   Date: 2006

89. Interest in intermediate soft-classified maps in land change model validation: suitability versus transition potential
   Authors: , , , , , 
   Date: 1996. 1996. 2013

90. Comparison of simulation models in terms of quantity and allocation of land change
   Authors: , , , , 
   Date: 2015. 2015

91. Geomatic approaches for modeling land change scenarios
   Authors: , , , 
   Date: 2018

92. Publisher Name Springer
   Authors: 

93. Geographically weighted methods for estimating local surfaces of overall, user and producer accuracies
   Authors: 
   Date: 2013

94. Forest resources assessment 1990. Global synthesis. FAO
   Authors: , , 
   Date: 2005. 1995. 2002

95. Harshness in image classification accuracy assessment
   Authors: 
   Date: 2008

96. Changes in the methodology used in the production of the Spanish CORINE: Uncertainty analysis of the new maps
   Authors: 
   Date: 1995. 2017

97. Kappa statistic is not satisfactory for assessing the extent of agreement between raters
   Authors: 
   Date: 2002

98. Optical remotely sensed time series data for land cover classification: A review
   Authors: , , 
   Date: 2016

99. Assessing MODIS land cover products over China with probability of interannual change
   Authors: , , 
   Date: 2014

100. Mapcurves: a quantitative method for comparing categorical maps
   Authors: , , 
   Date: 2006

101. Assessing inconsistency in global land cover products and synthesis of studies on land use and land cover dynamics during 2001 to 2017 in the southeastern region of Bangladesh
   Authors: , , , 
   Date: 2019

102. Reliability assessment for remote sensing data: beyond Cohen's kappa
   Authors: , , 
   Date: 2015

103. Revealing uncertainties in land change modeling using probabilities
   Authors: , 
   Date: 2016

104. Making better use of accuracy data in land change studies: estimating accuracy and area and quantifying uncertainty using stratified estimation
   Authors: , , , 
   Date: 2013. 2013

105. Good practices for estimating area and assessing accuracy of land change
   Authors: , , 
   Date: 2014

106. Benchmarking of LUCC modelling tools by various validation techniques and error analysis
   Authors: , , , , 
   Date: 2014. 2014

107. eBook Packages Earth and Environmental Science Pérez-Hoyos A, Udías A, Rembold F (2020) Integrating multiple land cover maps through a multi-criteria analysis to improve agricultural monitoring in Africa
   Authors: , , , 
   Date: 2018

108. Comparing two approaches to land use/cover change modeling and their
   Authors: , , 
   Date: 2012

109. Collect earth: land use and land cover assessment through augmented visual interpretation
   Authors: , , , 
   Date: 2016

110. A land cover map of Latin America and the Caribbean in the framework of the SERENA project
   Authors: , , , 
   Date: 2013

111. Mapping the land: aerial imagery for land use information
   Authors: 
   Date: 1983

112. Stable classification with limited sample: Transferring a 30-m resolution sample set collected in 2015 to mapping 10-m resolution global land cover in 2017
   Authors: , , 
   Date: 2019. 2009

113. Global land cover classification at 8 km spatial resolution: the use of training data derived from Landsat imagery in decision tree classifiers
   Authors: , , , 
   Date: 1995

114. NDVI-derived land cover classifications at a global scale
   Authors: , 
   Date: 1994

115. Land Cover Classification System (LCCS): classification concepts and user manual
   Authors: , , , 
   Date: 1998. 2017

116. Global land cover mapping from MODIS: algorithms and early results
   Authors: , , 
   Date: 2002

117. A global dataset of crowdsourced land cover and land use reference data
   Authors: , , 
   Date: 2017

118. A high-resolution and harmonized model approach for reconstructing and analysing historic land changes in Europe
   Authors: , , , 
   Date: 2013

119. Global land cover mapping and characterization: Present situation and future research priorities
   Authors: 
   Date: 2005

120. Land cover characterization and mapping of South America for the year 2010 using landsat 30 m satellite data
   Authors: , 
   Date: 2014

121. An overview of 21 global and 43 regional land-cover mapping products
   Authors: , , 
   Date: 2015

122. User guide for the MEaSURES Vegetation continuous fields product
   Authors: , , 
   Date: 2017

123. Development of 500 meter vegetation continuous field maps using MODIS data
   Authors: , , 
   Date: 2003

124. Global land cover classification at 1 km spatial resolution using a classification tree approach
   Authors: , , , 
   Date: 2000

125. A review of large area monitoring of land cover change using Landsat data
   Authors: , 
   Date: 2012

126. AntarcticaLC2000: the new Antarctic land cover database for the year 2000
   Authors: , , 
   Date: 2017

127. A global forest growing stock, biomass and carbon map based on FAO statistics
   Authors: , , , 
   Date: 2011. 2008

128. Global land cover SHARE (GLC-SHARE) database beta-release version 1
   Authors: , , , 
   Date: 2014

129. Annual dynamics of global land cover and its long-term changes from 1982 to 2015
   Authors: , , 
   Date: 2020

130. The global forest/non-forest map from TanDEM-X interferometric SAR data
   Authors: , , 
   Date: 2018

131. Global vegetation and land use: new high-resolution data bases for climate studies
   Authors: 
   Date: 1983

132. Carbon in live vegetation of major world ecosystems
   Authors: , , 
   Date: 1983

133. Estimating historical changes in global land cover: croplands from 1700 to 1992
   Authors: , 
   Date: 1999

134. A global reference database of crowdsourced cropland data collected using the Geo-Wiki platform
   Authors: , , 
   Date: 2017

135. Open land cover from OpenStreetMap and remote sensing
   Authors: , , 
   Date: 2017

136. Building a hybrid land cover map with crowdsourcing and geographically weighted regression
   Authors: , , 
   Date: 2015

137. Global, 30-m resolution continuous fields of tree cover: landsat-based rescaling of MODIS vegetation continuous fields with lidar-based estimates of error
   Authors: , , 
   Date: 2013

138. New global forest/non-forest maps from ALOS PALSAR data
   Authors: , , 
   Date: 2014. 2007-2010

139. Hierarchical mapping of annual global land cover 2001 to present: the MODIS collection 6 land cover product
   Authors: , , , 
   Date: 2019

140. Carbon stock and density of northern boreal and temperate forests
   Authors: , , 
   Date: 2014

141. A global 1-km consensus land-cover product for biodiversity and ecosystem modelling
   Authors: , 
   Date: 2014

142. A global archive of land cover and soils data for use in general circulation climate models
   Authors: , , 
   Date: 1985

143. Meta-discoveries from a synthesis of satellite-based land-cover mapping research
   Authors: , , 
   Date: 2014

144. References Conrad
   Date: 2007, 2015

145. 
   Authors: 
   Date: 2003-2020

146. 
   Authors: 
   Date: 2016

147. 
   Authors: , 

148. Evaluation of an integrated land use change model including a scenario analysis of land use change for continental Africa
   Authors: , , 
   Date: 2011

149. Semi-automatic classification plugin documentation
   Authors: 
   Date: 2016

150. SAGA-GIS module library documentation Costanza R (1989) Model goodness of fit: a multiple resolution procedure
   Authors: , , 
   Date: 2007, 2015. 2005

151. An improved fuzzy Kappa statistic that accounts for spatial autocorrelation
   Authors: 
   Date: 2009

152. A method and application of multi-scale validation in spatial land use models
   Authors: , , , 
   Date: 2001

153. Bayesian belief networks as a versatile method for assessing uncertainty in land-change modeling
   Authors: , 
   Date: 2015

154. Inductive pattern-based land use/cover change models: a comparison of four software packages
   Authors: , , 
   Date: 2014

155. Benchmarking of LUCC modelling tools by various validation techniques and error analysis
   Authors: , , , , 
   Date: 2014

156. A generalized cross-tabulation matrix to compare soft-classified maps at multiple resolutions
   Authors: , , 
   Date: 2006

157. Pontius matrix
   Authors: , 
   Date: 2018

158. Army Construction Engineering Research Laboratory © 2003-2020 GRASS Development Team, GRASS GIS 7.8.3dev Reference Manual References Bonham-Carter GF
   Authors: , 
   Date: 1994

159. Interest in intermediate soft-classified maps in land change model validation: suitability versus transition potential
   Authors: , , , 
   Date: 2013

160. 
   Authors: 
   Date: 2021

161. Transition potential modelling for land cover change
   Authors: , , 
   Date: 2005

162. Revealing uncertainties in land change modeling using probabilities
   Authors: , 
   Date: 2016

163. Measuring landscapes: a planner's handbook
   Authors: , , , , 
   Date: 2006

164. Landscapemetrics: an open-source R tool to calculate landscape metrics
   Authors: , , , , , 
   Date: 1995. 2019

165. The role of spatial metrics in the analysis and modeling of urban land use change
   Authors: , , 
   Date: 2005

166. Landscape division, splitting index, and effective mesh size: New measures of landscape fragmentation
   Authors: 
   Date: 2000

167. LecoS-a python plugin for automated landscape ecology analysis
   Authors: 
   Date: 2016

168. FRAGSTATS: spatial pattern analysis program for categorical and continuous maps Mcgarigal K (2018) Landscape metrics for Categorical Map Patterns
   Authors: , , , 
   Date: 2015

169. Landscape indices behavior: a review of scale effects
   Authors: , 
   Date: 2012

170. Map comparison methods that simultaneously address overlap and structure
   Authors: , 
   Date: 2006

171. Benchmarking of LUCC modelling tools by various validation techniques and error analysis
   Authors: , , , , , , , 
   Date: 2006. 2014

172. Global Land Cover mapping for the year 2000
   Authors: , , , 
   Date: 2002. 2000. 2002

173. GLC2000: a new approach to global land cover mapping from earth observation data
   Authors: , 
   Date: 2005

174. The IGBP-DIS global 1-km land-cover data set DISCover: a project overview
   Authors: , , 
   Date: 1999

175. The global land-cover characteristics database: the users' perspective
   Authors: , , , 
   Date: 1999

176. NDVI-derived land cover classifications at a global scale
   Authors: , 
   Date: 1994

177. Global land cover classification at 8 km spatial resolution: the use of training data derived from Landsat imagery in decision tree classifiers
   Authors: , , , 
   Date: 1995

178. A land cover map of South America
   Authors: , , , 
   Date: 2004

179. Assessing the applicability of Open-StreetMap data to assist the validation of land use/land cover maps
   Authors: , 
   Date: 2017

180. Generating up-to-date and detailed land use and land cover maps using OpenStreetMap and GlobeLand30
   Authors: , , 
   Date: 2017

181. Using Open-StreetMap to create land use and land cover maps: development of an application
   Authors: , , 
   Date: 2017

182. Harmonisation, mosaicing and production of the Global Land Cover 2000 database (Beta Version)
   Authors: , , 
   Date: 2003. September 24, 2020

183. Geo-Wiki: An online platform for improving global land cover
   Authors: , , 
   Date: 2012

184. Global land cover classification at 1 km spatial resolution using a classification tree approach
   Authors: , , , 
   Date: 2000

185. A comparison of the IGBP DISCover and University of Maryland 1 km global land cover products
   Authors: , 
   Date: 2000

186. Spatial consistency assessments for global land-cover datasets: a comparison among GLC2000, CCI LC, MCD12. GLOBCOVER and GLCNMO
   Authors: , , 
   Date: 2018

187. Global Land Cover SHARE (GLC-SHARE) database Beta-Release Version 1
   Authors: , , , 
   Date: 2014. August 2020

188. The IGBP-DIS global 1 km land cover data set, discover: first results
   Authors: , 
   Date: 1997

189. Development of a global land cover characteristics database and IGBP DISCover from 1-km AVHRR Data
   Authors: , , , , , , 
   Date: 2000

190. A spatial comparison of four satellite derived 1 km global land cover datasets
   Authors: , , , 
   Date: 2006. 2005

191. Comparative assessment of CORINE2000 and GLC2000: spatial analysis of land cover data for Europe
   Authors: , , , 
   Date: 2007

192. Conventional and fuzzy comparisons of large scale land cover products: application to CORINE, GLC2000, MODIS and GlobCover in Europe
   Authors: , , , 
   Date: 2012

193. Development of a global land cover characteristics database and IGBP DISCover from 1-km AVHRR Data
   Authors: , , 
   Date: 2000

194. Open land cover from OpenStreetMap and remote sensing
   Authors: , , 
   Date: 2017

195. Building a hybrid land cover map with crowdsourcing and geographically weighted regression
   Authors: , , 
   Date: 2015

196. Comparison and relative quality assessment of the GLC2000, GLOBCOVER, MODIS and ECOCLIMAP land cover data sets at the African continental scale
   Authors: , , , 
   Date: 2011

197. The value of OpenStreetMap historical contributions as a source of sampling data for multi-temporal land use/cover maps
   Authors: , , 
   Date: 2019

198. 

199. GLOBCOVER. Products description and validation report
   Authors: , , , 
   Date: 2008

200. Revisiting land cover observation to address the needs of the climate modeling community
   Authors: , , 
   Date: 2012

201. GLOBCOVER 2009 products description and validation report
   Authors: , , 
   Date: 2011. 20 Aug 2020

202. Copernicus global land cover layers-collection 2
   Authors: 
   Date: 2020. 2020

203. Copernicus global land service: land cover 100 m: version 3 globe 2015-2019: algorithm theoretical basis document
   Authors: , , 
   Date: 2020. 28 Dec 2020. 2020. 28 Dec 2020

204. Preliminary analysis of spatiotemporal pattern of global land surface water
   Authors: , , 
   Date: 2014

205. Consistency of accuracy assessment indices for soft classification: simulation analysis
   Authors: , , , 
   Date: 2010

206. Change vector analysis in posterior probability space: a new method for land cover change detection
   Authors: , , , 
   Date: 2011

207. A simple and effective method for filling gaps in Landsat ETM+ SLC-off images
   Authors: , , 
   Date: 2011b

208. An automated approach for updating land cover maps based on integrated change detection and classification methods
   Authors: , , , 
   Date: 2012

209. Temporal logic and operation relations based knowledge representation for land cover change web services
   Authors: , , 
   Date: 2013. 2013

210. Global land cover mapping at 30 m resolution: a POK-based operational approach
   Authors: , , 
   Date: 2014. 2014

211. A landscape shape index-based sampling approach for land cover accuracy assessment
   Authors: , , 
   Date: 2016

212. Towards a collaborative global land cover information service
   Authors: , , , 
   Date: 2017

213. Stable classification with limited sample: transferring a 30 m resolution sample set collected in 2015 to mapping 10 m resolution global land cover in 2017
   Authors: , , 
   Date: 2019

214. GlobCover and GlobCorine experiences
   Authors: , , 
   Date: 2010. 2015. 2016. 2017. 2018. 2019. 2010

215. Land cover CCI. Product user guide. Version 2.0
   Authors: 
   Date: 2017. 19 Aug 2020

216. Global land cover mapping from MODIS: algorithms and early results
   Authors: , , 
   Date: 2002

217. MODIS collection 5 global land cover: algorithm refinements and characterization of new datasets
   Authors: , , 
   Date: 2010

218. User guide to collection 6 MODIS land cover (MCD12Q1 and MCD12C1) product
   Authors: , 
   Date: 2019. Sept 2020

219. Comparison of land cover maps using fuzzy agreement
   Authors: , 
   Date: 2005

220. A comparative analysis of the global land cover 2000 and MODIS land cover data sets
   Authors: , , 
   Date: 2005

221. Finer resolution observation and monitoring of global land cover: first mapping results with Landsat TM and ETM+ data
   Authors: , , 
   Date: 2013

222. A web-based system for supporting global land cover data production
   Authors: , , 
   Date: 2015

223. Assessing MODIS land cover products over China with probability of interannual change
   Authors: , , 
   Date: 2014

224. The ESA climate change initiative: satellite data records for essential climate variables
   Authors: , , 
   Date: 2013

225. Spatial consistency assessments for global land-cover datasets: a comparison among GLC2000, CCI LC, MCD12, GLOBCOVER and GLCNMO
   Authors: , , 
   Date: 2018

226. Improving the accuracy of the water surface cover type in the 30 m FROM-GLC product
   Authors: , , , 
   Date: 2015

227. Open access to earth land-cover map
   Authors: , , 
   Date: 2014

228. Production of global land cover data-GLCNMO2013
   Authors: , , 
   Date: 2017

229. Annual dynamics of global land cover and its long-term changes from 1982 to 2015
   Authors: , , 
   Date: 2020

230. Comparison of global and continental land cover products for selected study areas in South Central and Eastern European Region
   Authors: , , 
   Date: 2018

231. Insights on the historical and emerging global land cover changes: the case of ESA-CCI-LC datasets
   Authors: , 
   Date: 2019. 2019

232. Climate effects of the GlobeLand30 land cover dataset on the Beijing climate center climate model simulations
   Authors: , , , 
   Date: 2016

233. Application and impacts of the GlobeLand30 land cover dataset on the Beijing climate center climate model
   Authors: , , , 
   Date: 2016b

234. Hierarchical mapping of annual global land cover 2001 to present: the MODIS collection 6 land cover product
   Authors: , , , 
   Date: 2019

235. Practice and thoughts of the automatic processing of multispectral images with 30 m spatial resolution on the global scale
   Authors: , , 
   Date: 2014

236. Production of global land cover data-GLCNMO
   Authors: , , , 
   Date: 2011

237. Production of global land cover data-GLCNMO2008
   Authors: , , 
   Date: 2014

238. Comparison and relative quality assessment of the GLC2000, GLOBCOVER, MODIS and ECOCLIMAP land cover data sets at the African continental scale
   Authors: , , , 
   Date: 2011

239. Copernicus global land service: land cover 100 m: version 2: validation report
   Authors: , , 
   Date: 2019. 28 Dec 2020

240. Comparative analysis of CORINE and climate change initiative land cover maps in Europe: implications for wildfire occurrence estimation at regional and local scales
   Authors: , , 
   Date: 2020. 28 Dec 2020. 2019. 2019

241. Mapping global land cover in 2001 and 2010 with spatial-temporal consistency at 250 m resolution
   Authors: , , 
   Date: 2015

242. Pragmatics driven land cover service composition utilizing behavior-intention model
   Authors: , , 
   Date: 2016

243. A multilevel stratified spatial sampling approach for the quality assessment of remote-sensing-derived products
   Authors: , , 
   Date: 2015

244. A scheme for the long-term monitoring of impervious-relevant land disturbances using high frequency Landsat archives and the Google Earth Engine
   Authors: , , 
   Date: 2019

245. Accuracy assessment and inter-comparison of eight medium resolution forest products on the Loess Plateau. China
   Authors: , , 
   Date: 2017

246. Improving 30 m global land-cover map FROM-GLC with time series MODIS and auxiliary data sets: a 310 D. García-Álvarez et al. segmentation-based approach
   Authors: , , 
   Date: 2013

247. An enhanced spatial and temporal adaptive reflectance fusion model for complex heterogeneous regions
   Authors: , , 
   Date: 2014. 2010

248. Accuracy and congruency of three different digital land-use maps
   Authors: , , , 
   Date: 2006

249. Integrating the MOLAND and the urban Atlas geo-databases to analyze urban growth in European cities
   Authors: , , , , 
   Date: 2014

250. Intellectual structure of CORINE land cover research applications in web of science: a Europe-wide review
   Authors: 
   Date: 2019. 2017

251. GlobCorine-a joint EEA-ESA project for operational land dynamics monitoring at pan-European scale
   Authors: , , , 
   Date: 2009

252. GlobCover 2009. Description and validation report
   Authors: , , , 
   Date: 2010. 21 Aug 2020

253. CORINE land cover technical guide: addendum 2000
   Authors: , , 
   Date: 2000. 19 Aug 2020

254. NOMENCLATURE and MAPPING GUIDELINE copernicus land monitoring service local component
   Authors: , 
   Date: 2018. 2000. Sept 2020

255. CORINE land cover and land cover change products
   Authors: 
   Date: 2014

256. Corine land cover update 2000 -technical guidelines
   Authors: , , 
   Date: 2002. 19 Aug 2020

257. Manual of CORINE land cover changes
   Authors: , 
   Date: 2011. 19 Aug 2020

258. Implementation and achievements of CLC
   Authors: , , , 
   Date: 2012. 2006. 19 Aug 2020

259. CLC2012. Addendum to CLC2006 technical guidelines
   Authors: , , 
   Date: 2014. 19 Aug 2020

260. PELCOM project: a 1-km pan-European land cover database for environmental monitoring and use in meteorological models
   Authors: , , 
   Date: 2000

261. Mapping Guide v6.1 for an European Urban Atlas
   Date: 2020. Sept 2020

262. GlobCorine. Product description manual
   Authors: , , , 
   Date: 2010. 21 Aug 2020

263. 
   Authors: , , , 
   Date: 2010b) GlobCorine 2009. 21 Aug 2020

264. 
   Authors: , , , 
   Date: 2010. 21 Aug 2020

265. Cities in Europe: the new OECD-EC definition
   Date: 2012. Sept 2020

266. corine land cover
   Date: 1994. 19 Aug 2020

267. CORINE land cover nomenclature illustrated guide
   Date: 2006. 19 Aug 2020

268. Assessment using LUCAS (land use/cover area frame statistical survey)
   Date: 2006. 2000. 19 Aug 2020

269. Land accounts for Europe
   Date: 2006. 1990-2000. 2006. 20 Aug 2020

270. CLC2006 technical guidelines
   Date: 2007. 19 Aug 2020

271. Copernicus land monitoring service-ocal component: coastal zones monitoring nomenclature guideline
   Date: 2021. March 2021

272. Determining changes and flows in European landscapes 1990-2000 using CORINE land cover data
   Authors: , , , 
   Date: 2010. 2009

273. A data-driven reconstruction of historic land cover/use change of Europe for the period 1900 to
   Authors: , , , 
   Date: 2016. 2015. 2010

274. A high-resolution and harmonized model approach for reconstructing and analysing historic land changes in Europe
   Authors: , , , 
   Date: 2013

275. Gross changes in reconstructions of historic land cover/use for Europe between 1900 and 2010
   Authors: , , 
   Date: 2015

276. The potential of old maps and encyclopaedias for reconstructing historic European land cover/use change
   Authors: , , , 
   Date: 2015

277. Urban Atlas 2012 validation report
   Authors: 
   Date: 2017. Sept 2020

278. Fine scale profile of CORINE land cover classes with LUCAS data
   Authors: 
   Date: 2001

279. Changes in the methodology used in the production of the Spanish CORINE: uncertainty analysis of the new maps
   Authors: , , 
   Date: 2017

280. a) Creation of training dataset for Sentinel-2 land cover classification
   Authors: , , 
   Date: 2019

281. Post-processing tools for land cover classification of Sentinel-2
   Authors: , , 
   Date: 2019b. 2019. 2018. 2012

282. 
   Authors: 
   Date: Sept 2020

283. CORINE land cover 2012. Final validation report
   Authors: 
   Date: 2017. 19 Aug 2020

284. Monitoring of urban fabric classes and their validation in selected European cities (Urban Atlas)
   Authors: , , 
   Date: 2016

285. Updated CLC illustrated nomenclature guidelines
   Authors: , , , 
   Date: 2019. 19 Aug 2020

286. Multitemporal Sentinel-2 data-remarks and observations
   Authors: , , 
   Date: 2017. 2017

287. 
   Authors: , , 
   Date: 2019

288. Phase2_FinalReport
   Date: Sept 2020

289. The European urban atlas
   Authors: , , 
   Date: 2014

290. PELCOM. Final report
   Authors: 
   Date: 2000. Sept 2020

291. Establishment of a 1-km pan-European land cover database for environmental monitoring
   Authors: , , , , , , 
   Date: 2000

292. Comparative assessment of CORINE2000 and GLC2000: spatial analysis of land cover data for Europe
   Authors: , , , 
   Date: 2007

293. Aggregation of Sentinel-2 time series classifications as a solution for multitemporal analysis
   Authors: , , 
   Date: 2017. 104270

294. Assessing microscale environmental changes: CORINE vs. the urban Atlas
   Authors: , 
   Date: 2015

295. Assessment of green infrastructure in Riparian zones using copernicus programme
   Authors: , , 
   Date: 2019

296. Global mapping of human settlement. experiences, datasets, and prospects
   Authors: , , 
   Date: 2011

297. Population estimation for the urban Atlas polygons
   Authors: , , , 
   Date: 2013. Sept 2020

298. Mapping population density in functional urban areas
   Authors: , 
   Date: 2016. Sept 2020

299. CLC2018 Technical guidelines
   Authors: , , 
   Date: 2017. 19 Aug 2020

300. Riparian zones nomenclature guideline
   Authors: , , , , , , 
   Date: 2018

301. Potential of Copernicus riparian layers to assess riparian zones integrity with landscape metrics
   Authors: 
   Date: 2019. June 2021

302. Riparian zones land cover/land use extension to Strahler 2 validation report
   Authors: , , 
   Date: 2018. Sept 2020

303. Europe's green arteriesa continental dataset of riparian zones
   Authors: , , 
   Date: 2016

304. Evaluating land cover changes in Eastern and Southern Africa from 2000 to 2010 using validated Landsat and MODIS data
   Authors: , , , 
   Date: 2017

305. Landscapes of West Africa-a window on a changing world
   Authors: 
   Date: 2016. September 28, 2020

306. West Africa land use and land cover time series
   Authors: 
   Date: 2017. 2017. September 28, 2020. 20173004

307. Mapping land cover through time with the Rapid Land Cover Mapper-Documentation and user manual
   Authors: , 
   Date: 2017

308. Land use, land cover and soil sciences -Volume I: Land cover, land use and the global change
   Authors: , , 
   Date: 1996. 1997. 2009

309. AFRICOVER land cover database and map of africa
   Authors: 
   Date: 1997. 2020. 1998

310. Evaluation of ESA CCI prototype land cover map at 20 m
   Authors: , , , , , , , , , 
   Date: 2017. 8 September 2020

311. FAO methodologies for land cover classification and mapping. Link People, Place
   Authors: , , 
   Date: 2002

312. Land cover mapping for green house gas inventories in Eastern and Southern Africa using landsat and high resolution imagery: approach and lessons learnt
   Authors: , , 
   Date: 2016

313. Servir: leveraging the expertise of a space agency and a development agency to increase impact of earth observation in the developing world
   Authors: , , 
   Date: 2019

314. Mapping Congo Basin vegetation types from 300 m and 1 km multi-sensor time series for carbon stocks and forest areas estimation
   Authors: , , , , 
   Date: 2012

315. Detecting change areas in Mexico between 2005 and 2010 using 250 m MODIS images
   Authors: , , , 
   Date: 2014

316. Annual land cover monitoring using 250M MODIS data for Mexico
   Authors: , , 
   Date: 2014

317. Generation and analysis of the 2005 land cover map for Mexico using 250m MODIS data
   Authors: , , , 
   Date: 2012

318. Detection of North American land cover change between 2005 and 2010 with 250m MODIS Data
   Authors: , , 
   Date: 2014c

319. MAD-MEX: automatic wall-to-wall land cover monitoring for the mexican REDD-MRV program using all landsat data
   Authors: , , 
   Date: 2014

320. The application of medium-resolution MERIS satellite data for continental land-cover mapping over South America results and caveats
   Authors: , , , , 
   Date: 2012

321. Completion of the 2011 national land cover database for the conterminous United Statesrepresenting a decade of land cover change information
   Authors: , , 
   Date: 2015

322. Overall methodology design for the United States national land cover database 2016 products
   Authors: , , 
   Date: 2019

323. A comprehensive change detection method for updating the National Land Cover Database to circa 2011
   Authors: , , , , , 
   Date: 2013

324. North American land change monitoring system
   Authors: , , 
   Date: 2012

325. Circa 2010 land cover of Canada: local optimization methodology and product development
   Authors: , , 
   Date: 2017

326. A map of the vegetation of South America based on satellite imagery
   Authors: , , , 
   Date: 1994

327. A new generation of the United States National Land Cover Database: requirements, research priorities, design, and implementation strategies
   Authors: , , 
   Date: 2018. 2018

328. Examining the PALSAR-2 Global forest/non-forest maps through Turkish afforestation practices
   Authors: , , , 
   Date: 2020

329. Evaluating MODIS-vegetation continuous field products to assess tree cover change and forest fragmentation in India-a multi-scale satellite remote sensing approach
   Authors: , , 
   Date: 2017

330. ATBD for LAI
   Authors: , , , 
   Date: 2016. 11 Feb 2021

331. TanDEM-X. Forest/Non-Forest map product description
   Authors: , , , , 
   Date: 2019. July 2021

332. Global forest resources assessment 2000. Main report
   Authors: 
   Date: 2000. 9 Feb 2021. 2001. February 9, 2021. 9 Feb 2021

333. Global forest resources assessment 2010. Rome
   Authors: 
   Date: 2010. Jan 2021

334. The FRA 2010 Remote sensing survey: an outline of objectives, data, methods and approach
   Authors: , , 
   Date: 2009. 10 Feb 2021

335. Global percent tree cover at a spatial resolution of 500 meters: first results of the modis vegetation continuous fields algorithm
   Authors: , , 
   Date: 2003a

336. Development of 500 meter vegetation continuous field maps using MODIS data
   Authors: , , 
   Date: 2003b

337. Estimation of tree cover using MODIS data at global, continental and regional/local scales
   Authors: , , , 
   Date: 2005

338. High-resolution global maps of 21st-century forest cover change
   Authors: , , 
   Date: 2013

339. Response to comment on High-resolution global maps of 21st-century forest cover change
   Authors: , , 
   Date: 2014

340. Global 25m Resolution PALSAR-2 / PALSAR Mosaic and Forest/Non-Forest Map (FNF)
   Authors: 
   Date: 2019. Sept 2020

341. Comparison of MODIS vegetation continuous field-based forest density maps with IRS-LISS III derived maps
   Authors: , , , 
   Date: 2009

342. Copernicus global land operations. Vegetation and energy (CGLOPS-1) Quality assessment report atmospheric correction for Sentinel-3 OLCI and SLSTR products (version 1.0)
   Authors: 
   Date: 2020. 11 Feb 2021

343. Exploiting synergies of global land cover products for carbon cycle modeling
   Authors: , , , 
   Date: 2006

344. Copernicus global land operations. vegetation and energy (CGLOPS-1) Product quality assurance document
   Authors: , , 
   Date: 2020. 11 Feb 2021

345. Copernicus global land operations: vegetation and energy (CGLOPS-1: scientific quality evaluation) LAI, FAPAR, FCOVER. collection 300 M (Version 1)
   Authors: , , , , , , , , , , , 
   Date: 2020. 11 Feb 2021. 2018

346. The global forest/non-forest map from TanDEM-X interferometric SAR data
   Authors: , , , , , , , , , 
   Date: 2018

347. Volume decorrelation effects in TanDEM-X interferometric SAR data
   Authors: , , 
   Date: 2016

348. Copernicus global land operations, vegetation and energy (CGLOPS-1) Algorithm theoretical basis document
   Authors: , , 
   Date: 2020. 11 Feb 2021

349. Global forest resources assessment 2010. Options and recommendations for a global remote sensing survey of forests
   Authors: 
   Date: 2007. 10 Feb 2021

350. Copernicus global land operations, vegetation and energy (CGLOPS-1) Quality assessment report. LAI, FAPAR, FCOVER, collection 300 M (version 1)
   Authors: , , 
   Date: 2018. 11 Feb 2021

351. Development of a global hybrid forest mask through the synergy of remote sensing, crowdsourcing and FAO statistics
   Authors: , , 
   Date: 2015

352. a) Earth science data records of global forest cover and change
   Authors: , , , , , , , , , , , , , 
   Date: 2016. July 2021

353. Earth science data records of global forest cover and change. Algorithm theoretical basis document
   Authors: , , , , , , , , , , , , , 
   Date: 2016b. July 2021

354. Global, 30-m resolution continuous fields of tree cover: landsat-based rescaling of MODIS vegetation continuous fields with lidar-based estimates of error
   Authors: , , 
   Date: 2013

355. New global forest/non-forest maps from ALOS PALSAR data
   Authors: , , 
   Date: 2014. 2007-2010

356. Gio global land component-Lot I operation of the global land component. Product user manual leaf area index (lai)
   Authors: , , 
   Date: 2018. 11 Feb 2021

357. Copernicus global land operations, vegetation and energy (CGLOPS-1) evaluation report of OLCI and SLSTR cloud, cloud shadow and snow detection
   Authors: , 
   Date: 2020. 11 Feb 2021

358. User guide for the MODIS vegetation continuous fields product, collection 5 (version 1)
   Authors: , , 
   Date: 2011

359. Copernicus global land operations. Vegetation and energy (CGLOPS-1) Algorithm theoretical basis document, Fraction of Absorbed Photosynthetically Active Radiation (FAPAR)
   Authors: 
   Date: 2020. 11 Feb 2021

360. Copernicus global land operations, vegetation and energy (CGLOPS-1) product user manual
   Authors: , , 
   Date: 2020

361. PUM_FCOVER300m-V1.1_I1.00.pdf
   Date: 11 Feb 2021

362. MODIS Collection 5 global land cover: Algorithm refinements and characterization of new datasets
   Authors: , , , 
   Date: 2010

363. Mapping global cropland and field size
   Authors: , , 
   Date: 2015

364. Cropland for sub-Saharan Africa: A synergistic approach using five land cover data sets
   Authors: , , 
   Date: 2011

365. Agricultural cropland extent and areas of South Asia derived using Landsat satellite 30-m time-series big-data using random forest machine learning algorithms on the Google Earth Engine cloud
   Authors: , , 
   Date: 2020

366. Intercomparison on four irrigated cropland maps in Mainland China
   Authors: , , 
   Date: 2018

367. A cultivated planet in 2010 -Part 1: The global synergy cropland map
   Authors: , , 
   Date: 2020

368. The warning classification scheme of ASAP -Anomaly hot Spots of Agricultural Production
   Authors: , , 
   Date: 2019. 20 April, 2021

369. Farming the planet: 2. Geographic distribution of crop areas, yields, physiological types, and net primary production in the year 2000
   Authors: , , 
   Date: 2008

370. Mapping cropland extent of Southeast and Northeast Asia using multi-year time-series Landsat 30-m data using a random forest classifier on the Google Earth Engine Cloud
   Authors: , , 
   Date: 2019

371. Comparison of global land cover datasets for cropland monitoring
   Authors: , , , 
   Date: 2017

372. 
   Authors: , , 
   Date: 2017. March 2017

373. Mapping croplands of Europe, Middle East, Russia, and Central Asia using Landsat, Random Forest, and Google Earth Engine
   Authors: , , 
   Date: 2020

374. Estimating global cropland extent with multi-year MODIS data
   Authors: , , 
   Date: 2010

375. MIRCA2000-Global monthly irrigated and rainfed crop areas around the year 2000: A new high-resolution data set for agricultural and hydrological modeling
   Authors: , , 
   Date: 2010

376. Farming the planet: 1. Geographic distribution of global agricultural lands in the year 2000
   Authors: , , , 
   Date: 2008

377. ASAP: A new global early warning system to detect anomaly hot spots of agricultural production for food security analysis
   Authors: , , 
   Date: 2019

378. Global rain-fed, irrigated, and paddy croplands: A new high resolution map derived from remote sensing, crop inventories and climate data
   Authors: , , , , 
   Date: 2015

379. A 30-m landsat-derived cropland extent product of Australia and China using random forest machine learning algorithm on Google Earth Engine cloud computing platform
   Authors: , , 
   Date: 2018

380. Global cropland area database (GCAD) derived from remote sensing in support of food security in the twenty-first century : current achievements and future possibilities
   Authors: , , 
   Date: 2015

381. Global food security support analysis data at nominal 1 km (GFSAD1km) derived from remote sensing in support of food security in the twenty-first century: current achievements and future possibilities
   Authors: , , 
   Date: 2020

382. Global irrigated area map (GIAM), derived from remote sensing, for the end of the last millennium
   Authors: , , , 
   Date: 2009. 2009

383. A holistic view of global croplands and their water use for ensuring global food security in the 21st century through advanced remote sensing and non-remote sensing approaches
   Authors: , , , 
   Date: 2010

384. Assessing future risks to agricultural productivity, water resources and food security: How can remote sensing help?
   Authors: , , 
   Date: 2012

385. Advances in hyperspectral remote sensing of vegetation
   Authors: , , 
   Date: 2011

386. NASA Making Earth System Data Records for Use in Research Environments (MEaSUREs) Global Food Security-support Analysis Data (GFSAD) 1 km datasets
   Authors: 
   Date: 2017. February 18, 2021

387. Harmonizing and combining existing land cover/land use datasets for cropland area monitoring at the African continental scale
   Authors: , , 
   Date: 2013

388. A unified cropland layer at 250 m for global agriculture monitoring
   Authors: , , , 
   Date: 2016

389. Nominal 30-m cropland extent map of continental africa by integrating pixel-based and object-based algorithms using sentinel-2 and landsat-8 data on google earth engine
   Authors: , , 
   Date: 2017

390. Accuracy assessment of Global Food Security-Support Analysis Data (GFSAD) cropland extent maps produced at three different spatial resolutions
   Authors: , 
   Date: 2018

391. FROM-GC: 30 m global cropland extent derived through multisource data integration
   Authors: , , 
   Date: 2013

392. a The label refers to the time when the pixel was sealed GUB GUB -Orig_FID: Unique identifier for each polygon -UrbanArea: area of the delimited urban area References
   Authors: , , 
   Date: 2020. 2018 8 2011 15 2004 22 1997 29 1990 2 2017 9 2010 16 2003 23 1996 30 1989 3 2016 10 2009 17 2002 24 1995 31 1988 4 2015 11 2008 18 2001 25 1994 32 1987 5 2014 12 2007 19 2000 26 1993 33 1986 6 2013 13 2006 20 1999 27 1992 34 1985 7 2012 14 2005 21 1998 28 1991. 2018

393. Automated global delineation of human settlements from 40 years of Landsat satellite data archives
   Authors: , , 
   Date: 2019a. 2019.1625528

394. GHSL Data Package
   Authors: , , 
   Date: 2019b. 2019. 20 Aug 2020

395. Convolutional neural networks for global human settlements mapping from Sentinel-2 satellite imagery
   Authors: , , , , , , , 
   Date: 2021

396. Documentation for the global man-made impervious surface (GMIS) Dataset from landsat
   Authors: , , 
   Date: 2017. 19 May 2021

397. Constructed area approaches the size of Ohio
   Authors: , , , , , , 
   Date: 2004

398. Global distribution and density of constructed impervious surfaces
   Authors: , , , , , , , 
   Date: 2007

399. Delineation of Urban footprints from TerraSAR-X data by analyzing speckle characteristics and intensity information
   Authors: , , , , , 
   Date: 2010

400. Characterization of land cover types in TerraSAR-X images by combined analysis of speckle statistics and intensity information
   Authors: , , , , , 
   Date: 2011

401. TanDEM-X mission-new perspectives for the inventory and monitoring of global settlement patterns
   Authors: , , , , , , , , 
   Date: 2012

402. Urban footprint processor-fully automated processing chain generating settlement masks from global data of the TanDEM-X mission
   Authors: , , , , , , , , , 
   Date: 2013

403. Dimensioning urbanization -An advanced procedure for characterizing human settlement properties and patterns using spatial network analysis
   Authors: , , , , , , , 
   Date: 2014

404. Breaking new ground in mapping human settlements from space-the global urban footprint
   Authors: , , , , , , , , 
   Date: 2017

405. a) New prospects in analysing big data from space-the urban thematic exploitation platform
   Authors: , , , , , , , , , , , , , , 
   Date: 2018

406. Digital world meets urban planet-new prospects for evidence-based urban studies arising from joint exploitation of big earth data, information technology and shared knowledge
   Authors: , , , , , , , , , , , , , , , , , , , , , , , 
   Date: 2020

407. human settlement changes in China reflected by impervious surfaces from satellite remote sensing
   Authors: , , 
   Date: 2019. 1978-2017. 2019

408. Annual maps of global artificial impervious area (GAIA) between
   Authors: , , , , , , , , , , 
   Date: 2020. 1985. 2018

409. Detecting global urban expansion over the last three decades using a fully convolutional network
   Authors: , , , , , 
   Date: 2019

410. Atlas of the Human Planet
   Date: 2020. 2019. 20 Aug 2020

411. A 30-year (1984-2013) record of annual urban dynamics of Beijing City derived from Landsat data
   Authors: , , 
   Date: 2015

412. An "exclusion-inclusion" framework for extracting human settlements in rapidly developing regions of China from Landsat images
   Authors: , 
   Date: 2016

413. Mapping global urban boundaries from the global artificial impervious area (GAIA) data
   Authors: , , , , , , , , , , , , , , , , , , , 
   Date: 2020

414. High-resolution multi-temporal mapping of global urban land using Landsat images based on the Google Earth Engine Platform
   Authors: , , , , , , , 
   Date: 2018. 2018

415. A novel method for building height estmation using TanDEM-X data
   Authors: , , , 
   Date: 2014

416. Outlining where humans live, the World Settlement Footprint
   Authors: , , , , , , , , , , , 
   Date: 2020. 2015

417. Unveiling 25 years of planetary urbanization with remote sensing: perspectives from the global human settlement layer
   Authors: , , 
   Date: 2018

418. Principles and applications of the global human settlement layer as baseline for the land use efficiency indicator-SDG 11.3.1
   Authors: , , 
   Date: 2019

419. Operating procedure for the production of the Global Human Settlement Layer from Landsat data of the epochs
   Authors: , , 
   Date: 2016. 1975. 1990, 2000, and 2014. 20 Aug 2020

420. A new method for earth observation data analytics based on symbolic machine learning
   Authors: , , 
   Date: 2016b

421. Documentation for the global human built-up and settlement extent (HBASE) Dataset from landsat
   Authors: , , 
   Date: 2017. 19 May 2021

422. Development of a global 30m impervious surface map using multisource and multitemporal remote sensing datasets with the google earth engine platform
   Authors: , , , , , , 
   Date: 2020

423. Copernicus Land Monitoring Service (2020a) Copernicus land monitoring service high resolution land cover characteristics. Lot1: imperviousness 2018, imperviousness change 2015-2018 and built
   Authors: , , , 
   Date: 2016. 2018. Sept 2020

424. Copernicus land monitoring service high resolution land cover characteristics. Grassland 2018 and grassland change 2015-2018
   Date: 2020. Sept 2020

425. Copernicus land monitoring service high resolution land cover characteristics. Lot4: water & wetness
   Date: 2020. 2018. Sept 2020

426. HRL water and wetness 2015 validation report
   Authors: , , 
   Date: 2019. Sept 2020

427. HRL small woody features validation concept
   Authors: , , 
   Date: 2018. Sept 2020

428. A new map of the European settlements by automatic classification of 2.5m resolution SPOT data
   Authors: , , 
   Date: 2014

429. The ESM green components. A dedicated focus on the production of the green in the European settlement map's workflow
   Authors: , , 
   Date: 2016. Sept 2020

430. How green are the European Cities? Exploring the green european settlement map
   Authors: , , , 
   Date: 2016. 2016. Sept 2020

431. The European settlement map
   Authors: , , 
   Date: 2017. 2017. Sept 2020

432. A new European settlement map from optical remotely sensed data
   Authors: , , 
   Date: 2016

433. A method for integrating MODIS and landsat data for systematic monitoring of forest cover and change in the Congo Basin
   Authors: , , 
   Date: 2008

434. GIO land (GMES/Copernicus initial operations land) High Resolution Layers (HRLs)-summary of product specifications
   Authors: 
   Date: 2015

435. Copernicus land monitoring service-high resolution layer imperviousness
   Authors: 
   Date: 2016. Sept 2020

436. Copernicus Land monitoring service-high resolution layer small woody features-2015 reference year
   Authors: , , 
   Date: 2019. Sept 2020

437. Copernicus land monitoring service-high resolution layer forest: product specifications document
   Authors: , , 
   Date: 2017. Sept 2020

438. Copernicus land monitoring service-high resolution layer grassland: product specifications document
   Authors: , , 
   Date: 2018a. Sept 2020

439. Copernicus land monitoring service-high resolution layer water and wetness: product specifications document
   Authors: , , , 
   Date: 2018b. Sept 2020

440. The suitability of decadal image data sets for mapping tropical forest cover change in the Democratic Republic of Congo: implications for the global land survey
   Authors: , , , 
   Date: 2008

441. Comparison of global and continental land cover products for selected study areas in South Central and Eastern European Region
   Authors: , , 
   Date: 2018

442. Visual assessment of the green European settlement map
   Authors: , , , 
   Date: 2016

443. Measuring the accessibility of urban green areas
   Authors: , , , , , , , 
   Date: 2016. Sept 2020. 2019. Sept 2020

444. b) Analysis of the mixed forest in the 20 m HRL forest type validation report
   Authors: , , 
   Date: 2019. Sept 2020

445. Development of a new harmonized land cover/land use dataset for agricultural monitoring in Africa
   Authors: , , 
   Date: 2017. 14-17 Mar 2017

446. Comparison of global land cover datasets for cropland monitoring
   Authors: , , , 
   Date: 2017

447. A global human settlement layer from optical HR/VHR RS data: concept and first results
   Authors: , , 
   Date: 2013

448. The European settlement map
   Authors: , , , 
   Date: 2019. 2019. Sept 2020

449. Comparative validation of HRL tree cover density and University of Maryland global forest change products
   Authors: , , 
   Date: 2017. Sept 2020

450. HRL Imperviousness degree 2015 validation report
   Authors: , , , 
   Date: 2019. Sept 2020

451. European settlement map
   Authors: , 
   Date: 2017. 2016. Sept 2020

452. A new forest cover map of continental Southeast Asia derived from satellite imagery of coarse spatial resolution
   Authors: , , 
   Date: 2004

453. Forest cover map of insular Southeast Asia at 1:5 500 000 derived from SPOT-vegetation satellite images
   Authors: , , 
   Date: 2002. 20 Apr 2021

454. Mapping of the tropical forest cover of insular Southeast Asia from SPOT4-vegetation images
   Authors: , , 
   Date: 2003

455. Forest cover of insular Southeast Asia mapped from recent satellite images of coarse spatial resolution
   Authors: , 
   Date: 2003. 2003

456. Harmonizing and combining existing land cover/land use datasets for cropland area monitoring at the African continental scale
   Authors: , , 
   Date: 2013

457. a) HRL Grassland 2015 validation report
   Authors: , , 
   Date: 2019. Sept 2020

458. HRL Grassland 2015 validation report
   Authors: , , 
   Date: 2019. Sept 2020

